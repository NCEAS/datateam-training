[["index.html", "NCEAS Data Team Reference Guide About", " NCEAS Data Team Reference Guide 2021-07-02 About This reference guide documents various workflows and related issues for processing data packages. "],["miscellaneous.html", "Miscellaneous ", " Miscellaneous "],["code-snippets.html", "Code Snippets", " Code Snippets Tools &gt; Global Options… &gt; Code &gt; Edit Snippets &gt; Add these chunks to the end of the file More info can be found in this blog post by Mara Averick on how to add them: https://maraaverick.rbind.io/2017/09/custom-snippets-in-rstudio-faster-tweet-chunks-for-all/ Usual ticket workflow snippet ticket library(dataone) library(arcticdatautils) library(EML) cn &lt;- CNode(&#39;PROD&#39;) adc &lt;- getMNode(cn, &#39;urn:node:ARCTIC&#39;) rm &lt;- &quot;add your rm&quot; pkg &lt;- get_package(adc, rm) doc &lt;- EML::read_eml(getObject(adc, pkg\\$metadata)) doc &lt;- eml_add_publisher(doc) doc &lt;- eml_add_entity_system(doc) eml_validate(doc) eml_path &lt;- &quot;eml.xml&quot; write_eml(doc, eml_path) #update &lt;- publish_update(adc, # metadata_pid = pkg\\$metadata, # resource_map_pid = pkg\\$resource_map, # metadata_path = eml_path, # data_pids = pkg\\$data, # public = F) #datamgmt::categorize_dataset(update\\$metadata, c(&quot;theme1&quot;), &quot;Your Name&quot;) The datapack version snippet datapack library(dataone) library(datapack) library(digest) library(uuid) d1c &lt;- D1Client(&quot;PROD&quot;, &quot;urn:node:ARCTIC&quot;) packageId &lt;- &quot;metadata id here&quot; dp &lt;- getDataPackage(d1c, identifier=packageId, lazyLoad=TRUE, quiet=FALSE) #get metadata id metadataId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;https://eml.ecoinformatics.org/eml-2.2.0&quot;) zipId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;application/vnd.shp+zip&quot;) removeMember(dp, zipId, removeRelationships = T) #add an existing data object dataObj &lt;- getDataObject(d1c, id=&quot;uuid of data object&quot;, lazyLoad=T, limit=&quot;1TB&quot;) dp &lt;- addMember(dp, dataObj, mo=metadataId) #upload the dataset myAccessRules &lt;- data.frame(subject=&quot;CN=arctic-data-admins,DC=dataone,DC=org&quot;, permission=&quot;changePermission&quot;) packageId &lt;- uploadDataPackage(d1c, dp, public=TRUE, accessRules=myAccessRules, quiet=FALSE) Quick way to give access to submitters to their datasets: snippet access library(dataone) library(arcticdatautils) library(EML) cn &lt;- CNode(&#39;PROD&#39;) adc &lt;- getMNode(cn, &#39;urn:node:ARCTIC&#39;) rm &lt;- &quot;rm here&quot; pkg &lt;- get_package(adc, rm) set_access(adc, unlist(pkg), &quot;orcid here&quot;) Quick access to the usual code for common Solr queries: snippet solr library(dataone) library(arcticdatautils) library(EML) cn &lt;- CNode(&#39;PROD&#39;) adc &lt;- getMNode(cn, &#39;urn:node:ARCTIC&#39;) result &lt;- query(adc, list(q = &quot;rightsHolder:*orcid.org/0000-000X-XXXX-XXXX* AND (*:* NOT obsoletedBy:*)&quot;, fl = &quot;identifier,rightsHolder,formatId, fileName, dateUploaded&quot;, sort = &#39;dateUploaded+desc&#39;, start =&quot;0&quot;, rows = &quot;1500&quot;), as=&quot;data.frame&quot;) "],["cyberduck-instructions.html", "Cyberduck instructions", " Cyberduck instructions To use Cyberduck to transfer local files onto the Datateam server: Open Cyberduck. Check that you have the latest version (Cyberduck -&gt; Check for Update…). If not, download and install the latest (you may need Dom, Jesse, or Jeanette to enter a password). Click “Open Connection”. From the drop-down, choose “SFTP (Secure File Transfer Protocol)”. Enter “datateam.nceas.ucsb.edu” for Server. Enter your username and password. Connect. From here, you can drag and drop files to and from the server. "],["file-paths.html", "File paths", " File paths This section contains some basic tips about file paths. Remote files vs. local files If you’re working on the datateam server (RStudio in an internet browser), you’re usually working in the home directory under your username. File paths generally follow this pattern: /home/dmullen/folder/file. Note: These files are not stored on your local machine, they are on a remote server. You can access them through remote SFTP software like Cyberduck. If you’re working with your local RStudio app then your file paths generally follow this pattern: /Users/datateam/folder/file. These files are stored on your local machine and you can access them with Finder. File path shortcuts Using . in a file path refers to your current working directory. You can print your working directory with the call: getwd(). If we assume that my current working directory is home/dmullen, then the following two calls are equivalent: file.exists(&#39;/home/dmullen/myFile.R&#39;) file.exists(&#39;./myFile.R&#39;) Using ~ in a file path does not always refer to your current working directory. This is a common misconception. It is actually set to the global R USER path. You can print this with the call: Sys.getenv('USER'). Let’s assume that I’m working on the arcticdatautils package which I open using a .RProj file. This will update my current working directory to: /home/dmullen/arcticdatautils. Now the following calls are equivalent: file.exists(&#39;/home/dmullen/arcticdatautils/myFile.R&#39;) file.exists(&#39;~/arcticdatautils/myFile.R&#39;) file.exists(&#39;./myFile.R&#39;) "],["metacat-version.html", "Metacat version", " Metacat version Check which version of metacat is currently deployed to - arcticdata.io here, - and to test.arcticdata.io here. "],["resources-for-r.html", "Resources for R", " Resources for R Packages The data team uses and develops a number of R packages. Here is a listing and description of the main packages: dataone reading and writing data at DataONE member nodes http://doi.org/10.5063/F1M61H5X datapack creating and managing data packages https://github.com/ropensci/datapack EML creating and editing EML metadata documents https://ropensci.github.io/EML arcticdatautils utility functions for processing data for the Arctic Data Center https://nceas.github.io/arcticdatautils/ datamgmt data management utilities for curating, documenting, and publishing data (sandbox package) https://nceas.github.io/datamgmt/ metadig authoring MetaDIG quality checks https://github.com/NCEAS/metadig-r metajam downloading and reading data and metadata from DataONE member nodes https://nceas.github.io/metajam/ Learning The following online books are useful for expanding your R knowledge and skills: Hands-On Programming with R R Programming for Data Science Exploratory Data Analysis with R R for Data Science Advanced R Efficient R Programming R Packages Mastering Software Development in R Geocomputation with R R Markdown: The Definitive Guide bookdown: Authoring Books and Technical Documents with R Markdown The Tidyverse Style Guide The RStudio cheatsheets are also useful references for functions in tidyverse and other packages. "],["edit-data-packages.html", "Edit data packages ", " Edit data packages "],["datapack-background.html", "datapack Background", " datapack Background adapted from the dataone and datapack vingettes datapack is written differently than most R packages you may have encountered in the past. This is because it uses the S4 system instead. library(dataone) library(datapack) library(uuid) Data packages Data packages are a class that has slots for relations (provenance), objects(the metadata and data file(s)) and systemMetadata. Navigating data packages Nodes Using this example on arcticdata.io d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) To use the member node information, use the mn slot d1c_test@mn To access the various slots using objects created by datapack and dataone (e.g. getSystemMetadata) requires the @ which is different from what you might have seen in the past. This is because these use the S4 system. Get an existing package from the Arctic Data Center dp &lt;- dataone::getDataPackage(d1c, &quot;resource_map_urn:uuid:1f9eee7e-2d03-43c4-ad7f-f300e013ab28&quot;) Data Check out the objects slot dp@objects Get the number for data and metadata files associated with this data package: length(dp@objects) Provenance View the provenance as a dataTable. We will get into detail in the Building provenance chapter. dp@relations$relations Get identifiers You can search by any of the sysmeta slots such as fileName and formatId and get the corresponding identifier(s): metadataId &lt;- selectMember(dp, name=&quot;sysmeta@ADD THE NAME OF THE SLOT&quot;, value=&quot;PATTERN TO SEARCH BY&quot;) For example: selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;image/tiff&quot;) selectMember(dp, name=&quot;sysmeta@fileName&quot;, value=&quot;filename.csv&quot;) Exercise 2a Select a dataset from the catalog on arcticdata.io and checkout the number of files and provencance relationships in the dataset. "],["create-a-new-data-package.html", "Create a new data package", " Create a new data package adapted from the dataone and datapack vingettes library(dataone) library(datapack) library(uuid) Create a new data package - data package is a class that has slots for relations (provenance), objects(the metadata and data file(s)) and systemMetadata. dp &lt;- new(&quot;DataPackage&quot;) Upload new data files Create and add a metadata file In this example we will use this previously written EML metadata. Here we are getting the file path from the dataone package and saving that as the object emlFile. emlFile &lt;- system.file(&quot;extdata/strix-pacific-northwest.xml&quot;, package=&quot;dataone&quot;) Create a new DataObject and add it to the package. metadataObj &lt;- new(&quot;DataObject&quot;, format=&quot;https://eml.ecoinformatics.org/eml-2.2.0&quot;, filename=emlFile) dp &lt;- addMember(dp, metadataObj) Check the dp object to see if the DataObject was added correctly. dp Add some additional data files sourceData &lt;- system.file(&quot;extdata/OwlNightj.csv&quot;, package=&quot;dataone&quot;) sourceObj &lt;- new(&quot;DataObject&quot;, format=&quot;text/csv&quot;, filename=sourceData) dp &lt;- addMember(dp, sourceObj, metadataObj) Add additional files with structure In this example adding the csv files to a folder named data and scripts outputData &lt;- system.file(&quot;extdata/Strix-occidentalis-obs.csv&quot;, package=&quot;dataone&quot;) outputObj &lt;- new(&quot;DataObject&quot;, format=&quot;text/csv&quot;, filename=outputData, targetPath = &quot;data&quot;) dp &lt;- addMember(dp, outputObj, metadataObj) progFile &lt;- system.file(&quot;extdata/filterObs.R&quot;, package=&quot;dataone&quot;) progObj &lt;- new(&quot;DataObject&quot;, format=&quot;application/R&quot;, filename=progFile, targetPath = &quot;scripts&quot;, mediaType=&quot;text/x-rsrc&quot;) dp &lt;- addMember(dp, progObj, metadataObj) If you want to change the formatId please use updateSystemMetadata Upload the package d1c &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) Make sure to give access privlidges to the ADC admins: myAccessRules &lt;- data.frame(subject=&quot;CN=arctic-data-admins,DC=dataone,DC=org&quot;, permission=&quot;changePermission&quot;) Get necessary token from test.arcticdata.io to upload the dataset prior uploading the datapackage: packageId &lt;- uploadDataPackage(d1c, dp, public=TRUE, accessRules=myAccessRules, quiet=FALSE) "],["update-packages-with-datapack.html", "Update packages with datapack", " Update packages with datapack Once you have updated the data objects and saved the metadata to a file, we can update the metadata and add the new pid to the resource map using datapack::updateDataObject(). This part of the training assumes that you have an EML file created from earlier parts of the training You must use these R packages for data packages that have provenance or folder hierarchy (arcticdatatutils does not support those features) Get the package Setting the node is done slightly differently: d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) Get a pre-existing package: packageId &lt;- &quot;the resource map&quot; dp &lt;- getDataPackage(d1c_test, identifier=packageId, lazyLoad=TRUE, quiet=FALSE) Update the metadata Get the metadata identifier metadataId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;https://eml.ecoinformatics.org/eml-2.2.0&quot;) Take note of the EML version. If it is EML 2.1.1 the value needs to be changed to -“eml://ecoinformatics.org/eml-2.1.1” Read the EML doc doc &lt;- read_eml(getObject(d1c@mn, metadataId)) Edit the EML as usual - see the documents in Edit EML for details Once you are happy with your changes, you can update your data package to include the new metadata file using replaceMember eml_path &lt;- &quot;path/to/your/saved/eml.xml&quot; write_eml(doc, eml_path) dp &lt;- replaceMember(dp, metadataId, replacement=eml_path) Modify the data files remove zip files zipId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;application/vnd.shp+zip&quot;) removeMember(dp, zipId, removeRelationships = T) add an existing data object dataObj &lt;- getDataObject(d1c_test, id=&quot;urn:uuid: here&quot;, lazyLoad=T, limit=&quot;1TB&quot;) dp &lt;- addMember(dp, dataObj, mo=metadataId) If you want to change the formatId please use updateSystemMetadata Publish update Using a DOI If you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), (a) you need to change the public argument to TRUE and generate a DOI identifier. This should only be done after the package is finalized and has been thoroughly reviewed! Updating a package with a new DOI: A Digital Object Identifier (DOI) may be assigned to the metadata DataObject, using the generateIdentifier: doi &lt;- dataone::generateIdentifier(d1c_test@mn, &quot;DOI&quot;) dp &lt;- replaceMember(dp, metadataId, replacement=eml_path, newId=doi) newPackageId &lt;- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE) Updating a package with a pre-issued DOI dp &lt;- replaceMember(dp, metadataId, replacement=eml_path, newId=&quot;your pre-issued doi previously generated&quot;) newPackageId &lt;- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE) Refresh the landing page at test.arcticdata.io/#view/… for this package and then follow the “newer version” link to view the latest. "],["add-a-pre-generated-identifier-to-the-eml.html", "Add a pre-generated identifier to the EML", " Add a pre-generated identifier to the EML When you pre-generate a UUID or DOI, the change is not automatically reflected in the packageId section of the EML. Use the code below to ensure that the EML lines up with the desired identifier: ## Generate DOI and add to EML # Note that you cannot generate a DOI on test nodes doiPid &lt;- generateIdentifier(mn, &quot;DOI&quot;) doc$packageId &lt;- doiPid Be sure to include the identifier= argument in your publish update command so the pre-generated identifier is applied. "],["edit-sysmeta.html", "Edit sysmeta", " Edit sysmeta To edit the sysmeta of an object (data file, EML, or resource map, etc.) with a PID, first load the sysmeta into R using the following command: sysmeta &lt;- getSystemMetadata(mn, pid) Then edit the sysmeta slots by using @ functionality. For example, to change the fileName use the following command: sysmeta@fileName &lt;- &#39;NewFileName.csv&#39; Note that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as datapack::addAccessRule() (which takes the sysmeta as an input) or arcticdatautils::set_rights_and_access(), which only requires a PID. In general, you most frequently need to use dataone::getSystemMetadata() to change either the formatId or fileName slots (see the DataONE list of format ids) for acceptable formats. # Example of setting the formatId slot sysmeta@formatId &lt;- &quot;eml://ecoinformatics.org/eml-2.1.1&quot; After you have changed the necessary slot, you can update the system metadata using the following command: updateSystemMetadata(mn, pid, sysmeta) Identifiers and sysmeta Importantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata are accurate when an object is first published). Additional resources For a more in-depth (and technical) guide to sysmeta, check out the DataONE documentation: System Metadata Data Types in CICore "],["obsolescence-chain.html", "Obsolescence chain", " Obsolescence chain You can obsolete a dataset using the function datamgmt::obsolete_package(). Use the documentation for instructions on using the function. The following workflow explains how the functions operate. This chunk is to obsolete one data set. If there are more to add to the chain, more steps can be added. Be very careful. Make sure to fill in obsoletes and obsoletedBy slots for each one. The obsoletes and obsoletedBy fields must be NA, once they are populated they can’t be modified. # get oldest version of the file you want to be visible. Use get_all_versions and look at the latest. # urn:uuid:... # PID for data set to be obsoleted (hidden): doi:10… # adding data set to obsolete (hide) in the slot before the first version of the visible data set sysmeta1 &lt;- getSystemMetadata(mn, &quot;urn:uuid:example_pid&quot;) sysmeta1@obsoletes &lt;- &quot;doi:10.example_doi&quot; updateSystemMetadata(mn, &quot;urn:uuid:example_pid&quot;, sysmeta1) # adding first version to obsolescence chain after obsoleted (hidden) version sysmeta0 &lt;- getSystemMetadata(mn, &quot;doi:10.example_doi&quot;) sysmeta0@obsoletedBy &lt;- &quot;urn:uuid:example_pid&quot; updateSystemMetadata(mn, &quot;doi:10.example_doi&quot;, sysmeta0) The following code is equivalent to the code chunk above. This method is recommended, however it is necessary to read the function documentation first. datamgmt::obsolete_package(mn, metadata_obsolete = &quot;doi:10.example_doi&quot;, metadata_new = &quot;urn:uuid:example_pid&quot;) "],["set-dataone-nodes.html", "Set DataONE nodes", " Set DataONE nodes DataONE is a network of data repositories that is structured with coordinating nodes (CN) and member nodes (MN). The network tree looks something like this: At the top level is DataONE itself. Within DataONE there are several coordinating nodes, including nodes for both production material and testing material. Within these coordinating nodes are many member nodes, including ones for both the Arctic Data Center and the KNB. To set the environment in which you want to publish data, you need to set both the coordinating node and the member node. For example, if you are publishing to the Arctic Data Center test site, you would want to set the coordinating node to STAGING and the member node to mnTestArctic. A note on nodes - be very careful about what you publish on production nodes (PROD, or arcticdata.io). These nodes should NEVER be used to publish test or training data sets. The primary nodes we work on, and how to set them in R, are below: Staging (Test) nodes # ADC (test.arcticdata.io) cn_staging &lt;- CNode(&#39;STAGING&#39;) adc_test &lt;- getMNode(cn_staging,&#39;urn:node:mnTestARCTIC&#39;) # KNB (dev.nceas.ucsb.edu) cn_staging2 &lt;- CNode(&quot;STAGING2&quot;) knb_test &lt;- getMNode(cn_staging2, &quot;urn:node:mnTestKNB&quot;) Production nodes # ADC (arcticdata.io) cn &lt;- CNode(&#39;PROD&#39;) adc &lt;- getMNode(cn,&#39;urn:node:ARCTIC&#39;) # KNB (knb.ecoinformatics.org) knb &lt;- getMNode(cn, &quot;urn:node:KNB&quot;) # GOA goa &lt;- getMNode(cn, &quot;urn:node:GOA&quot;) # You can also use the datamgmt::guess_membernode function to set a member node # Note: this pid looks like a URL - it&#39;s really a unique identifier dryad &lt;- datamgmt::guess_member_node(&#39;https://doi.org/10.5061/dryad.k6gf1tf/15?ver=2018-09-18T03:54:10.492+00:00&#39;) More DataONE STAGING nodes can be found here More DataONE PROD nodes can be found here "],["set-rights-and-access.html", "Set rights and access", " Set rights and access One final step when creating/updating packages is to make sure that the rights and access on all the objects that were uploaded are set correctly within the sysmeta. The function arcticdatautils::set_rights_and_access() will set both, and arcticdatautils::set_access() will just set access. There are two functions for this because a rightsHolder should always have access, but not all people who need access are rightsHolders. The rightsHolder of the data package is typically the submitter (if the data set is submitted through the web form (“editor”)), but if a data team member is publishing objects for a PI, the rightsHolder should be the main point of contact for the data set (i.e. the person who requested that we upload the data for them). To set the rights and access for all of the objects in a package, first get the ORCiD of the person to whom you are giving rights and access. You can set this manually, or grab it from one of the creators in an EML file. You can look up ORCID iDs here # Manually set ORCiD subject &lt;- &#39;http://orcid.org/PUT0-YOUR-ORCD-HERE&#39; # Set ORCiD from EML creator # if only 1 creator exists subject &lt;- doc$dataset$creator$userId$userId # if more than 1 creator exists and you want the first one subject &lt;- doc$dataset$creator[[1]]$userId$userId # As a convention we use `http:` instead of `https:` in our system metadata subject &lt;- sub(&quot;^https://&quot;, &quot;http://&quot;, subject) Note, when setting metadata, the ORCiD must start with http://. ORCiDs in EML should start with https://. The sub() command above will change this formatting for you. Next, set the rights and access using the following command: set_rights_and_access(mn, pids = c(pkg$metadata, pkg$data, pkg$resource_map), subject = subject, permissions = c(&#39;read&#39;,&#39;write&#39;,&#39;changePermission&#39;)) If you ever need to remove/add public access to your package or object, you can use remove_public_read() or set_public_read(), respectively. remove_public_read(mn, c(pkg$metadata, pkg$data, pkg$resource_map)) My profile The datasets that render under a user’s profile page like here are added if one of the following three System Metadata fields exists. The subject is the rightsHolder or the subject has one of either write or changePermission in the accessPolicy. If you ever need to remove a subject from the accessPolicy or update the rightsHolder you can use arcticdatautils::remove_access and arcticdatautils::set_rightsHolder, respectively. "],["show-indexing-status.html", "Show indexing status", " Show indexing status Sometimes it takes awhile for the website to render with the updates you’ve made in R. To check whether a PID has been indexed yet, use: show_indexing_status(mn, pid) The status bar will either show 0% (not indexed) or 100% (should be online already). "],["update-a-package-with-a-new-data-object.html", "Update a package with a new data object", " Update a package with a new data object Once you have updated the data objects and saved the metadata to a file, we can update the metadata and add the new pid to the resource map using publish_update(). Make sure you have the package you want to update, loaded into R using get_package(). Publish update Now we can update your data package to include the new data object. Assuming you have updated your data package earlier something like the below: d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) packageId &lt;- &quot;the resource map&quot; dp &lt;- getDataPackage(d1c_test, identifier=packageId, lazyLoad=TRUE, quiet=FALSE) metadataId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;https://eml.ecoinformatics.org/eml-2.2.0&quot;) #some modification to the EML here eml_path &lt;- &quot;path/to/your/saved/eml.xml&quot; write_eml(doc, eml_path) dp &lt;- replaceMember(dp, metadataId, replacement=eml_path) You can then upload your data package: myAccessRules &lt;- data.frame(subject=&quot;CN=arctic-data-admins,DC=dataone,DC=org&quot;, permission=&quot;changePermission&quot;) packageId &lt;- uploadDataPackage(d1c_test, dp, public=FALSE, accessRules=myAccessRules, quiet=FALSE) If a package is ready to be public, you can change the public argument in the datapack::uploadDataPackage() call to TRUE. If you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), you need to do this when replacing the metadata. This should only be done after the package is finalized and has been thoroughly reviewed! doi &lt;- dataone::generateIdentifier(d1c_test@mn, &quot;DOI&quot;) dp &lt;- replaceMember(dp, metadataId, replacement=eml_path, newId=doi) newPackageId &lt;- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE) If there is a pre-issued DOI (researcher requested the DOI for the publication first), please do the following: dp &lt;- replaceMember(dp, metadataId, replacement=eml_path, newId=&quot;your pre-issued doi previously generated&quot;) newPackageId &lt;- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE) If the package has children, see how to do this using arcticdatautils::publish_update in the nesting section of the reference manual. Refresh the landing page at test.arcticdata.io/#view/… for this package and then follow the “newer version” link to view the latest. "],["update-a-data-object.html", "Update a data object", " Update a data object To update a data file associated with a data package, you need to do three things: update the object itself, update the resource map (which affiliates the object with the metadata), and update the metadata that describes that object The datapack::replaceMember function takes care of the first two of these tasks. First you need to get the pid of the file you want to replace by using datapack::selectMember metadataId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;https://eml.ecoinformatics.org/eml-2.2.0&quot;) Then use replaceMember: dp &lt;- replaceMember(dp, metadataId, replacement=file_path) If the object was previously uploaded, we can use getDataObject to retrieve it: dataObj &lt;- getDataObject(d1c_test, id=&quot;urn:uuid: here&quot;, lazyLoad=T, limit=&quot;1TB&quot;) dp &lt;- addMember(dp, dataObj, mo=metadataId) If you want to remove some files from the data package we can use datapack::removeMember. In this example, we want to remove all the zip files associated with this data package, we can use datapack::removeMember: zipId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;application/vnd.shp+zip&quot;) removeMember(dp, zipId, removeRelationships = T) You will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. Use line 2 (Id:) exactly, character for character. To accomplish the second task, you will need to update the metadata using the EML package. This is covered in Chapter 4. After you update a file, you will always need to update the metadata because parts of the physical section (such as the file size, checksum) will be different, and it may also require different attribute information. Once you have updated your metadata and saved it, you can update the package itself. "],["explore-eml.html", "Explore EML ", " Explore EML "],["access-specific-elements.html", "Access specific elements", " Access specific elements The eml_get() function is a powerful tool for exploring EML (more on that here ). It takes any chunk of EML and returns all instances of the element you specify. Note: you’ll have to specify the element of interest exactly, according to the spelling/capitalization conventions used in EML. Here are some examples: doc &lt;- read_eml(system.file(&quot;example-eml.xml&quot;, package = &quot;arcticdatautils&quot;)) eml_get(doc, &quot;creator&quot;) individualName: givenName: Bryce surName: Mecum organizationName: National Center for Ecological Analysis and Synthesis eml_get(doc, &quot;boundingCoordinates&quot;) eastBoundingCoordinate: &#39;-134&#39; northBoundingCoordinate: &#39;59&#39; southBoundingCoordinate: &#39;57&#39; westBoundingCoordinate: &#39;-135&#39; eml_get(doc, &quot;url&quot;) &#39;&#39;: function: download url: ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456 &#39;&#39;: ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456 eml_get_simple() is a simplified alternative to eml_get() that produces a list of the desired EML element. eml_get_simple(doc$dataset$otherEntity, &quot;entityName&quot;) To find an eml element you can use either a combination of which_in_emlfrom the arcticdatautils package or eml_get_simple and which to find the index in an EML list. Use which ever workflow you see fit. An example question you may have: Which creators have a surName “Mecum”? Example using which_in_eml: n &lt;- which_in_eml(doc$dataset$creator, &quot;surName&quot;, &quot;Mecum&quot;) # Answer: doc$dataset$creator[[n]] Example using eml_get_simple and which: ent_names &lt;- eml_get_simple(doc$dataset$creator, &quot;surName&quot;) i &lt;- which(ent_names == &quot;Mecum&quot;) # Answer: doc$dataset$creator[[i]] "],["navigate-through-eml.html", "Navigate through EML", " Navigate through EML The first task when editing an EML file is navigating the EML file. An EML file is organized in a structure that contains many lists nested within other lists. The function View allows you to get a crude view of an EML file in the viewer. It can be useful for exploring the file. # Need to be in this member node to explore file d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) doc &lt;- read_eml(getObject(d1c_test@mn, &quot;urn:uuid:558eabf1-1e91-4881-8ba3-ef8684d8f6a1&quot;)) View(doc) The complex EML document is represented in R as as series of named, nested lists. We use lists all the time in R! A data.frame is one example of a special kind of list that we use all the time. You may be familiar with the syntax dataframe$column_name which allows us to select a particular column of a data.frame. Under the hood, a data.frame is a named list of vectors with the same length. You select one of those vectors using the $ operator, which is called the “list selector operator.” Just like you navigate in a data.frame, you can use the $ operator to navigate through the EML structure. The $ operator allows you to go deeper into the EML structure and to see what elements are nested within other elements. However, you have to tell R where you want to go in the structure when you use the $ symbol. For example, if you want to view the dataset element of your EML you would use the command doc$dataset. If you want to view the creators of your data set you would use doc$dataset$creator. Note here that creator is contained within dataset. If you aren’t sure where you want to go, hit the tab button on your keyboard after typing $ and a list of available elements in the structure will appear (e.g., doc$&lt;TAB&gt;): Note that if you hit tab, and nothing pops up, this most likely implies that you are trying to go into an EML element that can take a series items. For example doc$dataset$creator$&lt;TAB&gt; will not show a pop-up menu. This is because creator is a series-type object (i.e. you can have multiple creators). If you want to go deeper into creator, you first must tell R which creator you are interested in. Do this by writing [[i]] first where i is the index of the creator you are concerned with. For example, if you want to look at the first creator i = 1. Now doc$dataset$creator[[1]]$&lt;TAB&gt; will give you many more options. Note, an empty autocomplete result sometimes means you have reached the end of a branch in the EML structure. At this point stop and take a deep breath. The key takeaway is that EML is a hierarchical tree structure. The best way to get familiar with it is to explore the structure. Try entering doc$dataset into your console, and print it. Now make the search more specific, for instance: doc$dataset$abstract. "],["understand-the-eml-schema.html", "Understand the EML schema", " Understand the EML schema Another great resource for navigating the EML structure is looking at the schema which defines the structure. The schema diagrams on this page are interactive. Further explanations of the symbology can be found here. The schema is complicated and may take some time to get familiar with before you will be able to fully understand it. For example, let’s take a look at eml-party. To start off, notice that some elements have bolded lines leading to them. A bold line indicates that the element is required if the element above it (to the left in the schema) is used, otherwise the element is optional. Notice also that next to the givenName element it says “0..infinity”. This means that the element is unbounded — a single party can have many given names and there is no limit on how many you can add. However, this text does not appear for the surName element — a party can have only one surname. You will also see icons linking the EML slots together, which indicate the ordering of subsequent slots. These can indicate either a “sequence” or a “choice”. In our example from eml-party, a “choice” icon indicates that either an individualName, organizationName, or positionName is required, but you do not need all three. However, the “sequence” icon tells us that if you use an individualName, you must include the surName as a child element. If you include the optional child elements salutation and givenName, they must be written in the order presented in the schema. The eml schema sections you may find particularly helpful include eml-party, eml-attribute and eml-physical. For a more detailed description of the EML schema, see the reference section on exploring EML. "],["edit-eml.html", "Edit EML ", " Edit EML "],["edit-an-eml-element.html", "Edit an EML element", " Edit an EML element There are multiple ways to edit an EML element. Edit EML with strings The most basic way to edit an EML element would be to navigate to the element and replace it with something else. Easy! For example, to change the title one could use the following command: doc$dataset$title &lt;- &quot;New Title&quot; If the element you are editing allows for multiple values, you can pass it a list of character strings. Since a dataset can have multiple titles, we can do this: doc$dataset$title &lt;- list(&quot;New Title&quot;, &quot;Second New Title&quot;) However, this isn’t always the best method to edit the EML, particularly if the element has sub-elements. Edit EML with the “EML” package To edit a section where you are not 100% sure of the sub-elements, using the eml$elementName() helper functions from the EML package will pre-populate the options for you if you utilize the RStudio autocomplete functionality. The arguments in these functions show the available slots for any given EML element. For example, typing doc$dataset$abstract &lt;- eml$abstract()&lt;TAB&gt; will show you that the abstract element can take either the section or para sub-elements. doc$dataset$abstract &lt;- eml$abstract(para = &quot;A concise but thorough description of the who, what, where, when, why, and how of a dataset.&quot;) This inserts the abstract with a para element in our dataset, which we know from the EML schema is valid. Note that the above is equivalent to the following generic construction: doc$dataset$abstract &lt;- list(para = &quot;A concise but thorough description of the who, what, where, when, why, and how of a dataset.&quot;) The eml() family of functions provides the sub-elements as arguments, which is extremely helpful, but functionally all it is doing is creating a named list, which you can also do using the list function. Edit EML with objects A final way to edit an EML element would be to build a new object to replace the old object. To begin, you might create an object using an eml helper function. Let’s take keywords as an example. Sometimes keyword lists in a metadata record will come from different thesauruses, which you can then add in series (similar to the way we added multiple titles) to the element keywordSet. We start by creating our first set of keywords and saving it to an object. kw_list_1 &lt;- eml$keywordSet(keywordThesaurus = &quot;LTER controlled vocabulary&quot;, keyword = list(&quot;bacteria&quot;, &quot;carnivorous plants&quot;, &quot;genetics&quot;, &quot;thresholds&quot;)) Which returns: $keyword $keyword[[1]] [1] &quot;bacteria&quot; $keyword[[2]] [1] &quot;carnivorous plants&quot; $keyword[[3]] [1] &quot;genetics&quot; $keyword[[4]] [1] &quot;thresholds&quot; $keywordThesaurus [1] &quot;LTER controlled vocabulary&quot; We create the second keyword list similarly: kw_list_2 &lt;- eml$keywordSet(keywordThesaurus = &quot;LTER core area&quot;, keyword = list(&quot;populations&quot;, &quot;inorganic nutrients&quot;, &quot;disturbance&quot;)) Finally, we can insert our two keyword lists into our EML document just like we did with the title example above, but rather than passing character strings into list(), we will pass our two keyword set objects. doc$dataset$keywordSet &lt;- list(kw_list_1, kw_list_2) Note that you must use the function list here and not the c() function. The reasons for this are complex, and due to some technical subtlety in R - but the gist of the issue is that the c() function can behave in unexpected ways with nested lists, and frequently will collapse the nesting into a single level, resulting in invalid EML. "],["edit-attributelists.html", "Edit attributeLists", " Edit attributeLists Attributes are stored in an attributeList. When editing attributes in R, you need to create one to three objects: A data.frame of attributes A data.frame of custom units (if applicable) Attributes can exist in EML for dataTable, otherEntity, and spatialVector data objects. Please note that submitting attribute information through the website will store them in an otherEntity object by default. We prefer to store them in a dataTable object for tabular data or a spatialVector object for spatial data. To edit or examine an existing attribute table already in an EML file, you can use the following commands, where i represents the index of the series element you are interested in. Note that if there is only one item in the series (ie there is only one dataTable), you should just call doc$dataset$dataTable, as in this case doc$dataset$dataTable[[1]] will return the first sub-element of the dataTable (the entityName) # If they are stored in an otherEntity (submitted from the website by default) attributeList &lt;- EML::get_attributes(doc$dataset$otherEntity[[i]]$attributeList) # Or if they are stored in a dataTable (usually created by a datateam member) attributeList &lt;- EML::get_attributes(doc$dataset$dataTable[[i]]$attributeList) # Or if they are stored in a spatialVector (usually created by a datateam member) attributeList &lt;- EML::get_attributes(doc$dataset$spatialVector[[i]]$attributeList) attributes &lt;- attributeList$attributes print(attributes) Edit attributes Attribute information should be stored in a data.frame with the following columns: attributeName: The name of the attribute as listed in the csv. Required. e.g.: “c_temp” attributeLabel: A descriptive label that can be used to display the name of an attribute. It is not constrained by system limitations on length or special characters. Optional. e.g.: “Temperature (Celsius)” attributeDefinition: Longer description of the attribute, including the required context for interpreting the attributeName. Required. e.g.: “The near shore water temperature in the upper inter-tidal zone, measured in degrees Celsius.” measurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required. nominal: unordered categories or text. e.g.: (Male, Female) or (Yukon River, Kuskokwim River) ordinal: ordered categories. e.g.: Low, Medium, High dateTime: date or time values from the Gregorian calendar. e.g.: 01-01-2001 ratio: measurement scale with a meaningful zero point in nature. Ratios are proportional to the measured variable. e.g.: 0 Kelvin represents a complete absence of heat. 200 Kelvin is half as hot as 400 Kelvin. 1.2 meters per second is twice as fast as 0.6 meters per second. interval: values from a scale with equidistant points, where the zero point is arbitrary. This is usually reserved for degrees Celsius or Fahrenheit, or latitude and longitude coordinates, or any other human-constructed scale. e.g.: there is still heat at 0° Celsius; 12° Celsius is NOT half as hot as 24° Celsius. domain: One of: textDomain, enumeratedDomain, numericDomain, dateTime. Required. textDomain: text that is free-form, or matches a pattern enumeratedDomain: text that belongs to a defined list of codes and definitions. e.g.: CASC = Cascade Lake, HEAR = Heart Lake dateTimeDomain: dateTime attributes numericDomain: attributes that are numbers (either ratio or interval) formatString: Required for dateTime, NA otherwise. Format string for dates, e.g. “DD/MM/YYYY”. definition: Required for textDomain, NA otherwise. Definition for attributes that are a character string, matches attribute definition in most cases. unit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found using: standardUnits &lt;- EML::get_unitList() then running View(standardUnits$units). numberType: Required for numericDomain, NA otherwise. Options are real, natural, whole, and integer. real: positive and negative fractions and integers (…-1,-0.25,0,0.25,1…) natural: non-zero positive integers (1,2,3…) whole: positive integers and zero (0,1,2,3…) integer: positive and negative integers and zero (…-2,-1,0,1,2…) missingValueCode: Code for missing values (e.g.: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’ missingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists. You can create attributes manually by typing them out in R following a workflow similar to the one below: attributes &lt;- data.frame( attributeName = c(&#39;Date&#39;, &#39;Location&#39;, &#39;Region&#39;,&#39;Sample_No&#39;, &#39;Sample_vol&#39;, &#39;Salinity&#39;, &#39;Temperature&#39;, &#39;sampling_comments&#39;), attributeDefinition = c(&#39;Date sample was taken on&#39;, &#39;Location code representing location where sample was taken&#39;,&#39;Region where sample was taken&#39;, &#39;Sample number&#39;, &#39;Sample volume&#39;, &#39;Salinity of sample in PSU&#39;, &#39;Temperature of sample&#39;, &#39;comments about sampling process&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;nominal&#39;,&#39;nominal&#39;, &#39;nominal&#39;, &#39;ratio&#39;, &#39;ratio&#39;, &#39;interval&#39;, &#39;nominal&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;enumeratedDomain&#39;,&#39;enumeratedDomain&#39;, &#39;textDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;textDomain&#39;), formatString = c(&#39;MM-DD-YYYY&#39;, NA,NA,NA,NA,NA,NA,NA), definition = c(NA,NA,NA,&#39;Sample number&#39;, NA, NA, NA, &#39;comments about sampling process&#39;), unit = c(NA, NA, NA, NA,&#39;milliliter&#39;, &#39;dimensionless&#39;, &#39;celsius&#39;, NA), numberType = c(NA, NA, NA,NA, &#39;real&#39;, &#39;real&#39;, &#39;real&#39;, NA), missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, &#39;NA&#39;), missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, &#39;no sampling comments&#39;)) However, typing this out in R can be a major pain. Luckily, there’s a Shiny app that you can use to build attribute information. You can use the app to build attributes from a data file loaded into R (recommended as the app will auto-fill some fields for you) to edit an existing attribute table, or to create attributes from scratch. Use the following commands to create or modify attributes (these commands will launch a Shiny app in your web browser): #first download the CSV in your data package from Exercise #2 data &lt;- read.csv(text=rawToChar(getObject(adc_test, pkg$data))) # From data (recommended) EML::shiny_attributes(data = data) # From an existing attribute table attributeList &lt;- get_attributes(doc$dataset$dataTable[[i]]$attributeList) EML::shiny_attributes(data = NULL, attributes = attributeList$attributes) # From scratch atts &lt;- EML::shiny_attributes() Once you are done editing a table in the app, quit the app and the tables will be assigned to the atts variable as a list of data frames (one for attributes, factors, and units). Alternatively, each table can be to exported to a csv file by clicking the Download button. If you downloaded the table, read the table back into your R session and assign it to a variable in your script (e.g. attributes &lt;- data.frame(...)), or just use the variable that shiny_attributes returned. For simple attribute corrections, datamgmt::edit_attribute() allows you to edit the slots of a single attribute within an attribute list. To use this function, pass an attribute through datamgmt::edit_attribute() and fill out the parameters you wish to edit/update. An example is provided below where we are changing attributeName, domain, and measurementScale in the first attribute of a dataset. After completing the edits, insert the new version of the attribute back into the EML document. new_attribute &lt;- datamgmt::edit_attribute(doc$dataset$dataTable[[1]]$attributeList$attribute[[1]], attributeName = &#39;date_and_time&#39;, domain = &#39;dateTimeDomain&#39;, measurementScale = &#39;dateTime&#39;) doc$dataset$dataTable[[1]]$attributeList$attribute[[1]] &lt;- new_attribute Edit custom units EML has a set list of units that can be added to an EML file. These can be seen by using the following code: standardUnits &lt;- EML::get_unitList() View(standardUnits$units) Search the units list for your unit before attempting to create a custom unit. You can search part of the unit you can look up part of the unit ie meters in the table to see if there are any matches. If you have units that are not in the standard EML unit list, you will need to build a custom unit list. A unit typically consists of the following fields: id: The unit id (ids are camelCased) unitType: The unitType (run View(standardUnits$unitTypes) to see standard unitTypes) parentSI: The parentSI unit (e.g. for kilometer parentSI = “meter”) multiplierToSI: Multiplier to the parentSI unit (e.g. for kilometer multiplierToSI = 1000) name: Unit abbreviation (e.g. for kilometer name = “km”) description: Text defining the unit (e.g. for kilometer description = “1000 meters”) To manually generate the custom units list, create a dataframe with the fields mentioned above. An example is provided below that can be used as a template: custom_units &lt;- data.frame( id = c(&#39;siemensPerMeter&#39;, &#39;decibar&#39;), unitType = c(&#39;resistivity&#39;, &#39;pressure&#39;), parentSI = c(&#39;ohmMeter&#39;, &#39;pascal&#39;), multiplierToSI = c(&#39;1&#39;,&#39;10000&#39;), abbreviation = c(&#39;S/m&#39;,&#39;decibar&#39;), description = c(&#39;siemens per meter&#39;, &#39;decibar&#39;)) Using EML::get_unit_id for custom units will also generate valid EML unit ids. Custom units are then added to additionalMetadata using the following command: unitlist &lt;- set_unitList(custom_units, as_metadata = TRUE) doc$additionalMetadata &lt;- list(metadata = list(unitList = unitlist)) Edit factors For attributes that are enumeratedDomains, a table is needed with three columns: attributeName, code, and definition. attributeName should be the same as the attributeName within the attribute table and repeated for all codes belonging to a common attribute. code should contain all unique values of the given attributeName that exist within the actual data. definition should contain a plain text definition that describes each code. To build factors by hand, you use the named character vectors and then convert them to a data.frame as shown in the example below. In this example, there are two enumerated domains in the attribute list - “Location” and “Region”. Location &lt;- c(CASC = &#39;Cascade Lake&#39;, CHIK = &#39;Chikumunik Lake&#39;, HEAR = &#39;Heart Lake&#39;, NISH = &#39;Nishlik Lake&#39; ) Region &lt;- c(W_MTN = &#39;West region, locations West of Eagle Mountain&#39;, E_MTN = &#39;East region, locations East of Eagle Mountain&#39;) The definitions are then written into a data.frame using the names of the named character vectors and their definitions. factors &lt;- rbind(data.frame(attributeName = &#39;Location&#39;, code = names(Location), definition = unname(Location)), data.frame(attributeName = &#39;Region&#39;, code = names(Region), definition = unname(Region))) Finalize attributeList Once you have built your attributes, factors, and custom units, you can add them to EML objects. Attributes and factors are combined to form an attributeList using the following command: attributeList &lt;- EML::set_attributes(attributes = attributes, factors = factors) This attributeList must then be added to a dataTable. "],["edit-custom-units-1.html", "Edit custom units", " Edit custom units EML has a set list of units that can be added to an EML file. These can be seen by using the following code: standardUnits &lt;- EML::get_unitList() View(standardUnits$units) If you have units that are not in the standard EML unit list, you will need to build a custom unit list. A unit typically consists of the following fields: id: The unit id (ids are camelCased) unitType: The unitType (run View(standardUnits$unitTypes) to see standard unitTypes) parentSI: The parentSI unit (e.g. for kilometer parentSI = “meter”) multiplierToSI: Multiplier to the parentSI unit (e.g. for kilometer multiplierToSI = 1000) name: Unit abbreviation (e.g. for kilometer name = “km”) description: Text defining the unit (e.g. for kilometer description = “1000 meters”) To manually generate the custom units list, create a dataframe with the fields mentioned above. An example is provided below that can be used as a template: custom_units &lt;- data.frame( id = c(&#39;partsPerThousand&#39;, &#39;decibar&#39;, &#39;wattsPerSquareMeter&#39;, &#39;micromolesPerGram&#39;, &#39;practicalSalinityUnit&#39;), unitType = c(&#39;dimensionless&#39;, &#39;pressure&#39;, &#39;power&#39;, &#39;amountOfSubstanceWeight&#39;， &#39;dimensionless&#39;), parentSI = c(NA, &#39;pascal&#39;, &#39;watt&#39;, &#39;molesPerKilogram&#39;， NA), multiplierToSI = c(NA, &#39;10000&#39;, &#39;1&#39;, &#39;1000000000&#39;, NA), abbreviation = c(&#39;ppt&#39;, &#39;decibar&#39;, &#39;W/m^2&#39;, &#39;umol/g&#39;, &#39;PSU&#39;), description = c(&#39;parts per thousand&#39;, &#39;decibar&#39;, &#39;watts per square meter&#39;, &#39;micro moles per gram&#39;, &#39;used to describe the concentration of dissolved salts in water, the UNESCO Practical Salinity Scale of 1978 (PSS78) defines salinity in terms of a conductivity ratio&#39;)) Using EML::get_unit_id for custom units will also generate valid EML unit ids. Custom units are then added to additionalMetadata using the following command: unitlist &lt;- set_unitList(custom_units, as_metadata = TRUE) doc$additionalMetadata &lt;- unitlist If units that should be standardUnit are added as customUnit, you can use the following code to fix this issue: # add standard unit doc$dataset$dataTable[[i]]$attributeList$attribute[[i]]$measurementScale$ratio$unit$standardUnit &lt;- &quot;your standard unit&quot; # get rid of custom unit doc$dataset$dataTable[[i]]$attributeList$attribute[[i]]$measurementScale$ratio$unit$customUnit &lt;- NULL If you want to find all of the positions of a certain custom unit that should be standard, try this code: ls &lt;- purrr::map(doc$dataset$dataTable[[i]]$attributeList$attribute, ~str_detect(.x$measurementScale$ratio$unit$customUnit, &quot;your standard unit&quot;)) which(ls == TRUE) "],["edit-datatables.html", "Edit dataTables", " Edit dataTables To edit a dataTable, first edit/create an attributeList and set the physical. Then create a new dataTable using the eml$dataTable() helper function as below: dataTable &lt;- eml$dataTable(entityName = &quot;A descriptive name for the data (does not need to be the same as the data file)&quot;, entityDescription = &quot;A description of the data&quot;, physical = physical, attributeList = attributeList) The dataTable must then be added to the EML. How exactly you do this will depend on whether there are dataTable elements in your EML, and how many there are. To replace whatever dataTable elements already exist, you could write: doc$dataset$dataTable &lt;- dataTable If there is only one dataTable in your dataset, the EML package will usually “unpack” these, so that it is not contained within a list of length 1 - this means that to add a second dataTable, you cannot use the syntax doc$dataset$dataTable[[2]], since when unpacked this will contain the entityDescription as opposed to pointing to the second in a series of dataTable elements. Confusing - I know. Not to fear though - this syntax will get you on your way, should you be trying to add a second dataTable. doc$dataset$dataTable &lt;- list(doc$dataset$dataTable, dataTable) If there is more than one dataTable in your dataset, you can return to the more straightforward construction of: doc$dataset$dataTable[[i]] &lt;- dataTable Where i is the index that you wish insert your dataTable into. To add a list of dataTables to avoid the unpacking problem above you will need to create a list of dataTables dts &lt;- list() # create an empty list for(i in seq_along(tables_you_need)){ # your code modifying/creating the dataTable here dataTable &lt;- eml$dataTable(entityName = dataTable$entityName, entityDescription = dataTable$entityDescription, physical = physical, attributeList = attributeList) dts[[i]] &lt;- dataTable # add to the list } After getting a list of dataTables, assign the resulting list to dataTable EML doc$dataset$dataTable &lt;- dts By default, the online submission form adds all entities as otherEntity, even when most should probably be dataTable. You can use eml_otherEntity_to_dataTable to easily move items in otherEntity over to dataTable. Most tabular data or data that contain variables should be listed as a dataTable. Data that do not contain variables (eg: plain text readme files, pdfs, jpegs) should be listed as otherEntity. eml_otherEntity_to_dataTable(doc, 1, # which otherEntities you want to convert, for multiple use - 1:5 validate_eml = F) # set this to False if the physical or attributes are not added "],["edit-otherentities.html", "Edit otherEntities", " Edit otherEntities Remove otherEntities To remove an otherEntity use the following command. This may be useful if a data object is originally listed as an otherEntity and then transferred to a dataTable. doc$dataset$otherEntity[[i]] &lt;- NULL Create otherEntities If you need to create/update an otherEntity, make sure to publish or update your data object first (if it is not already on the DataONE MN). Then build your otherEntity. otherEntity &lt;- arcticdatautils::pid_to_eml_entity(mn, pkg$data[[i]]) Alternatively, you can build the otherEntity of a data object not in your package by simply inputting the data PID. otherEntity &lt;- arcticdatautils::pid_to_eml_entity(mn, &quot;your_data_pid&quot;, entityType = &quot;otherEntity&quot;, entityName = &quot;Entity Name&quot;, entityDescription = &quot;Description about entity&quot;) The otherEntity must then be set to the EML, like so: doc$dataset$otherEntity &lt;- otherEntity If you have more than one otherEntity object in the EML already, you can add the new one like this: doc$dataset$otherEntity[[i]] &lt;- otherEntity Where i is set to the number of existing entities plus one. Remember the warning from the last section, however. If you only have one otherEntity, and you are trying to add another, you have to run: doc$dataset$otherEntity &lt;- list(otherEntity, doc$dataset$otherEntity) "],["semantic-annotations.html", "Semantic annotations", " Semantic annotations For a brief overview of what a semantic annotation is, and why we use them check out this video. Even more information on how to add semantic annotations to EML 2.2.0 can be found here. Currently metacatUI does not support the editing of semantic annotations on the website so all changes will have to be done in R. There are several elements in the EML 2.2.0 schema that can be annotated: dataset entity (eg: otherEntity or dataTable) attribute On the datateam, we will only be adding annotations to attributes for now. How annotations are used This is a dataset that has semantic annotations included. On the website you can see annotations in each of the attributes. You can click on any one of them to search for more datasets with that same annotation. Attribute-level annotations To add annotations to the attributeList you will need information about the propertyURI and valueURI Annotations are essentially composed of a sentence, which contains a subject (the attribute), predicate (propertyURI), and object (valueURI). Because of the way our search interface is built, for now we will be using attribute annotations that have a propertyURI label of “contains measurements of type”. Here is what an annotation for an attribute looks like in R. Note that both the propertyURI and valueURI have both a label, and the URI itself. doc$dataset$dataTable[[i]]$attributeList$attribute[[i]]$annotation $id [1] &quot;ODBcOyaTsg&quot; $propertyURI $propertyURI$label [1] &quot;contains measurements of type&quot; $propertyURI$propertyURI [1] &quot;http://ecoinformatics.org/oboe/oboe.1.2/oboe-core.owl#containsMeasurementsOfType&quot; $valueURI $valueURI$label [1] &quot;Distributed Biological Observatory region identifier&quot; $valueURI$valueURI [1] &quot;http://purl.dataone.org/odo/ECSO_00002617&quot; Semantic attribute annotations can be applied to spatialRasters, spatialVectors and dataTables How to add an annotation 1. Decide which variable to annotate The goal for the datateam is to start annotating every dataset that comes in. Please make sure to add semantic annotations to spatial and temporal features such as latitude, longitude, site name and date and aim to annotate as many attributes as possible. 2. Find an appropriate valueURI The next step is to find an appropriate value to fill in the blank of the sentence: “this attribute contains measurements of _____.” There are several ontologies to search in. In order of most to least likely to be relevant to the Arctic Data Center they are: The Ecosystem Ontology (ECSO) this was developed at NCEAS, and has many terms that are relevant to ecosystem processes, especially those involving carbon and nutrient cycling The Environment Ontology (EnVO) this is an ontology for the concise, controlled description of environments National Center for Biotechnology Information (NCBI) Organismal Classification (NCBITAXON) The NCBI Taxonomy Database is a curated classification and nomenclature for all of the organisms in the public sequence databases. Information Artifact Ontology (IAO) this ontology contains terms related to information entities (eg: journals, articles, datasets, identifiers) To search, navigate through the “classes” until you find an appropriate term. When we are picking terms, it is important that we not just pick a similar term or a term that seems close - we want a term that is totally “right”. For example, if you have an attribute for carbon tetroxide flux and an ontology with a class hierarchy like this: – carbon flux |—- carbon dioxide flux Our exact attribute, carbon tetroxide flux is not listed. In this case, we should pick “carbon flux” as it’s completely correct and not “carbon dioxide flux” because it’s more specific but not quite right. For general attributes (such as ones named depth or length), it is important to be as specific as possible about what is being measured. e.g. selecting the lake area annotation for the area attribute in this dataset 3. Build the annotation in R Manually Annotating this method is great for when you are inserting 1 annotation, fixing an existing annotation or programmatically updating annotations for multiple attributeLists First you need to figure out the index of the attribute you want to annotate. eml_get_simple(doc$dataset$dataTable[[3]]$attributeList, &quot;attributeName&quot;) [1] &quot;prdM&quot; &quot;t090C&quot; &quot;t190C&quot; &quot;c0mS/cm&quot; &quot;c1mS/cm&quot; &quot;sal00&quot; &quot;sal11&quot; &quot;sbeox0V&quot; &quot;flECO-AFL&quot; [10] &quot;CStarTr0&quot; &quot;cpar&quot; &quot;v0&quot; &quot;v4&quot; &quot;v6&quot; &quot;v7&quot; &quot;svCM&quot; &quot;altM&quot; &quot;depSM&quot; [19] &quot;scan&quot; &quot;sbeox0ML/L&quot; &quot;sbeox0dOV/dT&quot; &quot;flag&quot; Next, assign an id to the attribute. It should be unique within the document, and it’s nice if it is human readable and related to the attribute it is describing. One format you could use is entity_x_attribute_y which should be unique in scope, and is nice and descriptive. doc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$id &lt;- &quot;entity_ctd_attribute_salinity&quot; Now, assign the propertyURI information. This will be the same for every annotation you build. doc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$annotation$propertyURI &lt;- list(label = &quot;contains measurements of type&quot;, propertyURI = &quot;http://ecoinformatics.org/oboe/oboe.1.2/oboe-core.owl#containsMeasurementsOfType&quot;) Finally, add the valueURI information from your search. You should see an ID on the Bioportal page that looks like a URL - this is the valueURI. Use the value to populate the label element. doc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$annotation$valueURI &lt;- list(label = &quot;Water Salinity&quot;, valueURI = &quot;http://purl.dataone.org/odo/ECSO_00001164&quot;) Shiny Attributes this method is great for when you are updating many attributes On the far right of the table of shiny_attributes there are 4 columns: id, propertyURI, propertyLabel, valueURI, valueLabel that can be filled out. "],["edit-spatial-data.html", "Edit spatial data", " Edit spatial data Occasionally, you may encounter a third type of data object: spatialVector and spatialRaster. These objects contains spatial data (ie maps), such as a shapefile or geodatabase. Editing a spatialVector or spatialRaster is similar to editing a dataTable or an otherEntity. A physical and attributeList should be present. We will focus on how to get the information unique to spatialData and how to create the spatialVector/spatialRaster File types File extensions to look for that might be spatial data: kml, geoJSON, geoTIFF, .dbf, .shp, and .shx Additionally, spatial data that involve multiple files should typically be archived within a .zip file to ensure all related and interdependent files stay together (ie . a geodatabase). This is one of the exceptions to our rule regarding .zip files. For example, a spatial dataset for a shapefile should, at a minimum, consist of separate .dbf, .shp, and .shx files with the same prefix in the same directory. All these files are required in order to use the data. Also note that shapefiles limit attribute names to 10 characters, so attribute names in the metadata may not match exactly to attribute names in the data. Some spatial raster data come as standalone files (.tiff or .nc) and some come as a group of files. If you aren’t sure whether to unzip a file, ask Jasmine or Jeanette. There are specific formatIds for these kinds of zipped files: application/vnd.shp+zip image/geotiff+zip. Remember to check that the files have the correct formatId Reading Spatial Files Read in the files to (1) help you in creating your attributes table and (2) sometimes also figure out the coordinate reference system. library(sf) spatial_file &lt;- sf::read_sf(&quot;example.kml&quot;) When you read kml files, read_sf() sometimes shows additional columns that aren’t in the actual file. Always open kml files in text editor to check if the columns actually exist. If it is a zipped shapefile there is a handy function you can use arcticdatautils::read_zip_shapefile(mn, pid) Coordinate Systems The coordinate system allow to work with spatial data using the same frame of reference (a Datum). A common coordinate system is “GCS_WGS_1984 (used in Google Maps!) which is suitable for plotting points distributed globally. There are many others that may be better suited for certain areas in the world. All latitudes and longitude coordinates should have a coordinate system (like a frame of reference). There are horizontal coordinate systems (earth’s surface) and vertical coordinate systems (depth). More information can be found here. To find the horizCoordSysName you can use: sf::st_crs(spatial_file) Take the Datum and add GCS (Geographic Coordinate System) in front. For example: “GCS_WGS_1984” spatialVector Adding Geometry One important difference is that a spatialVector object should also have a geometry slot that describes the geometry features of the data. The possible values include one or more (in a list) of ‘Point’, ‘LineString’, ‘LinearRing’, ‘Polygon’, ‘MultiPoint’, ‘MultiLineString’, ‘MultiPolygon’, or ‘MultiGeometry’. You will likely have to open the file itself within QGIS or R (ie . the sf package) to get the correct geometry value. To add just a geometry slot use: doc$dataset$spatialVector[[1]]$geometry &lt;- &quot;Polygon&quot; To add it using the data pid: 1. Get the geometry and spatialReference 2. Use pid_to_eml_entity() to generate the spatialVector spatialVector &lt;- pid_to_eml_entity(adc, pkg$data[n], entity_type = &quot;spatialVector&quot;, entityName = &quot;filename.kml&quot;, entityDescription = &quot;some desciption&quot;, attributeList = attributeList, geometry = &quot;Point&quot;, spatialReference = list(horizCoordSysName = &quot;GCS_WGS_1984&quot;)) Add the spatialVector to the doc doc$dataset$spatialVector[[1]] &lt;- spatialVector spatialRasters Most often these come in GeoTiff or Tiff files. The data is presented as a grid of “pixels”. For more information ESRI has a indepth article here. To use the helper function get: the path of your raster file an attribute table a coordinate system To get a coordinate system name, you can use the output of the function on your first try (which will print the coordinate reference system, if it is defined). You can use the return value of get_coord_list() (a large data.frame) to find the correct coordinate system name. Another way to get the coordinate system name is using rgdal::GDALinfo(path). This function can provide many details for your GeoTiff or Tiff files including the coordinate system name. More information can be found here here. rgdal::GDALinfo(path) eml_get_raster_metadata(path, coord_name, attributes) "],["format-text-in-eml.html", "Format text in EML", " Format text in EML Currently, only certain fields (abstracts, methods) support text formatting in EML. Check out this demo for a full example. Additional info is also available here. Many of these formatting functions only work when enclosed by &lt;para&gt;&lt;/para&gt; You can insert these tags directly into the EML document using syntax that looks like this: doc$dataset$abstract &lt;- eml$abstract(para = &quot;Some abstracts require subscripts like CO&lt;subscript&gt;2&lt;/subscript&gt;&quot;) Type-setting Subscripts, superscripts, and italics: &lt;subscript&gt;You can do subscripts&lt;/subscript&gt; &lt;superscript&gt;or superscipts&lt;/superscript&gt; &lt;emphasis&gt;or even italics.&lt;/emphasis&gt; Links Be sure to include the “https://” before the link or it will redirect incorrectly. Also, always check that your links go through to the correct page. Please be aware that most links are inherently unstable so always default to archiving files over pointing to websites when possible and appropriate. &lt;ulink url=&quot;https://some_url.com&quot;&gt; &lt;citetitle&gt;some text&lt;/citetitle&gt; &lt;/ulink&gt; Lists Unordered (bulleted) lists: &lt;itemizedlist&gt; &lt;listitem&gt; &lt;para&gt;Paragraphs&lt;/para&gt; &lt;/listitem&gt; &lt;listitem&gt; &lt;para&gt;Sections w/ subsections (w/ titles)&lt;/para&gt; &lt;/listitem&gt; &lt;/itemizedlist&gt; Ordered lists (1, 2, 3)… &lt;orderedlist&gt; &lt;listitem&gt; &lt;para&gt;something&lt;/para&gt; &lt;/listitem&gt; &lt;listitem&gt; &lt;para&gt;something else&lt;/para&gt; &lt;/listitem&gt; &lt;/orderedlist&gt; "],["series-identifier-sid.html", "Series Identifier (SID)", " Series Identifier (SID) A series identifier is a system metadata field that represents a single identifier across multiple versions of a data package, a feature often requested by submitters. These are useful for maintaining a single identifier when a data package is expected to receive updates (usually additional data) in the future. Adding a SID to a package Adding a series identifier should be the last step. In most cases the SID will be in DOI format. Once the data package is complete (peer-reviewd and approved by the PI) the series identifier can be added by updating the seriesId field of the system metadata of the metadata object. # The metadata identifier we&#39;re assigning an SID to metadata_pid &lt;- &#39;metadata_pid&#39; sys &lt;- getSystemMetadata(mn, metadata_pid) sys@seriesId &lt;- generateIdentifier(mn, scheme = &#39;DOI&#39;) #update the scheme argument if it should not be a DOI updateSystemMetadata(mn, metadata_pid, sys) Updating the child packages of a parent package with a SID When updating a parent package with a series identifier you need to use update_resource_map rather than publish_update. In this case our goal is add nested data packages to an existing parent package. # Call get_package on the parent package that has a `seriesId` in its system metadata parent_package &lt;- get_package(mn, &#39;resource_map_pid&#39;) # The resource map of the package we want to add to the &#39;child_pids&#39; of the parent resource_map_new_child &lt;- &#39;&#39; update_resource_map(mn, parent_package$resource_map, parent_package$metadata, parent_package$data, c(parent_package$child_packages, resource_map_new_child)) Updating the metadata of a package with a SID When updating the metadata (xml) of a package with a series identifier use publish_update. It’s important that you pass the metadata pid to the metadata_pid argument rather than seriesId. The metadata pid will usually have the text “version:” ahead of it, however it’s best to use get_package first to avoid mistakes. pkg &lt;- get_package(mn, &#39;resource_map_pid&#39;) pkg &lt;- publish_update(mn, pkg$metadata, pkg$resource_map, pkg$data, child_pids = pkg$child_pids) # + any additional arguments to &quot;publish_update&quot; "],["set-coverages.html", "Set coverages", " Set coverages Sometimes EML documents may lack coverage information describing the temporal, geographic, or taxonomic coverage of a data set. This example shows how to create coverage information from scratch, or replace an existing coverage element with an updated one. You can view the current coverage (if it exists) by entering doc$dataset$coverage into the console. Here the coverage, including temporal, taxonomic, and geographic coverages, is defined using set_coverage(). coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;), geographicDescription = &quot;The geographic region covers the lake region near Eagle Mountain, Alaska.&quot;, west = -154.6192, east = -154.5753, north = 68.3831, south = 68.3619) doc$dataset$coverage &lt;- coverage Set multiple coverages You can also set multiple geographic (or temporal) coverages. Here is an example of how you might set two geographic coverages. Note that we use nested eml function helpers in this construction. geocov1 &lt;- eml$geographicCoverage(geographicDescription = &quot;The geographich region covers area 1&quot;, boundingCoordinates = eml$boundingCoordinates( northBoundingCoordinate = 68, eastBoundingCoordinate = -154, southBoundingCoordinate = 67, westBoundingCoordinate = -155)) geocov2 &lt;- eml$geographicCoverage(geographicDescription = &quot;The geographich region covers area 2&quot;, boundingCoordinates = eml$boundingCoordinates( northBoundingCoordinate = 65, eastBoundingCoordinate = -151, southBoundingCoordinate = 62, westBoundingCoordinate = -153)) coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = list(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;)) doc$dataset$coverage$geographicCoverage &lt;- list(geocov1, geocov2) Special coverages Arctic Circle For arctic circle geographic coverage, we only have the starting vertical line of the circle shown in the projection. Here is an example with arctic circle geographic coverage. Geologic dates Example dataset with geologic coverages set using the following: geo_time_start &lt;- EML::eml$alternativeTimeScale(timeScaleName = &quot;Absolute&quot;, timeScaleAgeEstimate = &quot;7.5 Myr&quot;) coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, doc$dataset$coverage &lt;- coverage "],["set-methods.html", "Set methods", " Set methods The methods tree in the EML section has many different options, visible in the schema. You can create new elements in the methods tree by following the schema and using the eml helpers. Another simple and potentially useful way to add methods to an EML that has no methods at all is by adding them via a MS Word document. An example is shown below: methods1 &lt;- set_methods(&#39;methods_doc.docx&#39;) doc$dataset$methods &lt;- methods1 If you want to make minor changes to existing methods information that has a lot of nested elements, your best bet may be to edit the EML manually in a text editor (or in RStudio), otherwise there is a risk of accidentally overwriting nested elements with blank object classes, therefore losing methods information. Adding sampling info to methods section # add method steps as new variables step1 &lt;- eml$methodStep(description = &quot;text describing the methods used&quot;) stEx &lt;- eml$studyExtent(description = &quot;study extent description&quot;) samp &lt;- eml$sampling(studyExtent = stEx, samplingDescription = &quot;sampling description text&quot;) # combine all methods steps and sampling info methods1 &lt;- eml$methods(methodStep = step1, sampling = samp) doc$dataset$methods &lt;- methods1 "],["set-parties.html", "Set parties", " Set parties The address, creator, contact, and associatedParty classes can easily be created using functions from the EML package. However it is often easier to just edit this through the webform. To add people, with their addresses, you need to add addresses as their own object class, which you then add to the contact, creator, or associatedParty classes. NCEASadd &lt;- eml$address( deliveryPoint = &quot;735 State St #300&quot;, city = &quot;Santa Barbara&quot;, administrativeArea = &#39;CA&#39;, postalCode = &#39;93101&#39;) Here, we use eml_creator() to set our data set creator. JC_creator &lt;- eml$creator(individualName = list(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;), organization = &quot;NCEAS&quot;, electronicMailAddress = &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) doc$dataset$creator &lt;- JC_creator Similarly, we can set a single contact or multiple. In this case, there are two, so we set doc$dataset$contact as a list containing both of them. JC_contact &lt;- eml$contact(individualName = list(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;), organization = &quot;NCEAS&quot;, electronicMailAddress = &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;,id = eml$userId(directory = eml$dir) address = NCEASadd) JG_contact &lt;- eml$contact(individualName = list(givenName = &quot;Jesse&quot;, surName = &quot;Goldstein&quot;), organization = &quot;NCEAS&quot;, electronicMailAddress = &quot;jgoldstein@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) doc$dataset$contact &lt;- list(JC_contact, JG_contact) Finally, the associatedPartys are set. Note that associatedPartys MUST have a role defined, unlike creator or contact. JG_ap &lt;- eml$associatedParty(individualName = list(givenName = &quot;Jesse&quot;, surName = &quot;Goldstein&quot;), organization = &quot;NCEAS&quot;, email = &quot;jgoldstein@nceas.ucsb.edu&quot;, electronicMailAddress = &quot;123-456-7890&quot;, address = NCEASadd, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, role = &quot;metadataProvider&quot;) doc$dataset$associatedParty &lt;- JG_ap "],["set-physical.html", "Set physical", " Set physical To set the physical aspects of a data object, use the following commands to build a physical object from a data PID that exists in your package. Remember to set the member node to test.arcticdata.io! physical &lt;- arcticdatautils::pid_to_eml_physical(mn, pkg$data[[i]]) Alternatively, you can set the physical of a data object not yet in your package by simply inputting the data PID: physical &lt;- arcticdatautils::pid_to_eml_physical(mn, &quot;your_data_pid&quot;) The physical must then be assigned to the data object. A final, but not recommended option, is to set the physical by hand. To do so, one can use a workflow similar to the one below. However, the far superior workflow is to publish or update your data first and then use pid_to_eml_physical() to set the physical. id &lt;- &#39;your_data_pid&#39; # this should be an actual PID path &lt;- &#39;~/your/data/path&#39; # path to data table physical &lt;- set_physical(objectName = path, size = as.character(file.size(path)), sizeUnit = &#39;bytes&#39;, authentication = digest(fpath, algo=&quot;sha256&quot;, serialize=FALSE, file=TRUE), authMethod = &#39;SHA-256&#39;, numHeaderLines = &#39;1&#39;, fieldDelimiter = &#39;,&#39;, url = paste0(&#39;https://cn.dataone.org/cn/v2/resolve/&#39;, id)) "],["set-the-project-section.html", "Set the project section", " Set the project section The project section in an EML document is automatically filled out by the metacatUI editor. It sets the project title and project personnel to the submission’s title and creators. Most of the time at least some of this information is incorrect and we need to update it. Start by searching for the funding information using NSF’s award search. This will give us the project title, abstract, and personnel - along with some additional metadata. Using this information we will set the title, personnel, and funding number. For NSF funded projects prepend the funding number with “NSF”. If there are multiple awards associated with one dataset then additional funding, title, and personnel elements should be added to reflect the additional awards. doc$dataset$project$funding$para[[1]] &lt;- &#39;NSF 1503846&#39; doc$dataset$project$title[[1]] &lt;- &#39;Collaborative Research: Reconciling conflicting Arctic temperature and fire reconstructions using multi-proxy records from lake sediments north of the Brooks Range, Alaska doc$dataset$project$personnel[[1]] &lt;- eml$personnel(individualName = eml$individualName(givenName = &#39;Yongsong&#39;, surName = &#39;Huang&#39;), role = &#39;Principal Investigator&#39;) doc$dataset$project$personnel[[2]] &lt;- eml$personnel(individualName = eml$individualName(givenName = &#39;James&#39;, surName = &#39;Russell&#39;), role = &#39;co Principal Investigator&#39;) There is also a helper function eml_nsf_to_project() that can help do the searching for you. Just verify that the information retrieved is correct. # update NSF awards data awards &lt;- c(&quot;1311655&quot;, &quot;1417987&quot;, &quot;1417993&quot;) # list of award numbers proj &lt;- eml_nsf_to_project(awards) #helper function doc$dataset$project &lt;- proj On the web form right now only fills in the funding field. We need instructions on how to convert that to the new awards formatting until the form is updated to include these fields: project$award$funderName # required project$award$title # required project$award$awardNumber project$award$funderIdentifier Information can be found in using the Open Funder Registry. If a award title cannot be found you can use the dataset title. "],["use-references.html", "Use references", " Use references References are a way to avoid repeating the same information multiple times in the same EML record. There are a few benefits to doing this, including: Making it clear that two things are the same (e.g., the creator is the same person as the contact, two entities have the exact same attributes) Reducing the size on disk of EML records with highly redundant information Faster read/write/validate with the R EML package You may want to use EML references if you have the following scenarios (not exhaustive): One person has multiple roles in the dataset (creator, contact, etc) One or more entities shares all or some attributes Example with parties Do not use references for creators as it is used for the citation information. The creators will not show up on the top of the dataset if it is a reference. Until this issue is resolved in NCEAS/metacat#926 we will need to keep this in account. It’s very common to see the contact and creator referring to the same person with XML like this: &lt;eml packageId=&quot;my_test_doc&quot; system=&quot;my_system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1 eml.xsd&quot;&gt; &lt;dataset&gt; &lt;creator&gt; &lt;individualName&gt; &lt;givenName&gt;Bryce&lt;/givenName&gt; &lt;surName&gt;Mecum&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; &lt;contact&gt; &lt;individualName&gt; &lt;givenName&gt;Bryce&lt;/givenName&gt; &lt;surName&gt;Mecum&lt;/surName&gt; &lt;/individualName&gt; &lt;/contact&gt; &lt;/dataset&gt; &lt;/eml&gt; So you see those two times Bryce Mecum is referenced there? If you mean to state that Bryce Mecum is the creator and contact for the dataset, this is a good start. But with just a name, there’s some ambiguity as to whether the creator and contact are truly the same person. Using references, we can remove all doubt. doc$dataset$creator[[1]]$id &lt;- &quot;reference_id&quot; doc$dataset$contact &lt;- list(references = &quot;reference_id&quot;) print(doc) &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:stmml=&quot;http://www.xml-cml.org/schema/stmml-1.1&quot; packageId=&quot;id&quot; system=&quot;system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1/ eml.xsd&quot;&gt; &lt;dataset&gt; &lt;title&gt;A Minimal Valid EML Dataset&lt;/title&gt; &lt;creator id=&quot;reference_id&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Bryce&lt;/givenName&gt; &lt;surName&gt;Mecum&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; &lt;contact&gt; &lt;references&gt;reference_id&lt;/references&gt; &lt;/contact&gt; &lt;/dataset&gt; &lt;/eml:eml&gt; The reference id needs to be unique within the EML record but doesn’t need to have meaning outside of that. Example with attributes To use references with attributes: Add an attribute list to a data table Add a reference id for that attribute list Use references to add that information into the attributeLists of the other data tables For example, if all the data tables in our data package have the same attributes, we can set the attribute list for the first one, and use references for the rest: doc$dataset$dataTable[[1]]$attributeList &lt;- attribute_list doc$dataset$dataTable[[1]]$attributeList$id &lt;- &quot;shared_attributes&quot; # use any unique name for your id for (i in 2:length(doc$dataset$dataTable)) { doc$dataset$dataTable[[i]]$attributeList &lt;- list(references = &quot;shared_attributes&quot;) # use the id you set above } "],["pi-correspondence.html", "PI correspondence ", " PI correspondence "],["email-templates.html", "Email templates", " Email templates Please think critically when using these canned replies rather than just blindly sending them. Typically, content should be adjusted/ customized for each response to be as relevant, complete, and precise as possible. In your first few months, please run email drafts by the #datateam Slack and get approval before sending. Remember to consult the submission guidelines for details of what is expected. Quick reference: Initial email template Final email templates Additional email template Initial email template Hello [NAME OF REQUESTOR], Thank you for your recent submission to the NSF Arctic Data Center! From my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below: [COMMENTS HERE] After we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface. Best, [YOUR NAME] Comment templates based on what is missing Portals Multiple datasets under the same project - suggest data portal feature I would like to highlight the Data Portals feature that would enhance the ability of your datasets to be discovered together. It also enables some custom branding for the group or project. Here is an example page that we created for the Distributed Biological Observatory: https://arcticdata.io/catalog/portals/DBO. We highly suggest considering creating a portal for your datasets, you can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/. If they ask to nest the dataset We are no longer supporting new nested datasets. We recommend to create a data portal instead. Portals will allow more control and has the same functionality as a nested dataset. You can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/, but as always we are here to help over email. Dataset citations If there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. Title Provides the what, where, and when of the data We would like to add some more context to your data package title. I would like to suggest: ‘OUR SUGGESTION HERE, WHERE, WHEN’. Does not use acronyms We wanted to clarify a couple of abbreviations. Could you help us in defining some of these: [LIST OF ACRONYMS TO DEFINE HERE] Abstract Describes DATA in package (ideally &gt; 100 words) We would like to add some additional context to your abstract. We hope to add the following: [ADJUST THE DEPENDING ON WHAT IS MISSING] - The motivation of the study - Where and when the research took place - At least one sentence summarizing general methodologies - All acronyms are defined - At least 100 words long Offer this if submitter is reluctant to change: If you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [NSF AWARD URL]. Keywords We noticed that there were not keywords for this dataset. Adding keywords will help your dataset be discovered by others. Data Sensitive Data Example response. Tailor this reponse to the format in question. We wanted to check with you to make sure there is no sensitive data that could potentially be de-anonymized (eg. would there be small enough populations where the data could be disaggregated). Adding provenance Is the [mention the file names here] files related? If so we can add provenance to show the relationship between the files. Here is an example of how that is displayed: https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2WS8HM6C#urn%3Auuid%3Af00c4d71-0242-4e9d-9745-8999776fa2f2 At least one data file We noticed that no data files were submitted. With the exception of sensitive social science data, we seek to include all data products prior to publication. We wanted to check if additional files will be submitted before we move forward with the submission process. Open formats Example using xlsx. Tailor this reponse to the format in question. We noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term. Zip files Except for very specific file types, we do not recommend that data are archived in zip format. Data that are zipped together can present a barrier to reuse since it is difficult to accurately document each file within the zip file in a machine readable way. File contents and relationships among files are clear Could you provide a short description of the files submitted? Information about how each file was generated (what software, source files, etc.) will help us create more robust metadata for long term use. Data layout Would you be able to clarify how the data in your files is laid out? Specifically, what do the rows and columns represent? We try not to prescribe a way the researchers must format their data as long as reasonable. However, in extreme cases (for example Excel spreadsheets with data and charts all in one sheet) we will want to kindly ask them to reformat. We would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. [DESCRIBE WHAT MAY NEED TO BE CHANGED IN THE DATA SET]. Our data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help. Attributes Identify which attributes need additional information. If they are common attributes like date and time we do not need further clarification. Checklist for the datateam in reviewing attributes (NetCDF, CSV, shapefiles, or any other tabular datasets): A name (often the column or row header in the file). A complete definition. Any missing value codes along with explanations for those codes. For all numeric data, unit information is needed. For all date-time data, a date-time format is needed (e.g. “DD-MM-YYYY”). For text data, full descriptions for all patterns or code/definition pairs are needed if the text is constrained to a list of patterns or codes. Helpful templates: &gt; We would like your help in defining some of the attributes. Could you write a short description or units for the attributes listed? [Provide a the attribute names in list form] &gt; Could you describe ____? &gt; Please define “XYZ”, including the unit of measure. &gt; What are the units of measurement for the columns labeled “ABC” and “XYZ”? Missing value codes What do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice. We noticed that the data files contain [blank cells - replace with missing values found]. What do these represent? Funding All NSF funded datasets need a funding number. Non-NSF funded datasets might not have funding numbers, depending on the funding organization. We noticed that your dataset does not appear to contain a funding number. The field accepts NSF funding numbers as well as other numbers by different organizations. Methods We noticed that methods were missing from the submission. Submissions should: provide instrument names (if applicable) specify how sampling locations were chosen provide citations for sampling methods that are not explained in detail any software used to process the data Note - this includes software submissions as well (see https://arcticdata.io/submit/#metadata-guidelines-for-software) Your methods section appears to be missing some information. [ADJUST THIS DEPENDING ON WHAT IS MISSING - Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files.] Comprehensive methods information should be included directly in the metadata record. Pointers or URLs to other sites are unstable. Final email templates Asking for approval Hi [submitter], I have updated your data package and you can view it here after logging in: [URL] Please review and approve it for publishing or let us know if you would like anything else changed. For your convenience, if we do not hear from you within a week we will proceed with publishing with a DOI. After publishing with a DOI, any further changes to the dataset will result in a new DOI. However, any previous DOIs will still resolve and point the user to the newest version. If you would like to maintain the same DOI please let us know. Please let us know if you have any questions. DOI and data package finalization comments Replying to questions about DOIs We attribute DOIs to data packages as one might give a DOI to a citable publication. Thus, a DOI is permanently associated with a unique and immutable version of a data package. If the data package changes, a new DOI will be created and the old DOI will be preserved with the original version. DOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to that latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists. Clarification of updating with a DOI and version control We definitely support updating a data package that has already been assigned a DOI, but when we do so we mark it as a new revision that replaces the original and give it its own DOI. We do that so that any citations of the original version of the data package remain valid (i.e.: after the update, people still know exactly which data were used in the work citing it). Sending finalized URL before resolving ticket [NOTE: the URL format is very specific here, please try to follow it exactly (but substitute in the actual DOI of interest)] Here is the link to your finalized data package: https://doi.org/10.18739/A20X0X If in the future there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. Please let us know if you need any further assistance. New Submission: Abstract, methods, excel to csv, and attributes Thank you for your submission to the Arctic Data Center. From my preliminary examination of your dataset a few fields need to be updated before we can assign a DOI. Your abstract appears to be missing some information. We suggest that the abstract be sufficiently descriptive for a general scientific audience. It should provide an overview of the scientific context/ project/ hypotheses, how this data package fits into the larger project, a synopsis of the experimental or sampling design, and a summary of the data contents. If you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [INSERT URL] Your methods section appears to be missing some information. Enough detail should be included so that a reasonable scientist can interpret the study and data for reuse without consulting you nor any other resources. This should hold true today, or even decades from now. Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files. Please edit the data package to provide a more robust methods section. Comprehensive methods information should be included directly in the metadata record. Pointers or URLs to other sites are unstable and insufficient. Your submitted data files are in excel format. Please convert your files to a plain text/csv or another open (non-proprietary) format in order to facilitate an accurate transfer of information to users and to ensure preservation of the data in perpetuity. We can keep your excel files in the dataset, however we recommend at least one version of each file be stored in an open format. Please add attribute (column) metadata for your data files. If any files share the same attributes you only need to enter them once and we will replicate them to the files. You can add this by navigating to your dataset [INSERT URL], clicking the green “Edit” button, clicking the “Describe” button located to the right of each of your files at the top of the submission page, and in the window that pops up click over to the “Attributes” tab. Additional email templates Deadlines If the PI is checking about dates/timing: &gt; [give rough estimate of time it might take] &gt; Are you facing any deadlines? If so, we may be able to expedite publication of your submission. Pre-assigned DOI If the PI needs a DOI right away: We can provide you with a pre-assigned DOI that you can reference in your paper, as long as your submission is not facing a deadline from NSF for your final report. However, please note that it will not become active until after we have finished processing your submission and the package is published. Once you have your dataset published, we would appreciate it if you could register the DOI of your published paper with us by using the citations button beside the orange lock icon. We are working to build our catalog of dataset citations in the Arctic Data Center. Asking for dataset access As a security measure we ask that we get the approval from the original submitter of the dataset prior to granting edit permissions to all datasets. Replacing data files The data files can be replaced by going to the green Edit button &gt; Click the black triangle by the Describe button for the data file &gt; Select Replace (attached is also a screenshot on how to get there). Recovering Dataset submissions To recover dataset submissions that were not successful please do the following: Go to https://arcticdata.io/catalog/drafts Find your dataset and download the corresponding file Send us the file in an email Custom Search Link You could also use a permalink like this to direct users to the datasets: https://arcticdata.io/catalog/data/query=\"your search query here” for example: https://arcticdata.io/catalog/data/query=Beaufort%20Lagoon%20Ecosystems%20LTER Adding metadata via R KNB does not support direct uploading of EML metadata files through the website (we have a webform that creates metadata), but you can upload your data and metadata through R. Here are some training materials we have that use both the EML and datapack packages. It explains how to set your authentication token, build a package from metadata and data files, and publish the package to one of our test sites. I definitely recommend practicing on a test site prior to publishing to the production site your first time through. You can point to the KNB test node (dev.nceas.ucsb.edu) using this command: d1c &lt;- D1Client(\"STAGING2\", \"urn:node:mnTestKNB\") If you prefer, there are Java, Python, MATLAB, and Bash/cURL clients as well. Finding multiple data packages If linking to multiple data packages, you can send a link to the profile associated with the submitter’s ORCID iD and it will display all their data packages. e.g.: https://arcticdata.io/catalog/profile/http://orcid.org/0000-0002-2604-4533 NSF ARC data submission policy Please find an overview of our submission guidelines here: https://arcticdata.io/submit/, and NSF Office of Polar Programs policy information here: https://www.nsf.gov/pubs/2016/nsf16055/nsf16055.jsp. Investigators should upload their data to the Arctic Data Center (https://arcticdata.io), or, where appropriate, to another community endorsed data archive that ensures the longevity, interpretation, public accessibility, and preservation of the data (e.g., GenBank, NCEI). Local and university web pages generally are not sufficient as an archive. Data preservation should be part of the institutional mission and data must remain accessible even if funding for the archive wanes (i.e., succession plans are in place). We would be happy to discuss the suitability of various archival locations with you further. In order to provide a central location for discovery of ARC-funded data, a metadata record must always be uploaded to the Arctic Data Center even when another community archive is used. Linking ORCiD and LDAP accounts First create an account at orcid.org/register if you have not already. After that account registration is complete, login to the KNB with your ORCID iD here: https://knb.ecoinformatics.org/#share. Next, hover over the icon on the top right and choose “My Profile”. Then, click the “Settings” tab and scroll down to “Add Another Account”. Enter your name or username from your Morpho account and select yourself (your name should populate as an option). Click the “+”. You will then need to log out of knb.ecoinformatics.org and then log back in with your old LDAP account (click “have an existing account”, and enter your Morpho credentials with the organization set to “unaffiliated”) to finalize the linkage between the two accounts. Navigate to “My Profile” and “Settings” to confirm the linkage. After completing this, all of your previously submitted data pacakges should show up on your KNB “My Profile” page, whether you are logged in using your ORCiD or Morpho account, and you will be able to submit data either using Morpho or our web interface. Or, try reversing my instructions - log in first using your Morpho account (by clicking the “existing account” button and selecting organization “unaffiliated”), look for your ORCiD account, then log out and back in with ORCiD to confirm the linkage. "],["pi-faq-email-templates.html", "PI FAQ email templates", " PI FAQ email templates Data Q: Can I replace data that have already been uploaded and keep the DOI? A: Once you have published your data with the Arctic Data Center, it can still be updated by providing an additional version which can replace the original, while still preserving the original and making it available to anyone who might have cited it. To update your data, return to the data submission tool used to submit it, and provide an update. Any update to a data package qualifies as a new version and therefore requires a new DOI. This is because each DOI represents a unique, immutable version, just like for a journal article. DOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to the latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists. Q: Why don’t I see my data package on the ADC? Possible Answer #1: The data package is still private because we are processing it or awaiting your approval to publish it. Please login with your ORCID iD to view private data packages. Possible Answer #2: The data package is still private and you do not have access because you were not the submitter. If you need access please have the submitter send us a message from his/her email address confirming this, along with your ORCID iD. Once we receive that confirmation we will be happy to grant you permission to view and edit the data package. Possible Answer #3: The data package is still private and we accidentally failed to grant you access. We apologize for the mistake. We have since updated the access policy. Please let us know if you are still having trouble viewing the data package here: [URL]. Remember to login with your ORCID iD. Issue: MANY files to upload (100s or 1000s) or large cumulative size. A: Prior to accepting large uploads, take the time to make sure there is enough space available and uploading all the files is necessary. Can you upload the files to a drive we can access, such as Google Drive or Dropbox? Alternatively, if you have a publicly accessible FTP you can point us to, we could grab the files from there. If needed, we have a secure FTP you can access. Details are available here: https://help.nceas.ucsb.edu/remote_file_access . Please access our server at datateam.nceas.ucsb.edu with the username “visitor”. Let us know if you would like to use our SFTP and we will send you the password and the path to which directory to upload to. If you have files to transfer to us that total several terabytes it may be best to arrange a shipment of an external hard drive. Q: Can I add another data file to an existing submission without having to fill out another metadata form? A: Yes. Navigate to the data package after being sure to login. Then click the green “Edit” button. The form will populate with the already existing metadata so there is no need to fill it out again. Click “Add Files” and browse to the file you wish to add. Be aware that the DOI will change after you add this file (or make any changes to a data package) as, just like for a journal article, a DOI represents a unique and immutable version. The original URL and DOI will remain functional and valid, but clearly display a message at the top of that page stating that “A newer version of this dataset exists” with a link to the latest version. Only the newest version wil be discoverable via a search. Q: Can we submit data as an Excel file? A: While the Arctic Data Center supports the upload of any data file format, sharing data can be greatly enhanced if you use ubiquitous, easy-to-read formats. For instance, while Microsoft Excel files are commonplace, it is better to export these spreadsheets to Comma Separated Values (CSV) text files, which can be read on any computer without needing to have Microsoft products installed. So, yes, you are free to submit an Excel workbook, however we strongly recommend converting each sheet to a CSV. The goal is not only for users to be able to read data files, but to be able to analyze them with software, such as R Studio. Typically, we would extract any plots and include them as separate image files. [ONLY SAY THIS NEXT PART IF THE REQUESTOR CONTINUES TO INSIST and then USE PROV TO POINT FROM THE XLS TO THE CSVs] I understand that having the plots in the same file as the data they are built from simplifies organization. If you definitely prefer to have the Excel workbook included, we ask that you allow us to document the data in both formats and include a note in the metadata clarifying that the data are indeed duplicated (but in different formats). Q: Can we submit model output? A: Yes you can submit your model output to the Arctic Data Center. We also recommend including model source code directly in the dataset as it is critical to understand the model output dataset and will allow other researchers to re-produce your output. If the model is complex or it’s important to maintain file structure then it can submitted as a zip file. We also recommend included a detailed readme file that documents: version information, licensing information, a list of software/hardware used in development, a list of software/hardware dependencies needed to run the software, information detailing source data for models, any mathematical/physical explanations needed to understand models, any methods used to evaluate models, and instructions how to run the model if not including in the source code as comments. Please read our software submission guidelines for more detailed information: https://arcticdata.io/submit/#metadata-guidelines-for-software Access Q: May another person (e.g. my student) submit data using my ORCID iD so that it is linked to me? A: We recommend instead that the student set up their own ORCiD account at https://ORCiD.org/register and submit data packages from that account. Submissions are processed by our team and, at that point, we can grant you full rights to the metadata and data files even though another person submitted them. Issue: Web form not cooperating. A: To help us diagnose the problem, could you let us know the following: Which operating system (including the version) and browser (with version #) are you using? At which exact step did the issue arise? What error message did you receive? Do you have any reason to believe that you may be using a slow internet connection? Please provide us with any relevant screenshots and we will troubleshoot from there. Issue: Can’t log in to the Arctic Data Center A: Please accept our apologies that you are experiencing difficulty logging in. We suggest trying these troubleshooting steps: Try a different internet browser (Chrome, Firefox, Internet Explorer etc.) Try clearing your cache and then re-logging in with orcid Id Privacy blockers (like EFF’s privacy badger https://www.eff.org/privacybadger) block the cookies needed for the login to work. To enable the cookies you need to disable your privacy tracker for arcticdata.io. If the problem persists please let us know what step the error occured or if there were error messages you have recieved. non-NSF Issue: Can I submit a non-NSF funded dataset? The Arctic Data Center is open to all arctic-related data. For larger data packages, we would likely need to charge a one-time archival fee which amortizes the long-term costs of preservation in a single payment. Please let us know the size of your dataset in terms of total number and total size of all files. We look forward to receiving your submission! Q: May I submit a non-NSF-funded data package? A: Yes, you can submit non-NSF-funded Arctic data if you are willing to submit under the licensing terms of the Arctic Data Center (CC-0 or CC-BY), the data are moderately sized (with exact limits open to discussion), and a lot of support time to curate the submission is not required (i.e., you submit a complete metadata record and well formatted, open format data files). For larger data packages, we would likely need to charge a one-time archival fee which amortizes the long-term costs of preservation in a single payment. Also, please note that NSF-funded projects take priority when it comes to processing. Information on best practices for data and metadata organization is available here: https://arcticdata.io/submit/#organizing-your-data. "],["good-examples-of-data-packages.html", "Good Examples of data packages", " Good Examples of data packages Modeling Model with output: https://doi.org/10.18739/A24J09X55 Model code archived as a zip if preverving directory structure is important https://doi.org/10.18739/A2JS9H795 Model with provenance: https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2WS8HM6C#urn%3Auuid%3Ae6390181-85e9-46a2-b2ba-e352ece51cc6 Code and output: Note: We can only archive the code if the submitter has the rights to redistribute it (like if they wrote it themselves, or if the code has a license that allows for that) https://doi.org/10.18739/A2XS5JH4N Guidelines for large outputs: https://arcticdata.io/submit/#guidelines-for-large-models Spatial data Vector https://doi.org/10.18739/A2TB0XV89 Raster https://doi.org/10.18739/A2GT5FG0B Portals https://arcticdata.io/catalog/portals/DBO https://arcticdata.io/catalog/portals/CALM "],["final-checklist.html", "Final Checklist", " Final Checklist You can click on the assessment report on the website to for a general check. Fix anything you see there. Send the link over slack for peer review by your fellow datateam members. Usually we look for the following (the list is not exhaustive): System Metadata the format ids are correct General EML Included lines for FAIR: doc &lt;- eml_add_publisher(doc) doc &lt;- eml_add_entity_system(doc) Title No abbreviations, should include geographic and temporal coverage Abstract longer than 100 words no abbreviations or garbled text tags such as &lt;superscript&gt;2&lt;/superscript&gt; and &lt;subscript&gt;2&lt;/subscript&gt; can be used for nicer formatting DataTable / OtherEntity / SpatialVectors in the correct one: DataTable / OtherEntity / SpatialVector / SpatialRaster for the file type entityDescription - longer than 5 words and unique physical present and format correct Attribute Table complete attributeDefinitions longer than 3 words Variables match what is in the file Measurement domain - if appropirate (ie dateTime correct) Missing Value Code - accounted for if applicable Semantic Annotation - appropriate semantic annotations added, especially for spatial and temporal variables: lat, lon, date etc. People complete information for each person in each section including ORCID and e-mail address for all contacts people repeated across sections should have consistent information Geographic region the map looks correctand matches the geographic description check if negatives (-) are missing Project if it is an NSF award you can use the helper function: doc$dataset$project &lt;- eml_nsf_to_project(awards) for other awards that need to be set manually, see the set project page Methods present no garbled text Check EML Version currently using: eml-2.2.0 (as of July 30 2020) review to see if the EML version is set correctly by reviewing the doc$`@context` that it is indeed 2.2.0 under eml Re-run your code again and have the lineemld::eml_version(\"eml-2.2.0\") at the top Access Granted access to PI using set_rights_and_access() make sure it is http:// (no s) note if it is a part of portals there might be specific access requirements for it to be visible using set_access() SFTP Files if there are files transferred to us via SFTP, delete those files when the ticket is resolved "],["initial-review-checklist.html", "Initial review checklist", " Initial review checklist Before responding to a new submission use this checklist to review the submission. When your are ready to respond use the initial email template and insert comments and modify as needed. Social Science Data Any dataset involving human subjects (may include awards awarded by ASSP, topics such as COVID-19) Check if there is any sensitive information or personal identifying information in the data (eg. Names) Can the data be disaggregated and de-anonymized? (eg. a small sample size and individuals could be easily identified by their answers) Check for citations If the dataset appears to be in a publication please (might be in the abstract) Make sure that those citations are registered at the ADC Title WHAT, WHERE, and WHEN: Is descriptive of the work (provides enough information to understand the contents at a general scientific level), AND includes temporal coverage Provides a location of the work from the local to state or country level Provides a time frame of the work NO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out Abstract Describes the DATA as well as: The motivation (purpose) of the study Where and when the research took place At least one sentence summarizing general methodologies NO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out At least 100 words total tags such as &lt;superscript&gt;2&lt;/superscript&gt; and &lt;subscript&gt;2&lt;/subscript&gt; can be used for nicer formatting Any citations to papers can be registered with us Keywords Some keywords are included Data Data is normalized (if not suggest to convert the data if possible) At least one data file, or an identifier to the files at another approved archive, unless funded by ASSP (Arctic Social Sciences Program) No xls/xlsx files (or other proprietary files) File contents and relationships among files are clear Each file is well NAMED and DESCRIBED and clearly differentiated from all others All attributes in EML match attribute names in respective data files EXACTLY, are clearly defined, have appropriate units, and are in the same order as in the file. Quality control all dimensionless units. Missing value codes are explained (WHY are the data absent?) If it is a .rar file scan the file If there is the unit tons make sure to ask if it is metric tons or imperical tons if not clarified already People &amp; Parties At least one contact and one creator with a name, email address, and ORCID iD Coverages Includes coverages that make sense Temporal coverage - Start date BEFORE end date Geologic time scales are added if mentioned in metadata (e.g. 8 Million Years or a name of a time period like Jurassic) Spatial coverage matches geographic description (check hemispheres) Geographic description is from the local to state or country level, at the least Taxonomic coverage if appropriate Project Information At least one FUNDING number Title, personnel, and abstract match information from the AWARD (not from the data package) Methods This section is REQUIRED for ALL NSF-FUNDED data packages Enough detail is provided such that a reasonable scientist could interpret the study and data for reuse without needing to consult the researchers, nor any other resources Portals If there are multiple submissions from the same people/project let them know about the portals feature If this is part of a portal make sure this dataset can be found there. Additional steps might be needed to get that to work. Please consult Jeanette or Jasmine for more information on how to do that. "],["large-file-transfer.html", "Large file transfer", " Large file transfer Often if the researcher has many files to upload or the files are large in size, we will need to provide alternative methods of upload. We currently have two options available for large file transfer where sFTP is easier to set up but slower while GLOBUS takes a bit more setup but faster. Before providing access, try to get a sense of how big the transfer will be. df -h on Arctic server and datateam server - look at current space available sFTP As a team member for login info. After you login, you can add a folder with your last name to add your files. If you need more detailed access instructions, I have included the link here for your convenience: https://help.nceas.ucsb.edu/NCEAS/help/remote_file_access Globus Endpoint arctic-data-center #arctic-data-center login using your credentials on the datateam server Remember to remove the files on the datateam server after you finish a ticket to free up space. "],["navigate-rt.html", "Navigate RT", " Navigate RT The RT ticketing system is how we communicate with folks interacting with the Arctic Data Center. We use it for managing submissions, accessing issues, etc. It consists of three separate interfaces: Front Page All Tickets Ticket Page Front page This is what you see first Home - brings you to this homepage Tickets - to search for tickets (also see number 5) Tools - not needed New Ticket - create a new ticket Search - Type in the ticket number to quickly navigate to a ticket Queue - Lists all of the tickets currently in a particular queue (such as ‘arcticdata’) and their statuses New = unopened tickets that require attention Open = tickets currently open and under investigation and/or being processed by a support team member Stalled = tickets awaiting responses from the PI/ submitter Tickets I Own - These are the current open tickets that are claimed by me Unowned Tickets - Newest tickets awaiting claim Ticket Status - Status and how long ago it was created Take - claim the ticket as yours All tickets This is the queue interface from number 6 of the Front page 1. Ticket number and title 2. Ticket status 3. Owner - who has claimed the ticket Example ticket Title - Include the PI’s name for reference Display - homepage of the ticket History - Comment/Email history, see bottom of Display page Basics - edit the title, status, and ownership here People - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ submitter email addresses should be listed as “Requestors”. Requestors are only emailed on “Replys”, not “Comments”. Ensure your ticket has a Requestor before attempting to contact users/ PIs/ submitters Links - option to “Merge into” another ticket number if this is part of a larger conversation. Also option to add a reference to another ticket number Verify that this is indeed the two tickets you want to merge. It is non-reversible. Actions Reply - message the submitter/ PI/ all watchers Comment - attach internal message (no submitters, only Data Teamers) Open It - Open the ticket Stall - submitter has not responded in greater than 1 month Resolve - ticket completed History - message history and option to reply (to submitter and beyond) or comment (internal message) New data submission When notified by Arcticbot about a new data submission, here are the typical steps: Update the Requestor under the People section based on the email given in the submission (usually the user/ PI/ submitter). You may have to google for the e-mail address if the PI did not include it in the metadata record. Take the ticket (Actions &gt; Take) Review the submission based on the checklist Draft an email using the template and let others review it via Slack Send your reply via Actions "],["replicate-datasets.html", "Replicate Datasets", " Replicate Datasets Sometimes we get requests to have their datasets replicated to ADC. Usually it is a dataset on EDI where you can ge the Digital Object Identifier: which serves as the resource map on DataOne If a link isn’t given, you can use the package ID (i.e. knb-lter-arc.20129.1) and add it to the end of this link: https://portal.edirepository.org/nis/mapbrowse?packageid= If we are unsure about the identifer you can try querying for it on the CN: cn &lt;- CNode(&#39;PROD&#39;) #find the DOIs result &lt;- query(cn, list(q = paste0(&quot;(id:*10.6073/pasta/d4f567844673857239eec0cb61c6f543&quot;,&quot;* *:* NOT obsoletedBy:*)&quot;), fl = &quot;identifier,rightsHolder,formatId, fileName, dateUploaded, authoritativeMN, replicaMN&quot;, sort = &#39;dateUploaded+desc&#39;, start =&quot;0&quot;, rows = &quot;1500&quot;), as=&quot;data.frame&quot;) Cloning the packages Try cloning to the test node first to see how it will look. Set up your nodes from &lt;- dataone::D1Client(&quot;PROD&quot;, &quot;urn:node:LTER&quot;) to &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) For LTER datasets on EDI, there are two possibilities for the mn: “urn:node:LTER” and “urn:node:EDI”. Try the other one if one isn’t working Use clone_package to copy it over to the test node clone_package(&quot;doi:10.6073/pasta/d4f567844673857239eec0cb61c6f543&quot;, #example doi to replicate from = from, to = to, add_access_to = &quot;http://orcid.org/0000-0001-8888-547X&quot;, change_auth_node = F, new_pid = T) Once you have verified that it clones properly to the test node, do the same in production but with a couple of minor modifications: from &lt;- dataone::D1Client(&quot;PROD&quot;, &quot;urn:node:LTER&quot;) to &lt;- dataone::D1Client(&quot;PROD&quot;, &quot;urn:node:ARCTIC&quot;) clone_package(&quot;doi:10.6073/pasta/0af82d3c3d9d1710775cf9b1464ce70b&quot;, from = from, to = to, add_access_to = &quot;http://orcid.org/0000-0001-8888-547X&quot;, change_auth_node = F, new_pid = F) #uses the same pid make sure the new_pid argument is set to F when you publish to production "],["miscellaneous-file-types.html", "Miscellaneous file types", " Miscellaneous file types "],["wrangle-data.html", "Wrangle data", " Wrangle data "],["solr-queries.html", "Solr queries", " Solr queries Solr is what’s known as an index, which helps us keep track of the data, metadata, and resource map objects stored in our metadata catalog, Metacat. Solr allows you to quickly search through any coordinating or member node. Some more common use-cases include: Recovering lost PIDs - if you publish new data objects (publish_object) but forget to save the PIDs, you can recover them with a query. For example, you can search for all data objects that are not associated with a resource map and are published using your ORCiD (q=-resourceMap:*+AND+submitter:*XXXX-YYYY-ZZZZ-WWWW) Working with groups of packages - if you want to find all data, metadata, and resource maps associated with a PI, you can use their last name (q=origin:*SURNAME*) or perhaps their ORCiD (q=rightsHolder:*XXXX-YYYY-ZZZZ-WWWW) or both (q=origin:*SURNAME*+OR+rightsHolder:*XXXX-YYYY-ZZZZ-WWWW) to retrieve information about packages associated with them Once you understand the logic of queries, it becomes a flexible and useful tool that you can integrate into your R workflow. You can use queries to answer a variety of interesting questions, for example: What are the most recently updated data sets? What metadata and data objects are in a given data package? What is the total size (in terms of disk space) of all objects stored in Metacat? Querying Solr is possible by adding a query onto the end of a base URL or through the dataone::query() function in R. For now, we’ll just cover the basics of Solr queries in R. "],["adc-web-submissions.html", "ADC web submissions", " ADC web submissions "],["data-portals.html", "Data Portals", " Data Portals "],["using-arcticdatautils.html", "Using arcticdatautils", " Using arcticdatautils Please use datapack and dataone for regular data processing. This information is kept for troubleshooting and legacy purposes. "],["nesting-data.html", "Nesting Data", " Nesting Data "]]
