[["index.html", "NCEAS Data Team Training Welcome to NCEAS! First day to-dos Account information NCEAS events NCEAS Community Meetings/ Groups Internship Expectations Code of Conduct Citation Information", " NCEAS Data Team Training Jesse Goldstein, Dominic Mullen, Jeanette Clark, Irene Steves, Mitchell Maier, Bryce Mecum, Peter Slaughter, Emily O’Dean, Jasmine Lai, Daphne Virlar-Knight, Maggie Klope 2025-03-05 Welcome to NCEAS! First day to-dos Get a tour of the office from Ginger Fill out required paperwork you received via email Set up the remainder of your accounts Account information LDAP - this should be set up prior to your start date in order to help get other accounts set. This account and password control your access to: arcticdata RT queue GitHub Enterprise - arctic-data Datateam server - follow instructions in email from Nick at NCEAS to reset your datateam password in the terminal ORCiD - create an account Login to test.arcticdata.io with your ORCID iD NCEAS Slack - get an invite from slack.nceas.ucsb.edu Channels to join: #arctica, #arcticbot, #computing, #datateam, #devteam Introduce yourself in #datateam Arctic Data Center Team - after creation of ORCiD and sign-in to both arcticdata.io and test.arcticdata.io, request addition to the admin group from Justin or Maggie in Slack GitHub - if you do not have a public GitHub account already, please register for one here If you are an intern, fill out anticipated quarterly schedule on the intern google calendar shared with you. Electronic Timekeeping - make sure you can log on to electronic timekeeping via your UCSBNetID and password (may not be accessible on the first day, if you continue to have issues please let Ana know). If you are an hourly employee, log your hours for your first day! Under today’s date select ‘Hours Worked’ under the Pay Code column, enter the amount of hours under the Amount column, and finally click the ‘Save’ button in the top right. At the end of every two-week pay period you will also need to click the ‘Approve Timecard’ button in order to submit your timecard. UCPath - you can set up your paycheck preferences here, including direct deposit and income tax withholding. Detailed Instructions Let Justin or Maggie know what email you would like to use for general NCEAS updates from all@nceas.ucsb.edu NCEAS events NCEAS hosts a number of events that you are encouraged to attend. Keep an eye on your email but the recurring events are: Roundtable presentation and discussion of research by a visiting or local scientist first or second Thursdays of the month at 3:30 in the lounge and via zoom followed by happy hour at 4:30 on the terrace Coffee Klatch coffee, socializing, and news updates for NCEAS Tuesdays at 10:30 in the lounge NCEAS Community Meetings/ Groups Check out their individual calendar entries and channels for more information Early Career Researcher Community Forum - #ecr_community Hacky Hours - #hackyhour Data Science Chats - #data-science-chats NCEAS Diversity Team NCEAS Book Club - #bookclub NCEAS Social - #social Internship Expectations As an intern with the data team, there are a few expectations that the Project Coordinators have of you. Overall, we expect you to be communicative and proactive. We want you to learn and grow in this position, but we don’t want you spinning your wheels going nowhere fast! If you’ve spent 10-15 minutes on an issue and you’re not making any progress, reach out to us and your peers for help in the #datateam slack channel. The #datateam slack channel is our main form of communication, and we expect all interns to become comfortable communicating in this space. By posting your questions and code in the #datateam channel (instead of sending direct messages), multiple people will be able to help at once, and we all can learn from the problems that our peers encounter. Additionally, we expect interns to work within the standard business hours of 8am - 5pm (pacific time). We ask that you mark your expected work hours on the shared “Intern” Google Calendar. This is so that the Project Coordinators are aware of who’s working day-to-day and can plan their days accordingly. We also use this to verify time sheets when they are submitted. Ideally, interns would input their proposed hours on the calendar at least one week in advance. During exams and other unusually busy weeks at school, we understand you may need to shift your hours or reduce your workload. When this occurs, please make sure to email either Daphne or Jeanette so that we know not to expect you during your usual schedule. Code of Conduct Take a moment to review the diversity and inclusion page and code of conduct at NCEAS so that we can foster an environment that is safe, welcoming and inclusive for everyone. Citation Information This work is licensed under a Creative Commons Attribution 4.0 International License. Clark, Jeanette, Jesse Goldstein, Dominic Mullen, Irene Steves, Mitchell Maier, Stephanie Freund, Sharis Ochs, Bryce Mecum, Peter Slaughter, Emily O’Dean, Jasmine Lai, Daphne Virlar-Knight. 2022 Training Materials for the Arctic Data Center Curation Team. Arctic Data Center. doi:10.18739/A20G3GX8W. "],["introduction-to-open-science.html", "Chapter 1 Introduction to open science 1.1 Background reading 1.2 Effective data management 1.3 Using DataONE 1.4 Working on a remote server 1.5 A note on paths 1.6 A note on R 1.7 A note on effective troubleshooting in R 1.8 A note on Exercises 1.9 Exercise 1", " Chapter 1 Introduction to open science These materials are meant to introduce you to the principles of open science, effective data management, and data archival with the DataONE data repository. It also provides an overview on the tools we will be using (remote servers, Rstudio, R, Troubleshooting, Exercises) throughout the training. This document is meant to take multiple days to complete, depending on your previous knowledge. We believe in allowing employees the space to fully grasp concepts during training, even if it means taking longer than expected. Quality learning is our priority, and there’s no pressure to finish within a specific timeframe. You may find it helpful to take notes on important concepts, and you will always be able to refer back to this training during your time at NCEAS. If you see anything that needs fixing, submit a issue in the github issues 1.1 Background reading Read the content on the Arctic Data Center (ADC) webpage to learn more about data submission, preservation, and the history of the ADC. We encourage you to follow the links within these pages to gain a deeper understanding. about submission preservation history 1.2 Effective data management Read Matt Jones et al.’s paper on effective data management to learn how we will be organizing datasets prior to archival. (Please note that while the tips outlined in this article are best practices, we often do not reformat data files submitted to our repositories unless necessary. It is best to be conservative and not alter other people’s data without good reason.) You may also want to explore the DataONE education resources related to data management. 1.3 Using DataONE Data Observation Network for Earth (DataONE) is a community driven initiative that provides access to data across multiple member repositories, supporting enhanced search and discovery of Earth and environmental data. Read more about what DataONE is here and about DataONE member node (MN) guidelines here. Please feel free to ask your supervisor any questions you have about DataONE. We will be applying these concepts in the next chapter. 1.4 Working on a remote server All of the work that we do at NCEAS is done on our remote server, datateam.nceas.ucsb.edu. If you have never worked on a remote server before, you can think of it like working on a different computer via the internet. We access RStudio on our server through this link. This is the same as your desktop version of RStudio with one main difference is that files are on the server. Please do all your work here, and bookmark this link. Do not use RStudio on your local computer. By only using the RStudio server, it is easier to share your code with the rest of us. 1.4.1 Check your understanding Open a new tab in your browser and try logging into the remote server using your SSH credentials. If your R session is frozen and unresponsive check out the guide on how to fix it. 1.5 A note on paths On the servers, paths to files in your folder always start with /home/yourusername/.... Note - if you are a more advanced user, you may use the method you prefer as long as it is evident where your file is from. When you write scripts, try to avoid writing relative paths (which rely on what you have set your working directory ~/ to) as much as possible. Instead, write out the entire path as shown above, so that if another data team member needs to run your script, it is not dependent on a working directory. 1.6 A note on R This training assumes basic knowledge of R and RStudio. Spend at least 15 minutes walking through Jenny Bryan’s excellent materials here for a refresher. Throughout this training we will occasionally use the namespace syntax package_name::function_name() when writing a function. This syntax denotes which package a function came from. For example dataone::getSystemMetadata selects the getSystemMetadata function from the dataone R package. More detailed information on namespaces can be found here. 1.7 A note on effective troubleshooting in R One of the advantages with using the R programming language is the extensive documentation that is available for R packages. The R help operator ? can be used to learn more about functions from all of the R packages we use. You can put the operator before the name of any function to view its documentation in RStudio: ?function When asking for help in the #datateam channel in Slack, we suggest using a combination of minimal reproducible examples (MRE) and the package reprex to create reproducible examples. This will allow others to better help you if we can run the code on our own computers. A MRE is stripping down your code to only the parts that cause the bug. When troubleshooting errors over Slack, send the code that returned an error and the error message itself. How to generate a reprex: copy the code you want to ask about call reprex() fix until everything runs smoothly copy the result to ask your question When copy and paste code slack message or github issues, use three backticks for code blocks and two backticks for a small piece of code will prevent issues with slack formats quotation. For more information and examples check out more of Jenny Bryan’s slides or watch the video starting at about the 10 min mark. Note for EML related MREs: - Generating a reprex for these situations (ie. tokens) might be complicated but you can should still follow the MRE principles even if the reprex won’t render fully - You can include a minimal EML to avoid some get_package issues: me &lt;- list(individualName = list(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;)) attributes &lt;- data.frame(attributeName = &#39;length_1&#39;, attributeDefinition = &#39;def1&#39;, measurementScale = &#39;ratio&#39;, domain = &#39;numericDomain&#39;, unit = &#39;meter&#39;, numberType = &#39;real&#39;) att_list &lt;- set_attributes(attributes) doc_ex &lt;- list(packageId = &quot;id&quot;, system = &quot;system&quot;, dataset = list(title = &quot;A Minimal Valid EML Dataset&quot;, creator = me, contact = me, dataTable = list(entityName = &quot;data table&quot;, attributeList = att_list)) ) 1.8 A note on Exercises The rest of the training has a series of exercises. These are meant to take you through the process as someone submitting a dataset from scratch. This is slightly different than the usual workflow but important in understanding the underlying system behind the Arctic Data Center. Please note that you will be completing everything on the site for the training. In the future if you are unsure about doing anything with a dataset. The test site is a good place to try things out! 1.9 Exercise 1 This part of the exercise walks you through submitting data through the web form on “test.arcticdata.io”. In addition to learning to use the webform, this exercise will also help you practice sleuthing for information in order to provide complete metadata. Most datasets do not come with all contextual information, so you will need to skim cited literature and search google for definitions of discipline-specific jargon. Don’t be afraid to use the internet as a resource! 1.9.1 Part 1 Download the csv of Table 1 from this paper. Reformat the table to meet the guidelines outlined in the journal article on effective data management (this might be easier to do in an interactive environment like Excel). Hint: This table is in wide format and can be made longer. Note: we usually don’t edit the content in data submissions so don’t stress over this part too much 1.9.2 Part 2 Go to “test.arcticdata.io” and submit your reformatted file with appropriate metadata that you derive from the text of the paper: List yourself as the first ‘Creator’ so your test submission can easily be found. For the purposes of this training exercise, not every single author needs to be listed with full contact details, listing the first two authors is fine. Directly copying and pasting sections from the paper (abstract, methods, etc.) is also fine. Attributes (column names) should be defined, including correct units and missing value codes. Click “describe” to the right of the file name in order to add file-specific information. The title and description can be edited in the “Overview” tab, while attributes are defined in the “Attributes” tab. Submit the dataset Post a message to the #datateam Slack channel with a link to your package. "],["creating-a-data-package.html", "Chapter 2 Creating a data package 2.1 What is in a package? 2.2 Packages on the Website 2.3 About identifiers 2.4 Upload a package 2.5 datapack Background 2.6 Exercise 2a 2.7 Create a new data package 2.8 Exercise 2b", " Chapter 2 Creating a data package This chapter will teach you how to create and submit a data package to a DataONE MN via R. But first, please read this paper on the value of structured metadata, namely the Ecological Metadata Language (EML). 2.1 What is in a package? A data package generally consists of at least 3 components. Metadata: One object is the metadata file itself. In case you are unfamiliar with metadata, metadata are information that describe data (e.g. who made the data, how were the data made, etc.). The metadata file will be in an XML format, and have the extension .xml (extensible markup language). We often refer to this file as the EML (Ecological Metadata Language), which is the metadata standard that it uses. Each dataset page in the Arctic Data Center is a direct representation of an EML document, made to look prettier for the web. Data: Other objects in a package are the data files themselves. Most commonly these are data tables (.csv), but they can also be audio files, NetCDF files, plain text files, PDF documents, image files, etc. Resource Map: The final object is the resource map. This object is a plain text file with the extension .rdf (Resource Description Framework) that defines the relationships between all of the other objects in the data package. You can think of it like a “basket” that holds the metadata file and all data files together. It says things like “this metadata file describes this data file,” and is critical to making a data package render correctly on the website. Fortunately, we rarely, if ever, have to actually look at the contents of resource maps; they are generated for us using tools in R. From the DataOne Community Meeting (Session 7) 2.2 Packages on the Website All of the package information is represented when you go to the landing page for a dataset. In the previous section, you uploaded a data file and made edits to the metadata using the web editor. When you make changes to the metadata and data files through R, those published changes will also be reflected here. 2.3 About identifiers Each object (metadata files, data files, resource maps) on the ADC or the KNB (another repo) has a unique identifier, also sometimes called a “PID” (persistent identifier). When you look at the landing page for a dataset, for example here, you can find the resource map identifier listed under the title in the gray bar after the words “Files in this dataset Package:” (resource_map_doi:10.18739/A2836Z), the metadata identifier in the “General &gt; Identifier” section of the metadata record or after the title with blue font (doi:10.18739/A2836Z), and the data identifier by clicking the “more info” link next to the data object, and looking at the “Online Distribution Info” section (arctic-data.9546.1). Note, all datasets submitted are given a preliminary identifier (usually starting with urn:uuid:). When the dataset is finalized, a doi will be issued. Different versions of a package are linked together by what we call the “version chain” or “obsolescence chain”. Making an update to a data package, such as replacing a data file, changing a metadata record, etc, will result in a new identifier for the new version of the updated object. When making changes to a package, always use datapack::uploadDataPackage() for updating the entire package on the latest versions of all objects to ensure that the version chain is maintained. 2.4 Upload a package We will be using R to connect to the NSF Arctic Data Center (ADC) data repository to push and pull edits in actual datasets. To identify yourself as an admin you will need to pass a ‘token’ into R. Do this by signing in to the ADC with your ORCid and password, then hovering over your name in the top right corner and clicking on “My profile”, then navigating to “Settings” and “Authentication Token”, copying the “Token for DataONE R”, and finally pasting and running it in your R console. The console is the bottom left window in RStudio. This token is your identity on these sites, please treat it as you would a password (i.e. don’t paste into scripts that will be shared). The easiest way to do this is to always run the token in the console. There’s no need to keep it in your script since it’s temporary anyway. You will need to retrieve a new one after it either expires or you quit your R session. Setting the token does not produce any output in the console. If the token is not set or is set incorrectly, you will know when an error is produced after trying to load a private dataset. Sometimes you’ll see a placeholder in scripts to remind users to get their token, such as: options(dataone_test_token = &quot;...&quot;) Since we will be working on the test site and not the production site, please remember to get your token from test.arcticdata.io Next, please be sure these packages are loaded for the training (these should already exist if you are working on the server): library(devtools) library(dataone) library(datapack) library(EML) library(remotes) library(XML) library(uuid) If any package could not be loaded, use the following command (replacing package_name with the actual package name) to install the package, then load them. install.packages(&quot;package_name&quot;) Now you’ll install the arcticdatautils and datamgmt packages with the code below. If prompted to update packages during the installation process, skip the updates. Now, run the following code to install and load the libraries. remotes::install_github(&quot;nceas/arcticdatautils&quot;) library(arcticdatautils) remotes::install_github(&quot;nceas/datamgmt&quot;) library(datamgmt) When you are usually working with data packages you will only need the following: library(dataone) library(datapack) library(EML) library(arcticdatautils) For this training, we will be working exclusively on the Arctic test site, or “node.” In many of the functions you will use this will be the first argument. It is often referred to in documentation as mn, short for member node. Different repositories use different member nodes. More information on the other nodes can be found in the reference section under Set DataONE nodes Set DataONE nodes For example, if we are using the test site, set the node to the test Arctic node: d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) Once all set up you can first publish an object (data) If you are curious how everything magically works, here is a handy diagram: From the DataOne Community Meeting (Session 7) 2.5 datapack Background adapted from the dataone and datapack vignettes datapack is written differently than most R packages you may have encountered in the past. This is because it uses the S4 system instead. library(dataone) library(datapack) library(uuid) Data packages Data packages are a class that has slots for relations (provenance), objects(the metadata and data file(s)) and systemMetadata. 2.5.1 Navigating data packages Nodes Using this example on arcticdata.io d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) To use the member node information, use the mn slot d1c_test@mn To access the various slots using objects created by datapack and dataone (e.g. getSystemMetadata) requires the @ which is different from what you might have seen in the past. This is because these use the S4 system. Get an existing package from the Arctic Data Center. Make sure you know as you go through this training whether you are reading or writing to test or production. We don’t want to upload any of your test datasets to production! d1c &lt;- dataone::D1Client(&quot;PROD&quot;, &quot;urn:node:ARCTIC&quot;) dp &lt;- dataone::getDataPackage(d1c, &quot;resource_map_urn:uuid:1f9eee7e-2d03-43c4-ad7f-f300e013ab28&quot;) 2.5.2 Data Objects You can see what slots are in an S4 object after typing the subsetting operator @, or pressing TAB with the cursor after an existing @. Try viewing the slots of the data package. dp@ Check out the objects slot dp@objects The objects slot contains a list of object PIDs that are accessed using the $ subsetting operator. Both are found within the structure of data packages in R. Get the number for data and metadata files associated with this data package: getSize(dp) Get the file names and corresponding pids getValue(dp, name=&quot;sysmeta@fileName&quot;) Get identifiers You can search by any of the sysmeta slots such as fileName and formatId and get the corresponding identifier(s): metadataId &lt;- selectMember(dp, name=&quot;sysmeta@ADD THE NAME OF THE SLOT&quot;, value=&quot;PATTERN TO SEARCH BY&quot;) Example: selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;image/tiff&quot;) selectMember(dp, name=&quot;sysmeta@fileName&quot;, value=&quot;filename.csv&quot;) 2.5.3 Provenance View the provenance as a dataTable. We will get into detail in the Building provenance chapter. dp@relations$relations 2.6 Exercise 2a Select a dataset from the catalog on the Arctic Data Center. Observe the number of data files in the dataset. Try to find identifiers for the metadata file and resource map on the landing page for the dataset based on the screenshot shown above. 2.7 Create a new data package adapted from the dataone and datapack vingettes library(dataone) library(datapack) library(uuid) Create a new data package - data package is a class that has slots for relations (provenance), objects(the metadata and data file(s)) and systemMetadata. dp &lt;- new(&quot;DataPackage&quot;) 2.7.1 Upload new data files 2.7.1.1 Create and add a metadata file In this example we will use this previously written EML metadata. Here we are getting the file path from the dataone package and saving that as the object emlFile. This is a bit of an unusual way to reference a local file path, but all this does is looks within the R package dataone and grabs the path to a metadata document stored within that package. If you print the value of emlFile you’ll see it is just a file path, but it points to a special place on the server where that package is installed. Usually you will just reference EML paths that are stored within your user file system. emlFile &lt;- system.file(&quot;extdata/strix-pacific-northwest.xml&quot;, package=&quot;dataone&quot;) Create a new DataObject for the metadata and add it to the package. metadataObj &lt;- new(&quot;DataObject&quot;, format=&quot;https://eml.ecoinformatics.org/eml-2.2.0&quot;, filename=emlFile) dp &lt;- addMember(dp, metadataObj) Check the dp object to see if the metadata was added correctly. dp 2.7.1.2 Add some additional data files sourceData &lt;- system.file(&quot;extdata/OwlNightj.csv&quot;, package=&quot;dataone&quot;) sourceObj &lt;- new(&quot;DataObject&quot;, format=&quot;text/csv&quot;, filename=sourceData) dp &lt;- addMember(dp, sourceObj, metadataObj) # The third argument of addMember() associates the new DataObject to the metadata that was just added. If you want to change the formatId please use updateSystemMetadata (more on this later in the book) 2.7.2 Upload the package d1c &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) Make sure to give access privileges to the ADC admins: myAccessRules &lt;- data.frame(subject=&quot;CN=arctic-data-admins,DC=dataone,DC=org&quot;, permission=&quot;changePermission&quot;) Get necessary token from test.arcticdata.io to upload the dataset prior uploading the datapackage: packageId &lt;- uploadDataPackage(d1c, dp, public=TRUE, accessRules=myAccessRules, quiet=FALSE) 2.8 Exercise 2b This exercise will take you through how to do the submission process through R instead of the webform (exercise 1). 2.8.1 Part 1 - Gather your data files For our convenience, we will be grabbing the metadata and data files from the file we published earlier: Locate the data package you published in Exercise 1 by navigating to the “My Profile &gt; My Data” section on test.arcticdata.io. Download the metadata and data files and transfer them to the Datateam server. 2.8.2 Part 2 - Working in R Now we want to publish the metadata and data files we downloaded again to test.arcticdata.io Obtain a token and please note that for this exercise please make sure you grab the token from the arcticdata test site Publish your metadata and data file to the site. #set the node d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) dp &lt;- new(&quot;DataPackage&quot;) #add your metadata metadataObj &lt;- new(...) dp &lt;- addMember(...) #add your data files sourceObj &lt;- new(...) dp &lt;- addMember(...) #upload your package myAccessRules &lt;- data.frame(...) packageId &lt;- uploadDataPackage(...) View your new data set by appending the metadata PID to the end of the URL test.arcticdata.io/view/… If you are successful it should look the same as the dataset you created in exercise 1 Send a message to #datateam with the exercise number and a link to your new package. "],["exploring-eml.html", "Chapter 3 Exploring EML 3.1 Navigate through EML 3.2 Understand the EML schema 3.3 Access specific elements", " Chapter 3 Exploring EML We use the Ecological Metadata Language (EML) to store structured metadata for all datasets submitted to the Arctic Data Center. EML is written in XML (extensible markup language) and functions for building and editing EML are in the EML R package. Currently the Arctic Data Center website supports editing EML version 2.2.0. There are still some metadata in 2.1.1 that will be converted eventually. For additional background on EML and principles for metadata creation, check out this paper. If you aren’t too familiar with lists and how to navigate them yet take a look at the relevant sections in the Stat 545 class. 3.1 Navigate through EML The first task when editing an EML file is navigating the EML file. An EML file is organized in a structure that contains many lists nested within other lists. The function View allows you to get a crude view of an EML file in the viewer. It can be useful for exploring the file. # Need to be in this member node to explore file d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) doc &lt;- read_eml(getObject(d1c_test@mn, &quot;urn:uuid:558eabf1-1e91-4881-8ba3-ef8684d8f6a1&quot;)) View(doc) The complex EML document is represented in R as as series of named, nested lists. We use lists all the time in R! A data.frame is one example of a special kind of list that we use all the time. You may be familiar with the syntax dataframe$column_name which allows us to select a particular column of a data.frame. Under the hood, a data.frame is a named list of vectors with the same length. You select one of those vectors using the $ operator, which is called the “list selector operator.” Just like you navigate in a data.frame, you can use the $ operator to navigate through the EML structure. The $ operator allows you to go deeper into the EML structure and to see what elements are nested within other elements. However, you have to tell R where you want to go in the structure when you use the $ symbol. For example, if you want to view the dataset element of your EML you would use the command doc$dataset. If you want to view the creators of your data set you would use doc$dataset$creator. Note here that creator is contained within dataset. If you aren’t sure where you want to go, hit the tab button on your keyboard after typing $ and a list of available elements in the structure will appear (e.g., doc$&lt;TAB&gt;): Note that if you hit tab, and nothing pops up, this most likely implies that you are trying to go into an EML element that can take a series items. For example doc$dataset$creator$&lt;TAB&gt; will not show a pop-up menu. This is because creator is a series-type object (i.e. you can have multiple creators). If you want to go deeper into creator, you first must tell R which creator you are interested in. Do this by writing [[i]] first where i is the index of the creator you are concerned with. For example, if you want to look at the first creator i = 1. Now doc$dataset$creator[[1]]$&lt;TAB&gt; will give you many more options. Note, an empty autocomplete result sometimes means you have reached the end of a branch in the EML structure. Below is the structure of doc$dataset. There are a series of multiple creators, which can be accessed individually by index: doc$dataset$creator[[#]]. At this point stop and take a deep breath. The key takeaway is that EML is a hierarchical tree structure. The best way to get familiar with it is to explore the structure. Try entering doc$dataset into your console, and print it. Now make the search more specific, for instance: doc$dataset$abstract. 3.2 Understand the EML schema Another great resource for navigating the EML structure is looking at the schema which defines the structure. The schema diagrams on this page are interactive. Further explanations of the symbology can be found here. The schema is complicated and may take some time to get familiar with before you will be able to fully understand it. Use your browser’s “search in page” function (usually CTRL-F or Command-F) to navigate the EML schema page quickly. For example, let’s take a look at eml-party. To start off, notice that some elements have bolded lines leading to them. A bold line indicates that the element is required if the element above it (to the left in the schema) is used, otherwise the element is optional. Notice also that next to the givenName element it says “0..infinity”. This means that the element is unbounded — a single party can have many given names and there is no limit on how many you can add. However, this text does not appear for the surName element — a party can have only one surname. You will also see icons linking the EML slots together, which indicate the ordering of subsequent slots. These can indicate either a “sequence” or a “choice”. In our example from eml-party, a “choice” icon indicates that either an individualName, organizationName, or positionName is required, but you do not need all three. However, the “sequence” icon tells us that if you use an individualName, you must include the surName as a child element. If you include the optional child elements salutation and givenName, they must be written in the order presented in the schema. The eml schema sections you may find particularly helpful include eml-party, eml-attribute and eml-physical. For a more detailed description of the EML schema, see the reference section on exploring EML. 3.2.1 Check your understanding Find otherEntity within the EML schema. Which elements are required? Can otherEntity be a series object? Answer otherEntity requires entityType and entityName children, or alternatively will accept only references. It is a series object, so there can be multiple otherEntities. Along with otherEntity and creator, dataTable and attribute can also be series objects. 3.3 Access specific elements The eml_get() function is a powerful tool for exploring EML (more on that here ). It takes any chunk of EML and returns all instances of the element you specify. Note: you’ll have to specify the element of interest exactly, according to the spelling/capitalization conventions used in EML. Here are some examples: doc &lt;- read_eml(system.file(&quot;example-eml.xml&quot;, package = &quot;arcticdatautils&quot;)) eml_get(doc, &quot;creator&quot;) individualName: givenName: Bryce surName: Mecum organizationName: National Center for Ecological Analysis and Synthesis eml_get(doc, &quot;boundingCoordinates&quot;) eastBoundingCoordinate: &#39;-134&#39; northBoundingCoordinate: &#39;59&#39; southBoundingCoordinate: &#39;57&#39; westBoundingCoordinate: &#39;-135&#39; eml_get(doc, &quot;url&quot;) &#39;&#39;: function: download url: ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456 &#39;&#39;: ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456 eml_get_simple() is a simplified alternative to eml_get() that produces a list of the desired EML element. eml_get_simple(doc$dataset$otherEntity, &quot;entityName&quot;) To find an eml element you can use either a combination of which_in_emlfrom the arcticdatautils package or eml_get_simple and which to find the index in an EML list. Use which ever workflow you see fit. An example question you may have: Which creators have a surName “Mecum”? Example using which_in_eml: n &lt;- which_in_eml(doc$dataset$creator, &quot;surName&quot;, &quot;Mecum&quot;) # Answer: doc$dataset$creator[[n]] Example using eml_get_simple and which: ent_names &lt;- eml_get_simple(doc$dataset$creator, &quot;surName&quot;) i &lt;- which(ent_names == &quot;Mecum&quot;) # Answer: doc$dataset$creator[[i]] "],["editing-eml.html", "Chapter 4 Editing EML 4.1 Edit an EML element 4.2 FAIR data practices 4.3 Edit attributeLists 4.4 Set physical 4.5 Edit dataTables 4.6 Edit otherEntities 4.7 Semantic annotations 4.8 Exercise 3a 4.9 Validate EML and update package 4.10 Exercise 3b", " Chapter 4 Editing EML This chapter is a practical tutorial for using R to read, edit, write, and validate EML documents. Much of the information here can also be found in the vignettes for the R packages used in this section (e.g. the EML package). Most of the functions you will see in this chapter will use the arcticdatautils and EML packages. This chapter will be longest of all the sections! This is a reminder to take frequent breaks when completing this section. If you struggle with getting a piece of code to work more than 10 minutes, reach out to your supervisor for help. When using R to edit EML documents, run each line individually by highlighting the line and using CTRL+ENTER). Many EML functions only need to be ran once, and will either produce errors or make the EML invalid if run multiple times. 4.1 Edit an EML element There are multiple ways to edit an EML element. 4.1.1 Edit EML with strings The most basic way to edit an EML element would be to navigate to the element and replace it with something else. Easy! For example, to change the title one could use the following command: doc$dataset$title &lt;- &quot;New Title&quot; If the element you are editing allows for multiple values, you can pass it a list of character strings. Since a dataset can have multiple titles, we can do this: doc$dataset$title &lt;- list(&quot;New Title&quot;, &quot;Second New Title&quot;) However, this isn’t always the best method to edit the EML, particularly if the element has sub-elements. Adding directly to doc without a helper function can overwrite these parts of the doc that we need. 4.1.2 Edit EML with the “EML” package To edit a section where you are not 100% sure of the sub-elements, using the eml$elementName() helper functions from the EML package will pre-populate the options for you if you utilize the RStudio autocomplete functionality. The arguments in these functions show the available slots for any given EML element. For example, typing doc$dataset$abstract &lt;- eml$abstract()&lt;TAB&gt; will show you that the abstract element can take either the section or para sub-elements. doc$dataset$abstract &lt;- eml$abstract(para = &quot;A concise but thorough description of the who, what, where, when, why, and how of a dataset.&quot;) This inserts the abstract with a para element in our dataset, which we know from the EML schema is valid. Note that the above is equivalent to the following generic construction: doc$dataset$abstract &lt;- list(para = &quot;A concise but thorough description of the who, what, where, when, why, and how of a dataset.&quot;) The eml() family of functions provides the sub-elements as arguments, which is extremely helpful, but functionally all it is doing is creating a named list, which you can also do using the list function. 4.1.3 Edit EML with objects A final way to edit an EML element would be to build a new object to replace the old object. To begin, you might create an object using an eml helper function. Let’s take keywords as an example. Sometimes keyword lists in a metadata record will come from different thesauruses, which you can then add in series (similar to the way we added multiple titles) to the element keywordSet. We start by creating our first set of keywords and saving it to an object. kw_list_1 &lt;- eml$keywordSet(keywordThesaurus = &quot;LTER controlled vocabulary&quot;, keyword = list(&quot;bacteria&quot;, &quot;carnivorous plants&quot;, &quot;genetics&quot;, &quot;thresholds&quot;)) Which returns: $keyword $keyword[[1]] [1] &quot;bacteria&quot; $keyword[[2]] [1] &quot;carnivorous plants&quot; $keyword[[3]] [1] &quot;genetics&quot; $keyword[[4]] [1] &quot;thresholds&quot; $keywordThesaurus [1] &quot;LTER controlled vocabulary&quot; We create the second keyword list similarly: kw_list_2 &lt;- eml$keywordSet(keywordThesaurus = &quot;LTER core area&quot;, keyword = list(&quot;populations&quot;, &quot;inorganic nutrients&quot;, &quot;disturbance&quot;)) Finally, we can insert our two keyword lists into our EML document just like we did with the title example above, but rather than passing character strings into list(), we will pass our two keyword set objects. doc$dataset$keywordSet &lt;- list(kw_list_1, kw_list_2) Note that you must use the function list here and not the c() function. The reasons for this are complex, and due to some technical subtlety in R - but the gist of the issue is that the c() function can behave in unexpected ways with nested lists, and frequently will collapse the nesting into a single level, resulting in invalid EML. 4.2 FAIR data practices The result of these function calls won’t show up on the webpage but they will add a publisher element to the dataset element and a system to all of the entities based on what the PID is. This will help make our metadata more FAIR (Findable, Accessible, Interoperable, Reusable). These two functions come from the arcticatautils package, an R package we wrote to help with some very specific data processing tasks. Add these function calls to all of your EML processing scripts. library(arcticdatautils) doc &lt;- eml_add_publisher(doc) doc &lt;- eml_add_entity_system(doc) 4.3 Edit attributeLists Attributes are descriptions of variables, typically columns or column names in tabular data. Attributes are stored in an attributeList. When editing attributes in R, we convert the attribute list information to data frame (table) format so that it is easier to edit. When editing attributes you will need to create one to three data frame objects: A data.frame of attributes A data.frame of custom units (if applicable) A data.frame of factors (if applicable) The attributeList is an element within one of 4 different types of entity objects. An entity corresponds to a file, typically. Multiple entities (files) can exist within a dataset. The 4 different entity types are dataTable (most common for us), spatialVector, spatialRaster, and otherEntity. Please note that submitting attribute information through the website will store them in an otherEntity object by default. We prefer to store them in a dataTable object for tabular data or a spatialVector or spatialRaster object for spatial data. To edit or examine an existing attribute list already in an EML file, you can use the following commands, where i represents the index of the series element you are interested in. Note that if there is only one item in the series (ie there is only one dataTable), you should just call doc$dataset$dataTable, as in this case doc$dataset$dataTable[[1]] will return the first sub-element of the dataTable (the entityName) # If they are stored in an otherEntity (submitted from the website by default) attribute_tables &lt;- EML::get_attributes(doc$dataset$otherEntity[[i]]$attributeList) # Or if they are stored in a dataTable (usually created by a datateam member) attribute_tables &lt;- EML::get_attributes(doc$dataset$dataTable[[i]]$attributeList) The get_attributes() function returns the attribute_tables object, which is a list of the three data frames mentioned above. The data frame with the attributes is called attribute_tables$attributes. attribute_tables$attributes print(attribute_tables$attributes) 4.3.1 Edit attributes Attribute information should be stored in a data.frame with the following columns: attributeName: The name of the attribute as listed in the csv. Required. e.g.: “c_temp” attributeLabel: A descriptive label that can be used to display the name of an attribute. It is not constrained by system limitations on length or special characters. Optional. e.g.: “Temperature (Celsius)” attributeDefinition: Longer description of the attribute, including the required context for interpreting the attributeName. Required. e.g.: “The near shore water temperature in the upper inter-tidal zone, measured in degrees Celsius.” measurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required. nominal: unordered categories or text. e.g.: (Male, Female) or (Yukon River, Kuskokwim River) ordinal: ordered categories. e.g.: Low, Medium, High dateTime: date or time values from the Gregorian calendar. e.g.: 01-01-2001 ratio: measurement scale with a meaningful zero point in nature. Ratios are proportional to the measured variable. e.g.: 0 Kelvin represents a complete absence of heat. 200 Kelvin is half as hot as 400 Kelvin. 1.2 meters per second is twice as fast as 0.6 meters per second. interval: values from a scale with equidistant points, where the zero point is arbitrary. This is usually reserved for degrees Celsius or Fahrenheit, or any other human-constructed scale. e.g.: there is still heat at 0° Celsius; 12° Celsius is NOT half as hot as 24° Celsius. domain: One of: textDomain, enumeratedDomain, numericDomain, dateTime. Required. textDomain: text that is free-form, or matches a pattern enumeratedDomain: text that belongs to a defined list of codes and definitions. e.g.: CASC = Cascade Lake, HEAR = Heart Lake dateTimeDomain: dateTime attributes numericDomain: attributes that are numbers (either ratio or interval) formatString: Required for dateTime, NA otherwise. Format string for dates, e.g. “DD/MM/YYYY”. definition: Required for textDomain, NA otherwise. Defines a format for attributes that are a character string. e.g.: “Any text” or “7-digit alphanumeric code” unit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found using: standardUnits &lt;- EML::get_unitList() then running View(standardUnits$units). numberType: Required for numericDomain, NA otherwise. Options are real, natural, whole, and integer. real: positive and negative fractions and integers (…-1,-0.25,0,0.25,1…) natural: non-zero positive integers (1,2,3…) whole: positive integers and zero (0,1,2,3…) integer: positive and negative integers and zero (…-2,-1,0,1,2…) missingValueCode: Code for missing values (e.g.: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’ missingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists. You can create attributes manually by typing them out in R following a workflow similar to the one below: attributes &lt;- data.frame( attributeName = c(&#39;Date&#39;, &#39;Location&#39;, &#39;Region&#39;,&#39;Sample_No&#39;, &#39;Sample_vol&#39;, &#39;Salinity&#39;, &#39;Temperature&#39;, &#39;sampling_comments&#39;), attributeDefinition = c(&#39;Date sample was taken on&#39;, &#39;Location code representing location where sample was taken&#39;,&#39;Region where sample was taken&#39;, &#39;Sample number&#39;, &#39;Sample volume&#39;, &#39;Salinity of sample in PSU&#39;, &#39;Temperature of sample&#39;, &#39;comments about sampling process&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;nominal&#39;,&#39;nominal&#39;, &#39;nominal&#39;, &#39;ratio&#39;, &#39;ratio&#39;, &#39;interval&#39;, &#39;nominal&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;enumeratedDomain&#39;,&#39;enumeratedDomain&#39;, &#39;textDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;textDomain&#39;), formatString = c(&#39;MM-DD-YYYY&#39;, NA,NA,NA,NA,NA,NA,NA), definition = c(NA,NA,NA,&#39;Six-digit code&#39;, NA, NA, NA, &#39;Any text&#39;), unit = c(NA, NA, NA, NA,&#39;milliliter&#39;, &#39;dimensionless&#39;, &#39;celsius&#39;, NA), numberType = c(NA, NA, NA,NA, &#39;real&#39;, &#39;real&#39;, &#39;real&#39;, NA), missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, &#39;NA&#39;), missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, &#39;no sampling comments&#39;)) However, typing this out in R can be a major pain. Luckily, there’s a Shiny app that you can use to build attribute information. You can use the app to build attributes from a data file loaded into R (recommended as the app will auto-fill some fields for you) to edit an existing attribute table, or to create attributes from scratch. Use the following commands to create or modify attributes. Use the following commands to create or modify attributes. These commands will launch a “Shiny” app in your web browser. #first download the CSV in your data package from Exercise #2 data_pid &lt;- selectMember(dp, name = &quot;sysmeta@fileName&quot;, value = &quot;.csv&quot;) data &lt;- read.csv(text=rawToChar(getObject(d1c_test@mn, data_pid))) # From data (recommended) attribute_tables &lt;- EML::shiny_attributes(data = data) # From scratch attribute_tables &lt;- EML::shiny_attributes() # From an existing attribute list attribute_tables &lt;- get_attributes(doc$dataset$dataTable[[i]]$attributeList) attribute_tables &lt;- EML::shiny_attributes(attributes = attribute_tables$attributes) Once you are done editing a table in the browser app, quit the app by pressing the red “Quit App” button in the top right corner of the page. If you close the Shiny app tab in your browser instead of using the “Quit App” button, your work will not be saved, R will think that the Shiny app is still open, and you will not be able to run other code. You can tell if R is confused if you have closed the Shiny app and the bottom line in the console still says Listening on http://.... If this happens, press the red stop sign button on the right hand side of the console window in order to interrupt R. The tables you constructed in the app will be assigned to the attribute_tables variable as a list of data frames (one for attributes, factors, and units). Be careful to not overwrite your completed attribute_tables object when trying to make edits. The last line of code can be used in order to make edits to an existing attribute_tables object. Alternatively, each table can be to exported to a csv file by clicking the Download button. If you downloaded the table, read the table back into your R session and assign it to a variable in your script (e.g. attributes &lt;- data.frame(...)), or just use the variable that shiny_attributes returned. For simple attribute corrections, datamgmt::edit_attribute() allows you to edit the slots of a single attribute within an attribute list. To use this function, pass an attribute through datamgmt::edit_attribute() and fill out the parameters you wish to edit/update. An example is provided below where we are changing attributeName, domain, and measurementScale in the first attribute of a dataset. After completing the edits, insert the new version of the attribute back into the EML document. new_attribute &lt;- datamgmt::edit_attribute(doc$dataset$dataTable[[1]]$attributeList$attribute[[1]], attributeName = &#39;date_and_time&#39;, domain = &#39;dateTimeDomain&#39;, measurementScale = &#39;dateTime&#39;) doc$dataset$dataTable[[1]]$attributeList$attribute[[1]] &lt;- new_attribute 4.3.2 Edit custom units EML has a set list of units that can be added to an EML file. These can be seen by using the following code: standardUnits &lt;- EML::get_unitList() View(standardUnits$units) Search the units list for your unit before attempting to create a custom unit. You can search part of the unit you can look up part of the unit ie meters in the table to see if there are any matches. If you have units that are not in the standard EML unit list, you will need to build a custom unit list. Attribute tables with custom units listed will generate a warning indicating that a custom unit will need to be described. A unit typically consists of the following fields: id: The unit id (ids are camelCased) unitType: The unitType (run View(standardUnits$unitTypes) to see standard unitTypes) parentSI: The parentSI unit (e.g. for kilometer parentSI = “meter”). The parentSI does not need to be part of the unitList. multiplierToSI: Multiplier to the parentSI unit (e.g. for kilometer multiplierToSI = 1000) name: Unit abbreviation (e.g. for kilometer name = “km”) description: Text defining the unit (e.g. for kilometer description = “1000 meters”) To manually generate the custom units list, create a dataframe with the fields mentioned above. An example is provided below that can be used as a template: custom_units &lt;- data.frame( id = c(&#39;siemensPerMeter&#39;, &#39;decibar&#39;), unitType = c(&#39;resistivity&#39;, &#39;pressure&#39;), parentSI = c(&#39;ohmMeter&#39;, &#39;pascal&#39;), multiplierToSI = c(&#39;1&#39;,&#39;10000&#39;), abbreviation = c(&#39;S/m&#39;,&#39;decibar&#39;), description = c(&#39;siemens per meter&#39;, &#39;decibar&#39;)) Custom units can also be created in the shiny app, under the “units” tab. They cannot be edited again in the shiny app once created. attribute_tables &lt;- EML::shiny_attributes() custom_units &lt;- attribute_tables$units Using EML::get_unit_id for custom units will also generate valid EML unit ids. Custom units are then added to additionalMetadata using the following command: unitlist &lt;- set_unitList(custom_units, as_metadata = TRUE) doc$additionalMetadata &lt;- list(metadata = list(unitList = unitlist)) 4.3.3 Edit factors For attributes that are enumeratedDomains, a table is needed with three columns: attributeName, code, and definition. attributeName should be the same as the attributeName within the attribute table and repeated for all codes belonging to a common attribute. code should contain all unique values of the given attributeName that exist within the actual data. definition should contain a plain text definition that describes each code. To build factors by hand, you use the named character vectors and then convert them to a data.frame as shown in the example below. In this example, there are two enumerated domains in the attribute list - “Location” and “Region”. Location &lt;- c(CASC = &#39;Cascade Lake&#39;, CHIK = &#39;Chikumunik Lake&#39;, HEAR = &#39;Heart Lake&#39;, NISH = &#39;Nishlik Lake&#39; ) Region &lt;- c(W_MTN = &#39;West region, locations West of Eagle Mountain&#39;, E_MTN = &#39;East region, locations East of Eagle Mountain&#39;) The definitions are then written into a data.frame using the names of the named character vectors and their definitions. factors &lt;- rbind(data.frame(attributeName = &#39;Location&#39;, code = names(Location), definition = unname(Location)), data.frame(attributeName = &#39;Region&#39;, code = names(Region), definition = unname(Region))) Factors can also be created in the shiny app, under the “factors” tab. They cannot be edited again in the shiny app once created. attribute_tables &lt;- EML::shiny_attributes() attribute_tables$factors 4.3.4 Finalize attributeList Once you have built your attributes, factors, and custom units, you can add them to EML objects. Attributes and factors are combined to form an attributeList using set_attributes(): # Create an attributeList object attributeList &lt;- EML::set_attributes(attributes = attribute_tables$attributes, factors = attribute_tables$factors) This attributeList object can then be checked for errors and added to a dataTable in the EML document. # Edit EML document with object doc$dataset$dataTable[[i]]$attributeList &lt;- attributeList Remember to use: d1c_test &lt;- dataone::D1Client(“STAGING”, “urn:node:mnTestARCTIC”) d1c_test@mn 4.4 Set physical To set the physical aspects of a data object, use the following commands to build a physical object from a data PID that exists in your package. Remember to set the member node to test.arcticdata.io! Every entity that we upload needs a physical description added. When replacing files, the physical must be replaced as well. The word ‘physical’ derives from database systems, which distinguish the ‘logical’ model (e.g., what attributes are in a table, etc) from the physical model (how the data are written to a physical hard disk (basically, the serialization)). so, we grouped metadata about the file (eg. dataformat, file size, file name) as written to disk in physical. It includes info like the file size. For CSV files, the physical describes the number of header lines and the attribute orientation. # Get the PID of a file data_pid &lt;- selectMember(dp, name = &quot;sysmeta@fileName&quot;, value = &quot;your_file_name.csv&quot;) # Get the physical info and store it in an object physical &lt;- arcticdatautils::pid_to_eml_physical(mn, data_pid) The physical object can then be checked for errors and added to the EML document. # Edit EML document with object doc$dataset$dataTable[[i]]$physical &lt;- physical Note that the above workflow only works if your data object already exists on the member node. Physicals can be seen in the website representation of the EML below the entity description. 4.5 Edit dataTables Entities that are dataTables require an attribute list. To edit a dataTable, first edit/create an attributeList and set the physical. Then create a new dataTable using the eml$dataTable() helper function as below: dataTable &lt;- eml$dataTable(entityName = &quot;A descriptive name for the data (does not need to be the same as the data file)&quot;, entityDescription = &quot;A description of the data&quot;, physical = physical, attributeList = attributeList) The dataTable must then be added to the EML. How exactly you do this will depend on whether there are dataTable elements in your EML, and how many there are. To replace whatever dataTable elements already exist, you could write: doc$dataset$dataTable &lt;- dataTable If there is only one dataTable in your dataset, the EML package will usually “unpack” these, so that it is not contained within a list of length 1 - this means that to add a second dataTable, you cannot use the syntax doc$dataset$dataTable[[2]], since when unpacked this will contain the entityDescription as opposed to pointing to the second in a series of dataTable elements. Confusing - I know. Not to fear though - this syntax will get you on your way, should you be trying to add a second dataTable. doc$dataset$dataTable &lt;- list(doc$dataset$dataTable, dataTable) If there is more than one dataTable in your dataset, you can return to the more straightforward construction of: doc$dataset$dataTable[[i]] &lt;- dataTable Where i is the index that you wish insert your dataTable into. To add a list of dataTables to avoid the unpacking problem above you will need to create a list of dataTables dts &lt;- list() # create an empty list for(i in seq_along(tables_you_need)){ # your code modifying/creating the dataTable here dataTable &lt;- eml$dataTable(entityName = dataTable$entityName, entityDescription = dataTable$entityDescription, physical = physical, attributeList = attributeList) dts[[i]] &lt;- dataTable # add to the list } After getting a list of dataTables, assign the resulting list to dataTable EML doc$dataset$dataTable &lt;- dts By default, the online submission form adds all entities as otherEntity, even when most should probably be dataTable. You can use eml_otherEntity_to_dataTable to easily move items in otherEntity over to dataTable, and delete the old otherEntity. Most tabular data or data that contain variables should be listed as a dataTable. Data that do not contain variables (eg: plain text readme files, pdfs, jpegs) should be listed as otherEntity. eml_otherEntity_to_dataTable(doc, 1, # Indexes of otherEntities you want to convert, for multiple use 1:5 or c(1,3,5..) validate_eml = F) # set this to False if the physical or attributes are not added 4.6 Edit otherEntities 4.6.1 Remove otherEntities To remove an otherEntity use the following command. This may be useful if a data object is originally listed as an otherEntity and then transferred to a dataTable. doc$dataset$otherEntity[[i]] &lt;- NULL 4.6.2 Create otherEntities If you need to create/update an otherEntity, make sure to publish or update your data object first (if it is not already on the DataONE MN). Then build your otherEntity. otherEntity &lt;- arcticdatautils::pid_to_eml_entity(mn, pkg$data[[i]]) Alternatively, you can build the otherEntity of a data object not in your package by simply inputting the data PID. otherEntity &lt;- arcticdatautils::pid_to_eml_entity(mn, &quot;your_data_pid&quot;, entityType = &quot;otherEntity&quot;, entityName = &quot;Entity Name&quot;, entityDescription = &quot;Description about entity&quot;) The otherEntity must then be set to the EML, like so: doc$dataset$otherEntity &lt;- otherEntity If you have more than one otherEntity object in the EML already, you can add the new one like this: doc$dataset$otherEntity[[i]] &lt;- otherEntity Where i is set to the number of existing entities plus one. Remember the warning from the last section, however. If you only have one otherEntity, and you are trying to add another, you have to run: doc$dataset$otherEntity &lt;- list(otherEntity, doc$dataset$otherEntity) 4.7 Semantic annotations For a brief overview of what a semantic annotation is, and why we use them check out this video. Even more information on how to add semantic annotations to EML 2.2.0 can be found here. There are several elements in the EML 2.2.0 schema that can be annotated: dataset entity (eg: otherEntity or dataTable) attribute Attribute annotations can be edited in R and also on the website. Dataset and entity annotations are only done in R. 4.7.1 How annotations are used This is a dataset that has semantic annotations included. On the website you can see annotations in each of the attributes. You can click on any one of them to search for more datasets with that same annotation. Semantic attribute annotations can be applied to spatialRasters, spatialVectors and dataTables 4.7.1.1 Attribute-level annotations on the website editor The website has a searchable list of attribute annotations that are grouped by category and specificity. Open your dataset on the test website from earlier and enter the attribute editor. Look through all of the available annotations. Adding attribute annotations using the website is the easiest way, however adding them using R and/or the Shiny app may be quicker with very larger datasets. 4.7.1.2 Attribute-level annotations in R To manually add annotations to the attributeList in R you will need information about the propertyURI and valueURI. Annotations are essentially composed of a sentence, which contains a subject (the attribute), predicate (propertyURI), and object (valueURI). Because of the way our search interface is built, for now we will be using attribute annotations that have a propertyURI label of “contains measurements of type”. Here is what an annotation for an attribute looks like in R. Note that both the propertyURI and valueURI have both a label, and the URI itself. doc$dataset$dataTable[[i]]$attributeList$attribute[[i]]$annotation $id [1] &quot;ODBcOyaTsg&quot; $propertyURI $propertyURI$label [1] &quot;contains measurements of type&quot; $propertyURI$propertyURI [1] &quot;http://ecoinformatics.org/oboe/oboe.1.2/oboe-core.owl#containsMeasurementsOfType&quot; $valueURI $valueURI$label [1] &quot;Distributed Biological Observatory region identifier&quot; $valueURI$valueURI [1] &quot;http://purl.dataone.org/odo/ECSO_00002617&quot; 4.7.2 How to add an annotation 1. Decide which variable to annotate The goal for the datateam is to start annotating every dataset that comes in. Please make sure to add semantic annotations to spatial and temporal features such as latitude, longitude, site name and date and aim to annotate as many attributes as possible. 2. Find an appropriate valueURI The next step is to find an appropriate value to fill in the blank of the sentence: “this attribute contains measurements of _____.” There are several ontologies to search in. In order of most to least likely to be relevant to the Arctic Data Center they are: The Ecosystem Ontology (ECSO) this was developed at NCEAS, and has many terms that are relevant to ecosystem processes, especially those involving carbon and nutrient cycling The Environment Ontology (EnVO) this is an ontology for the concise, controlled description of environments National Center for Biotechnology Information (NCBI) Organismal Classification (NCBITAXON) The NCBI Taxonomy Database is a curated classification and nomenclature for all of the organisms in the public sequence databases. Information Artifact Ontology (IAO) this ontology contains terms related to information entities (eg: journals, articles, datasets, identifiers) To search, navigate through the “classes” until you find an appropriate term. When we are picking terms, it is important that we not just pick a similar term or a term that seems close - we want a term that is 100% accurate. For example, if you have an attribute for carbon tetroxide flux and an ontology with a class hierarchy like this: – carbon flux |—- carbon dioxide flux Our exact attribute, carbon tetroxide flux, is not listed. In this case, we should pick “carbon flux” as it’s completely correct/accurate and not “carbon dioxide flux” because it’s more specific/precise but not quite right. For general attributes (such as ones named depth or length), it is important to be as specific as possible about what is being measured. e.g. selecting the lake area annotation for the area attribute in this dataset 3. Build the annotation in R 4.7.2.1 Manually Annotating this method is great for when you are inserting 1 annotation, fixing an existing annotation or programmatically updating annotations for multiple attributeLists First you need to figure out the index of the attribute you want to annotate. eml_get_simple(doc$dataset$dataTable[[3]]$attributeList, &quot;attributeName&quot;) [1] &quot;prdM&quot; &quot;t090C&quot; &quot;t190C&quot; &quot;c0mS/cm&quot; &quot;c1mS/cm&quot; &quot;sal00&quot; &quot;sal11&quot; &quot;sbeox0V&quot; &quot;flECO-AFL&quot; [10] &quot;CStarTr0&quot; &quot;cpar&quot; &quot;v0&quot; &quot;v4&quot; &quot;v6&quot; &quot;v7&quot; &quot;svCM&quot; &quot;altM&quot; &quot;depSM&quot; [19] &quot;scan&quot; &quot;sbeox0ML/L&quot; &quot;sbeox0dOV/dT&quot; &quot;flag&quot; Next, assign an id to the attribute. It should be unique within the document, and it’s nice if it is human readable and related to the attribute it is describing. One format you could use is entity_x_attribute_y which should be unique in scope, and is nice and descriptive. doc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$id &lt;- &quot;entity_ctd_attribute_salinity&quot; Now, assign the propertyURI information. This will be the same for every annotation you build. doc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$annotation$propertyURI &lt;- list(label = &quot;contains measurements of type&quot;, propertyURI = &quot;http://ecoinformatics.org/oboe/oboe.1.2/oboe-core.owl#containsMeasurementsOfType&quot;) Finally, add the valueURI information from your search. You should see an ID on the Bioportal page that looks like a URL - this is the valueURI. Use the value to populate the label element. doc$dataset$dataTable[[3]]$attributeList$attribute[[6]]$annotation$valueURI &lt;- list(label = &quot;Water Salinity&quot;, valueURI = &quot;http://purl.dataone.org/odo/ECSO_00001164&quot;) 4.7.2.2 Shiny Attributes this method is great for when you are updating many attributes On the far right of the table of shiny_attributes there are 4 columns: id, propertyURI, propertyLabel, valueURI, valueLabel that can be filled out. 4.7.3 Dataset Annotations Dataset annotations can only be made using R. There are several helper functions that assist with making dataset annotations. 4.7.3.1 Data Sensitivity Sensitive datasets that might cover protected characteristics (human subjects data, endangered species locations, etc) should be annotated using the data sensitivity ontology: https://bioportal.bioontology.org/ontologies/SENSO/?p=classes&amp;conceptid=root. 4.7.3.2 Dataset Discipline As a final step in the data processing pipeline, we will categorize the dataset. We are trying to categorize datasets so we can have a general idea of what kinds of data we have at the Arctic Data Center. Datasets will be categorized using the Academic Ontology. These annotations will be seen at the top of the landing page, and can be thought of as “themes” for the dataset. In reality, they are dataset-level annotations. Be sure to ask your peers in the #datateam slack channel whether they agree with the themes you think best fit your dataset. Once there is consensus, use the following line of code: doc &lt;- arcticdatautils::eml_categorize_dataset(doc, c(&quot;Soil Science&quot;, &quot;Plant Science&quot;, &quot;Ecology&quot;)) Be careful not to duplicate dataset annotations. The above code does not existing prior dataset annotations. Duplicate annotations can be removed by setting them to NULL. doc$dataset$annotation[[i]] &lt;- NULL 4.8 Exercise 3a The metadata for the dataset created earlier in Exercise 2 was not very complete. Here we will add a attribute and physical to our entity (the csv file). Make sure your package from before is loaded into R. Convert otherEntity into dataTable. Replace the existing dataTable with a new dataTable object with an attributelist you write in R using the above commands. We will continue using the objects created and updated in this exercise in 3b. Below is some pseudo-code for how to accomplish the above steps. Fill in the dots according to the above sections to complete the exercise. # get the latest version of the resource map identifier from your dataset on the arctic data center resource_map_pid &lt;- ... dp &lt;- getDataPackage(d1c_test, identifier=resource_map_pid, lazyLoad=TRUE, quiet=FALSE) # get metadata pid metadataId &lt;- selectMember(...) # read in EML doc &lt;- read_eml(getObject(...)) # convert otherEntity to dataTable doc &lt;- eml_otherEntity_to_dataTable(...) # write an attribute list using shiny_attributes based on the data in your file ex_data &lt;- read.csv(...) atts &lt;- shiny_attributes(data = ex_data) # set the attributeList doc$dataset$dataTable$attributeList &lt;- set_attributes(...) 4.9 Validate EML and update package To make sure that your edited EML is valid against the EML schema, run eml_validate() on your EML. Fix any errors that you see. eml_validate(doc) You should see something like if everything passes: &gt;[1] TRUE &gt;attr(,“errors”) &gt;character(0) When troubleshooting EML errors, it is helpful to run eml_validate() after every edit to the EML document in order to pinpoint the problematic code. Then save your EML to a path of your choice or a temp file. You will later pass this path as an argument to update the package. # Create a standardized EML name from the dataset title eml_path &lt;- arcticdatautils::title_to_file_name(doc$dataset$title) write_eml(doc, eml_path) 4.10 Exercise 3b Make sure you have everything from before in R. After adding more metadata, we want to publish the dataset onto test.arcticdata.io. Before we publish updates we need to do a couple checks before doing so. Validate your metadata using eml_validate. Use the checklist to review your submission. Make edits where necessary (e.g. physicals) Once eml_validate returns TRUE go ahead and run write_eml, replaceMember, and uploadDataPackage. There might be a small lag for your changes to appear on the website. This part of the workflow will look roughly like this: # validate and write the EML eml_validate(...) write_eml(...) # replace the old metadata file with the new one in the local package dp &lt;- replaceMember(dp, metadataId, replacement = eml_path) # upload the data package packageId &lt;- uploadDataPackage(...) "],["updating-a-data-package.html", "Chapter 5 Updating a data package 5.1 Update an object 5.2 Update a package with a new data object 5.3 Exercise 4", " Chapter 5 Updating a data package This chapter will teach you how to edit and update an existing data package in R. Earlier, we updated metadata. In this section we will learn how to update a data file, and how to update a package by adding an additional data file. 5.1 Update an object To update a data file associated with a data package, you need to do three things: update the object itself, update the resource map (which affiliates the object with the metadata), and update the metadata that describes that object The datapack::replaceMember function takes care of the first two of these tasks. First you need to get the pid of the file you want to replace by using datapack::selectMember # Select the EML PID dataId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;text/csv&quot;) Then use replaceMember: # Replace the old file with the new dp &lt;- replaceMember(dp, dataId, replacement=file_path) If you want to remove some files from the data package we can use datapack::removeMember. If we wanted to remove all the zip files associated with this data package, we can use datapack::removeMember: zipId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;application/vnd.shp+zip&quot;) removeMember(dp, zipId, removeRelationships = T) You will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. Use line 2 (Id:) exactly, character for character. To accomplish the second task, you will need to update the metadata using the EML package. This is covered in Chapter 4. After you update a file, you will always need to update the metadata because parts of the physical section (such as the file size, checksum) will be different, and it may also require different attribute information. Once you have updated your metadata and saved it, you can update the package itself. 5.2 Update a package with a new data object Once you have updated the data objects and saved the metadata to a file, we can update the metadata and use replaceMember to update the package with the new metadata. Make sure you have the package you want to update loaded into R using dataone::getDataPackage(). 5.2.1 Publish update Now we can update your data package to include the new data object. Assuming you have updated your data package earlier something like the below: d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) packageId &lt;- &quot;resource_map_urn:uuid...&quot; dp &lt;- getDataPackage(d1c_test, identifier=packageId, lazyLoad=TRUE, quiet=FALSE) metadataId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;https://eml.ecoinformatics.org/eml-2.2.0&quot;) #some modification to the EML here eml_path &lt;- arcticdatautils::title_to_file_name(doc$dataset$title) write_eml(doc, eml_path) dp &lt;- replaceMember(dp, metadataId, replacement=eml_path) You can then upload your data package: myAccessRules &lt;- data.frame(subject=&quot;CN=arctic-data-admins,DC=dataone,DC=org&quot;, permission=&quot;changePermission&quot;) packageId &lt;- uploadDataPackage(d1c_test, dp, public=FALSE, accessRules=myAccessRules, quiet=FALSE) If a package is ready to be public, you can change the public argument in the datapack::uploadDataPackage() call to TRUE. If you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), you need to do this when replacing the metadata using the optional newId argument in replaceMember(). This should only be done after the package is finalized and has been thoroughly reviewed! doi &lt;- dataone::generateIdentifier(d1c_test@mn, &quot;DOI&quot;) dp &lt;- replaceMember(dp, metadataId, replacement=eml_path, newId=doi) newPackageId &lt;- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE) If there is a pre-issued DOI (researcher requested the DOI for the publication first), please do the following: dp &lt;- replaceMember(dp, metadataId, replacement=eml_path, newId=&quot;doi:10.../...&quot;) newPackageId &lt;- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE) If the package has children, see how to do this using arcticdatautils::publish_update in the nesting section of the reference manual. Refresh the landing page at test.arcticdata.io/#view/… for this package and then follow the “newer version” link to view the latest. 5.3 Exercise 4 What if the researcher notices that some information needed to be updated in the data file? We can use replaceMember to do just that! If you haven’t already: Locate the data package you published in the previous exercise by navigating to the URL test.arcticdata.io/#view/… Load the package and EML into R using the above commands. Make a slightly different data file to upload to test.arcticdata.io for this exercise: Load the data file associated with the package into R as a data.frame. (Hint: use read.csv() to upload the data file from your computer/the server.) Make an edit to the data in R (e.g. change one of the colnames to \"TEST\"). Save the edited data. (Hint: use write.csv(data, row.names = FALSE).) Upload the new csv file, get a new pid and publish those updates: Update the data file in the package with the edited data file using replaceMember. Update your package using uploadDataPackage. "],["editing-system-metadata.html", "Chapter 6 Editing system metadata 6.1 Edit sysmeta 6.2 Exercise 5", " Chapter 6 Editing system metadata Every object on the ADC (or the KNB) has “system metadata”. An object’s system metadata have information about the file itself, such as the name of the file (fileName), the format (formatId), who the rightsHolder is, what the accessPolicy is, and more. Sometimes we will need to edit system metadata in order to make sure that things on the webpage display correctly, or to ensure a file downloads from the website with the correct file name and extension. Although the majority of system metadata changes that need to be made are done automatically, sometimes we need to change aspects of the system metadata (or ‘sysmeta’ for short) manually. 6.1 Edit sysmeta To edit the sysmeta of an object (data file, EML, or resource map, etc.) with a PID, first load the sysmeta into R using the following command: sysmeta &lt;- getSystemMetadata(d1c_test@mn, pid) Then edit the sysmeta slots by using @ functionality. For example, to change the fileName use the following command: sysmeta@fileName &lt;- &#39;NewFileName.csv&#39; Note that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as datapack::addAccessRule() (which takes the sysmeta as an input) or arcticdatautils::set_rights_and_access(), which only requires a PID. In general, you most frequently need to use dataone::getSystemMetadata() to change either the formatId or fileName slots (see the DataONE list of format ids) for acceptable formats. # Example of setting the formatId slot sysmeta@formatId &lt;- &quot;eml://ecoinformatics.org/eml-2.1.1&quot; After you have changed the necessary slot, you can update the system metadata using the following command: updateSystemMetadata(mn, pid, sysmeta) 6.1.1 Identifiers and sysmeta Importantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata are accurate when an object is first published). 6.1.2 Additional resources For a more in-depth (and technical) guide to sysmeta, check out the DataONE documentation: System Metadata Data Types in CICore 6.2 Exercise 5 Sometimes the system doesn’t recognize the file types properly. For example you have a csv file but the File type on the website says Microsoft Excel Read the system metadata in from the data file you uploaded previously. Check to make sure the fileName and formatId are set correctly (the extension in fileName should match the formatId). Update the system metadata if necessary. CSVs have the formatId “text/csv”. "],["building-provenance.html", "Chapter 7 Building provenance 7.1 Using the prov editor 7.2 Understanding resource maps 7.3 Using the datapack package 7.4 References", " Chapter 7 Building provenance Note - It is rare for a dataset to have provenance - though we would like to see that change by encouraging researchers to submit scripts whenever it is reasonable. When processing datasets if you notice that provenance is needed let Daphne or Jeanette know. The provenance chain describes the origin and processing history of data. Provenance (or “prov”) can exist on a continuum, ranging from prose descriptions of the history, to formal provenance traces, to fully executable environments. In this section we will describe how to build provenance using formal provenance traces in DataONE. Provenance is becoming increasingly important in the face of what is being called a reproducibility crisis in science. J. P. A. Ioannidis (2005) wrote that “Most Research Findings Are False for Most Research Designs and for Most Fields”. Ioannidis outlined ways in which the research process has lead to inflated effect sizes and hypothesis tests that codify existing biases. The first step towards addressing these issues is to be able to evaluate the data, analyses, and models on which conclusions are drawn. Under current practice, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. And yet, this is easily remedied. Researchers can achieve computational reproducibility through open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)). At NCEAS and in the datateam, not only do we archive data and code openly, but we also describe the workflows that involve that data and code using provenance, formalizing the provenance trace for a workflow that might look like this into an easily understandable trace including archived data objects, such as what is shown here. There are two ways that we add provenance in the datateam - the prov editor and the R datapack package. 7.1 Using the prov editor Provenance can easily be added to production Arctic Data Center packages using the provenance editor on the Arctic Data Center. On the landing page of a data package within beta, in the dataTable or otherEntity section where you would like to add a provenance relationship, you can choose to add either a “source” or a “derivation”, to the left or right of the object pane, respectively. To add a source data file, click on the circle with the “+ add” text. Similarly, a source script would be added by selecting the arrow. Selecting the circle to add a source file pulls up the following screen, where you can select the source from other data objects within the same data package. A data package with an object that has multiple sources added will look like this. For simple packages on the Arctic Data Center, adding prov through the prov editor is super easy! 7.2 Understanding resource maps Before we dive further into constructing prov in R, we need to talk more about resource maps (or “resmaps”). All data packages have a single resource map. But what is a resource map and how do we use one to find out what objects are in a particular data package? This document is a short introduction but a more complete guide can be found here. A resource map is a special kind of XML document that describes (among other things) an “aggregation”. The aggregation describes the members of a data package (metadata and data, usually). We can use the dataone R package to download a resource map if we know its PID: library(dataone) d1c_test &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) pid &lt;- &quot;urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot; # A resource map PID path &lt;- tempfile(fileext = &quot;.xml&quot;) # We&#39;re saving to a temporary file but you can save elsewhere writeLines(rawToChar(getObject(d1c_test@mn, pid)), path) # Write the object to `path` If we open that file up in a text editor, we see this: &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; &lt;rdf:RDF xmlns:cito=&quot;http://purl.org/spar/cito/&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:dcterms=&quot;http://purl.org/dc/terms/&quot; xmlns:foaf=&quot;http://xmlns.com/foaf/0.1/&quot; xmlns:ore=&quot;http://www.openarchives.org/ore/terms/&quot; xmlns:prov=&quot;http://www.w3.org/ns/prov#&quot; xmlns:provone=&quot;http://purl.dataone.org/provone/2015/01/15/ontology#&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot; xmlns:rdfs=&quot;http://www.w3.org/2000/01/rdf-schema#&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema#&quot;&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;&gt; &lt;cito:isDocumentedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;rdf:type rdf:resource=&quot;http://www.openarchives.org/ore/terms/Aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;cito:isDocumentedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;cito:documents rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;cito:documents rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;&gt; &lt;ore:isAggregatedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;dc:title&gt;DataONE Aggregation&lt;/dc:title&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot;&gt; &lt;dcterms:identifier rdf:datatype=&quot;http://www.w3.org/2001/XMLSchema#string&quot;&gt;urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556&lt;/dcterms:identifier&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot;&gt; &lt;ore:describes rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;ore:isAggregatedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;ore:aggregates rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;ore:aggregates rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot;&gt; &lt;rdf:type rdf:resource=&quot;http://www.openarchives.org/ore/terms/ResourceMap&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;&gt; &lt;dcterms:identifier rdf:datatype=&quot;http://www.w3.org/2001/XMLSchema#string&quot;&gt;urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27&lt;/dcterms:identifier&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;dcterms:identifier rdf:datatype=&quot;http://www.w3.org/2001/XMLSchema#string&quot;&gt;urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5&lt;/dcterms:identifier&gt; &lt;/rdf:Description&gt; &lt;/rdf:RDF&gt; Whoa! What is this thing and how do you read it to find the members of the data package? The short answer is to look for lines like this: &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;ore:aggregates rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; This line says “The aggregation aggregates urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5” so that means urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5 is in our data package! The key bit is the &lt;rdf:Description rdf:about=\"...#aggregation part. If you look for another similar statement, you’ll also see that urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27 is part of our data package. Now we know which objects are in our data package but we don’t know which one contains metadata and which one contains data. For that, we need to get a copy of the system metadata for each object: getSystemMetadata(d1c_test@mn, &quot;urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5&quot;)@formatId getSystemMetadata(d1c_test@mn, &quot;urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;)@formatId From the formatIds, we can see the first PID is the EML (formatId: [1]\"eml://ecoinformatics.org/eml-2.1.1\") and the second PID is a data object (formatId: [1]\"application/octet-stream\"). Now we know enough to know what’s in the data package: Resource map: urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556 Metadata: urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5 Data: urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27 Now that you’ve actually seen a resource map, we can dive further into prov. 7.3 Using the datapack package For packages not on the ADC, or packages that are extremely complicated, it may be best to upload prov relationships using R. The datapack package has several functions which help add relationships in a very simple way. These relationships are stored in the resource map. When you update a package to only add prov, the package will not be assigned any new identifiers with the exception of the resource map. First, we set the environment, in a similar, but slightly different way than what you may be used to. Here the function D1Client() sets the DataONE client with the coordinating node instance as the first argument, and member node as the second argument. library(dataone) library(datapack) d1c &lt;- D1Client(&quot;STAGING2&quot;, &quot;urn:node:mnTestKNB&quot;) Next, get the PID of the resource map of the data package you are adding prov to, and load that package into R using the getDataPackage() function. packageId &lt;- &quot;urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b&quot; dp &lt;- getDataPackage(d1c, id=packageId, lazyLoad=TRUE, limit=&quot;0MB&quot;, quiet=FALSE) Printing pkg in your console shows you the contents of the data package, including all of the objects and their names: &gt; pkg Members: filename format mediaType size identifier modified local esc...er.R application/R NA 888 knb.92049.1 n n PWS....csv text/csv NA 1871469 knb.92050.1 n n PWS....csv text/csv NA 1508128 knb.92051.1 n n NA eml:/...-2.1.1 NA 15658 urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b n y Package identifier: resource_map_urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b RightsHolder: http://orcid.org/0000-0002-2192-403X It will also show the existing relationships in the resource map, which in this case are mostly the “documents” relationships that specify that the metadata record is describing all of these data files. Relationships: subject predicate object 2 esc_reformatting_PWSweirTower.R cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b 4 PWS_weirTower.csv cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b 1 PWS_Weir_Tower_export.csv cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b 3 urn:uuid:8f501606-...4-b22d-050a4176a97b dcterms:creator _r1515542097r415842r1 5 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents esc_reformatting_PWSweirTower.R 6 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents PWS_weirTower.csv 7 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents PWS_Weir_Tower_export.csv 8 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents urn:uuid:8f501606-...4-b22d-050a4176a97b 9 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b In this example above, the data package has two .csv files, with an R script that converts one to the other. To create our provenance trace, first we need to select the source object, and save the PID to a variable. We do this using the selectMember() function, and we can query part of the system metadata to select the file that we want. This function takes the data package (pkg), the name of the sysmeta field to query (in this case we use the fileName), and the value that you want to match that field to (in this case, ‘PWS_Weir_Tower_export.csv’). sourceObjId &lt;- selectMember(dp, name=&quot;sysmeta@fileName&quot;, value=&#39;PWS_Weir_Tower_export.csv&#39;) This returns a list of the source object PIDs that match the query (in this case only one object matches). &gt; sourceObjId [1] &quot;knb.92051.1&quot; Now we need to select our output object. Here, we use the selectMember() function again, and save the result to a new variable. outputObjId &lt;- selectMember(dp, name=&quot;sysmeta@fileName&quot;, value=&#39;PWS_weirTower.csv&#39;) Now we query for the R script. In this case, we query based on the value of the formatId as opposed to the fileName. This can be useful if you wish to select a large list of PIDs that are all similar. programObjId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;application/R&quot;) Next, you use these lists of PIDs and a function called describeWorkflow() to add these relationships to the data package. Note that if you do not have a program in the workflow, or a source file, you can simply leave those arguments blank. dp &lt;- describeWorkflow(dp, sources=sourceObjId, program=programObjId, derivations=outputObjId) Viewing pkg again confirms that these relationships have been inserted into the data package, as shown by the wasDerivedFrom and wasGeneratedBy statements. It is always a good idea to print pkg to confirm that your PID selection process worked as expected, and your prov relationships make sense. Relationships (updated): subject predicate object 15 _1db49d06-ae98-4...9101-39f7c0b45a95 rdf:type prov:Association 14 _1db49d06-ae98-4...9101-39f7c0b45a95 prov:hadPlan esc_reformatting_PWSweirTower.R 1 esc_reformatting_PWSweirTower.R cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b 16 esc_reformatting_PWSweirTower.R rdf:type provone:Program 8 PWS_weirTower.csv cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b 11 PWS_weirTower.csv rdf:type provone:Data 20 PWS_weirTower.csv prov:wasDerivedFrom PWS_Weir_Tower_export.csv 19 PWS_weirTower.csv prov:wasGeneratedBy urn:uuid:3dd59b0...bc38-3b5d8fa644ac 6 PWS_Weir_Tower_export.csv cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b 10 PWS_Weir_Tower_export.csv rdf:type provone:Data 9 _r1515544826r415842r1 foaf:name DataONE R Client 17 urn:uuid:3dd59b0...bc38-3b5d8fa644ac dcterms:identifier urn:uuid:3dd59b0...bc38-3b5d8fa644ac 13 urn:uuid:3dd59b0...bc38-3b5d8fa644ac rdf:type provone:Execution 12 urn:uuid:3dd59b0...bc38-3b5d8fa644ac prov:qualifiedAssociation _1db49d06-ae98-4...9101-39f7c0b45a95 18 urn:uuid:3dd59b0...bc38-3b5d8fa644ac prov:used PWS_Weir_Tower_export.csv 5 urn:uuid:8f50160...b22d-050a4176a97b cito:documents esc_reformatting_PWSweirTower.R 4 urn:uuid:8f50160...b22d-050a4176a97b cito:documents PWS_weirTower.csv 3 urn:uuid:8f50160...b22d-050a4176a97b cito:documents PWS_Weir_Tower_export.csv 2 urn:uuid:8f50160...b22d-050a4176a97b cito:documents urn:uuid:8f50160...b22d-050a4176a97b 7 urn:uuid:8f50160...b22d-050a4176a97b cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b Finally, you can upload the data package using the uploadDataPackage() function, which takes the DataONE client d1c we set in the beginning, the updated pkg variable, some options for public read, and whether or not informational messages are printed during the upload process. First, you will need to run a token obtained from https://dev.nceas.ucsb.edu/ to publish to that node. resmapId_new &lt;- uploadDataPackage(d1c, dp, public = TRUE, quiet = FALSE) If successful you should be able to navigate to the landing page of your dataset, and icons should show up where the sources and derivations are, such as in this example: 7.3.1 Fixing mistakes If you messed up updating a data package using datapack, there unfortunately isn’t a great way to undo your work, as the describeWorkflow() only adds prov relationships, it does not replace them. If you messed up, the best course of action is to update the resource map with a clean version that does not have prov using update_resource_map(), and then go through the steps outlined above again. Note: this has not been thoroughly tested, and more extreme actions may be necessary to fully nuke the prov relationships. Consult Peter or Jeanette if things do not work as expected. 7.4 References Ioannidis, John P A. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124. Hampton, Stephanie E, Sean Anderson, Sarah C Bagby, Corinna Gries, Xueying Han, Edmund Hart, Matthew B Jones, et al. 2015. “The Tao of Open Science for Ecology.” Ecosphere 6 (July). https://doi.org/10.1890/ES14-00402.1. Munafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1 (1): 0021. https://doi.org/10.1038/s41562-016-0021. "],["using-git.html", "Chapter 8 Using Git 8.1 Usage in the datateam 8.2 What is Git? 8.3 Setting up Git 8.4 GitHub Authentication 8.5 Working with the repository 8.6 My Git tab disappeared", " Chapter 8 Using Git 8.1 Usage in the datateam We use git and Github to manage our packages (ie datamgmt, arcticdatautils) and even this document! 8.2 What is Git? Git is a distributed version control system. The NCEAS Github repository contains many processing scripts and templates that can help when working with datasets. Important! If you have never used Git before, or only used it a little, or have no idea what it is, check out this intro to Git put together by the ecodatascience group at UCSB. Don’t worry too much about the forking and branching sections, as we will primarily be using the basic commit-pull-push commands. After you have read through that presentation, come back to this chapter. 8.2.1 So why do I need to use this again? There are several reasons why using the arctic-data GitHub repository is helpful, both for you and for the rest of the data team. Here are a few: Versioning: Did you accidentally make a change to your code and can’t figure out why it broke? Do you wish you could go back to the version that worked? If you add your code to the GitHub repo you can do this! Reproducibility: Being able to reproduce how you accomplished something is incredibly important. We should be able to tell anyone exactly how data have been reformatted, how metadata have been altered, and how packages have been created. As a data center, this is especially important for us with data team members that stay for 6-12 months because we may need to go back and figure out how something was done after the intern or fellow who wrote the code left the team. Troubleshooting: If you are building a particularly complicated EML, or doing some other advanced task, it is much easier for others to help you troubleshoot your code if it is on the GitHub repo. We can view, troubleshoot, and fix bugs very easily when code is on the GitHub repo, with the added bonus of being able to go back a version if something should break. Solve future problems: Some of the issues we see in ADC submissions come up over and over again. When all of our code is on GitHub, we can easily reference code built for other submissions, instead of trying to solve the same problems over and over again from scratch. 8.3 Setting up Git Before using git, you need to tell it who you are. The only way to do this is through the command line. When you open RStudio, you should see a Terminal tab located to the right of the Console tab. If the Terminal tab is not visible, you can open it by selecting Tools -&gt; Terminal -&gt; New Terminal at the top of your RStudio window. To tell git who you are, you’re going to set the global options. This includes setting your name, email address, PAT (Personal Access Token), and caching the PAT. Make sure you run the following commands one at a time. Type the following command in the Terminal window, with your actual name, and press enter. If you do this correctly, it will look as though nothing happened. git config --global user.name &quot;Your Name&quot; Next, enter the following line, with the email address associated with your GitHub account. git config --global user.email MyEmail@domain.com After running these commands, the Terminal window should look like this: Next, we will tell Git to store your PAT. We must do this because of the way our server operating system handles credentials. If you don’t run the next line, your PAT will expire immediately on the server, even though we will set it up on GitHub to be valid for 90 days. git config --global credential.helper &#39;cache -- timeout=10000000&#39; Finally, check to make sure everything looks correct by entering the following command, which will return the options that you have set. git config --global --list 8.4 GitHub Authentication GitHub recently deprecated password authentication for accessing repositories, so we need to set up a secure way to authenticate. That’s where the PAT comes in. In the tabs at the bottom of your RStudio window, make sure to switch from the Terminal to the Console. Now that, you’ve done that, follow these steps: Run usethis::create_github_token() In the browser window that pops up, log into GitHub In the “Expiration” drop down menu, select “90 days” Click “Generate token” in the green box at the bottom of the page On the next page, copy the token from the green box Back in RStudio, run credentials::set_github_pat() Paste your token into the dialog box that pops up You’ve now successfully set your PAT in RStudio, and connected R, Git, and GitHub. Great job! Now, you’re going to clone the repository we work in so that you can version control your code. 8.4.1 Cloning the arctic-data repo To clone the arctic-data repository, navigate to the repository on GitHub: https://github.nceas.ucsb.edu/KNB/arctic-data/. If you are not already logged on, this will be the enterprise account you use to log in when you go to RT. If you have trouble with this, send a message to Nick Outin (@Nick) in the datateam slack channel. Once you are logged on and can view the above link, click on the green “Code” button, and copy the URL in the gray box. In your RStudio window, click File -&gt; New Project -&gt; Version Control -&gt; Git. Paste the URL you just copied into the “Repository URL” box, and press tab on your keyboard. This should autofill the “Project directory name.” If it does not, then type in arctic-data. Fill it out as shown in the image below to clone the arctic-data repository into the top level of your home directory. Click “Create Project,” and now you should have a directory called arctic-data in your RStudio files window. If you are prompted to save your workspace during this process, make sure all of your work is saved, and you don’t need anything in your environment, and then click ‘Don’t Save’. 8.5 Working with the repository 8.5.1 Adding a new script If you have been working on a script that you want to put in the arctic-data GitHub repo, you first need to save it somewhere in the arctic-data folder you cloned to your account on the Datateam server (/home/username/…). You can do this by either moving your script into the folder or using the save-as functionality. Note that Git will try and version anything that you save in this folder, so you should be careful about what you save here. For our purposes, things that probably shouldn’t be saved in this folder include: Tokens: Any token file or script with a token in it should NOT be saved in the repository. Others could steal your login credentials if you put a token in GitHub. Data files: Git does not version data files very well. You shouldn’t save any .csv files or any other data files (including metadata). Workspaces/.RData: If you are in the habit of saving your R workspace, you shouldn’t save it in this directory. Plots/Graphics: For the same reasons as data files. Note: Do not EVER make a commit that you don’t understand. If something unexpected (like a file you have never worked on) shows up in your Git tab, ask for help before committing. After you save your script in the appropriate place within the arctic-data folder, it will show up in your Git tab looking like this: Before you commit your changes, you need to click the little box under “Staged”. Do not stage or commit any .Rproj file. After clicking the box for your file, click “Commit” to commit your changes. In the window that pops up (you may need to force the browser to allow pop-ups), write your commit message. Always include a commit message. Remember that the commit message should be a concise description of the changes that were made to a file. Your window should look like this: Push ‘Commit’, and your commit will be saved to your local repository (this will not push it to the remote repository, yet). Now you want to merge the commits you made with the master version of the remote repository. You do this by using the command “Push.” But before you push, you always need to pull first to avoid merge conflicts. Pulling will merge the current version of the remote repository with your local repository, on your local machine. Click “Pull” and type in your credentials. Then, assuming you don’t have a merge conflict, you can push your changes by clicking “Push”. Always remember, the order is commit-pull-push. 8.5.2 Editing a script If you want to change a script, the workflow is the same. Just open the script that was saved in the arctic-data folder on your server account, make your changes, save the changes, stage them by clicking the box, commit, pull, then push to merge your version with the main version on the website. Do NOT edit scripts using the GitHub website. It is much easier to accidentally overwrite the history of a file this way. One thing you might be wondering as you are working on a script is, how often should I be committing my changes? It might not make sense to commit-pull-push after every single tiny change - if only because it would slow you way down. Personally, I commit every time I feel that a significant change has happened and that the chunk of code I was working on is “done”. Sometimes this is an entire script, other times it is just a few lines within a script. A good sign that you are committing too infrequently might be if many of your commit messages address a wide variety of coding tasks, such as: “wrote for loop to create referenced attribute lists for tables 1:20. also created nesting structure for this package with another package. also created attribute list for data table 40”. One final note, you can make multiple commits before you push to the repo. If you are making lots of changes to the script, you might want to make several commits before pull-push. You can see how many commits you are ahead of the “origin/master” branch (i.e. what you see on the website) by looking for text in your Git tab in RStudio that looks like this: 8.5.3 Where do I commit? The default right now is to save data-processing scripts in the arctic-data/datateam/data-processing/ directory, with sub-folders listed by project. Directories can be created as needed but please ask Dom or Jesse first so we can try and maintain some semblance of order in the file structure. The Github workflow diagram from stamminator shows each step and their functions explicitly. 8.6 My Git tab disappeared Sometimes R will crash so hard it loses your project information, causing your Git tab to disappear. Most likely, RStudio has just closed your “project” and all you need to do is reopen it. If your Git tab has disappeared, in the top right of your RStudio session, you should see a little R logo with “Project: (None)” next to it. This means you do not currently have a project open. Clicking the arrow should give you a dropdown menu of recent projects, where you can select “arctic-data” or “sasap-data.” Once you have opened your project, the Git tab should reappear! This is also a convenient way to switch between projects if you are working in multiple repositories. "],["first-ticket.html", "Chapter 9 First Ticket 9.1 Navigate RT 9.2 Initial review checklist 9.3 Processing templates 9.4 Final Checklist 9.5 Email templates 9.6 Categorize datasets 9.7 Congrats!", " Chapter 9 First Ticket After completing the previous chapters, your supervisor will assign a ticket from RT. Login using your LDAP credentials got get familiarized with RT. 9.1 Navigate RT The RT ticketing system is how we communicate with folks interacting with the Arctic Data Center. We use it for managing submissions, accessing issues, etc. It consists of three separate interfaces: Front Page All Tickets Ticket Page 9.1.1 Front page This is what you see first Home - brings you to this homepage Tickets - to search for tickets (also see number 5) Tools - not needed New Ticket - create a new ticket Search - Type in the ticket number to quickly navigate to a ticket Queue - Lists all of the tickets currently in a particular queue (such as ‘arcticdata’) and their statuses New = unopened tickets that require attention Open = tickets currently open and under investigation and/or being processed by a support team member Stalled = tickets awaiting responses from the PI/ submitter Tickets I Own - These are the current open tickets that are claimed by me Unowned Tickets - Newest tickets awaiting claim Ticket Status - Status and how long ago it was created Take - claim the ticket as yours 9.1.2 All tickets This is the queue interface from number 6 of the Front page 1. Ticket number and title 2. Ticket status 3. Owner - who has claimed the ticket 9.1.3 Example ticket Title - Include the PI’s name for reference Display - homepage of the ticket History - Comment/Email history, see bottom of Display page Basics - edit the title, status, and ownership here People - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ submitter email addresses should be listed as “Requestors”. Requestors are only emailed on “Replys”, not “Comments”. Ensure your ticket has a Requestor before attempting to contact users/ PIs/ submitters Links - option to “Merge into” another ticket number if this is part of a larger conversation. Also option to add a reference to another ticket number Verify that this is indeed the two tickets you want to merge. It is non-reversible. Actions Reply - message the submitter/ PI/ all watchers Comment - attach internal message (no submitters, only Data Teamers) Open It - Open the ticket Stall - submitter has not responded in greater than 1 month Resolve - ticket completed History - message history and option to reply (to submitter and beyond) or comment (internal message) 9.1.4 New data submission When notified by Arcticbot about a new data submission, here are the typical steps: Update the Requestor under the People section based on the email given in the submission (usually the user/ PI/ submitter). You may have to google for the e-mail address if the PI did not include it in the metadata record. Take the ticket (Actions &gt; Take) Review the submission based on the checklist Draft an email using the template and let others review it via Slack Send your reply via Actions Before opening a R script first look over the initial checklist first to identify what you will need to update in the metadata. 9.2 Initial review checklist Before responding to a new submission use this checklist to review the submission. When your are ready to respond use the initial email template and insert comments and modify as needed. 9.2.0.1 Sensitive Data If any of the below is in the dataset, please alert the #arctica team know before proceeding. Check if there is any sensitive information or personal identifying information in the data (eg. Names) Can the data be disaggregated and de-anonymized? (eg. a small sample size and individuals could be easily identified by their answers) Dryad Human Subject data guidelines can be a good place to start Common Cases: Social Science: Any dataset involving human subjects (may include awards awarded by ASSP and topics such as COVID-19) Archaeology: archaeological site location information, which is protected from public access by law Biology: protected species location coordinates 9.2.0.2 Data citations If the dataset appears to be in a publication please (might be in the abstract) make sure that those citations are registered. 9.2.0.3 Title WHAT, WHERE, and WHEN: Is descriptive of the work (provides enough information to understand the contents at a general scientific level), AND includes temporal coverage Provides a location of the work from the local to state or country level Provides a time frame of the work NO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out 9.2.0.4 Abstract Describes the DATA as well as: The motivation (purpose) of the study Where and when the research took place At least one sentence summarizing general methodologies NO UNDEFINED ACRONYMS, ABBREVIATIONS, nor INITIALISMS unless approved of as being more widely-known in that form than spelled out At least 100 words total tags such as &lt;superscript&gt;2&lt;/superscript&gt; and &lt;subscript&gt;2&lt;/subscript&gt; can be used for nicer formatting Any citations to papers can be registered with us 9.2.0.5 Keywords Some keywords are included 9.2.0.6 Data Data is normalized (if not suggest to convert the data if possible) At least one data file, or an identifier to the files at another approved archive, unless funded by ASSP (Arctic Social Sciences Program) No xls/xlsx files (or other proprietary files) File contents and relationships among files are clear Each file is well NAMED and DESCRIBED and clearly differentiated from all others All attributes in EML match attribute names in respective data files EXACTLY, are clearly defined, have appropriate units, and are in the same order as in the file. Quality control all dimensionless units. Missing value codes are explained (WHY are the data absent?) If it is a .rar file -&gt; scan the file If there is the unit tons make sure to ask if it is metric tons or imperical tons if not clarified already 9.2.0.7 People &amp; Parties At least one contact and one creator with a name, email address, and ORCID iD 9.2.0.8 Coverages Includes coverages that make sense Temporal coverage - Start date BEFORE end date Geologic time scales are added if mentioned in metadata (e.g. 8 Million Years or a name of a time period like Jurassic) Spatial coverage matches geographic description (check hemispheres) Geographic description is from the local to state or country level, at the least Taxonomic coverage if appropriate 9.2.0.9 Project Information At least one FUNDING number Title, personnel, and abstract match information from the AWARD (not from the data package) 9.2.0.10 Methods This section is REQUIRED for ALL NSF-FUNDED data packages Enough detail is provided such that a reasonable scientist could interpret the study and data for reuse without needing to consult the researchers, nor any other resources 9.2.0.11 Portals If there are multiple submissions from the same people/project let them know about the portals feature If this is part of a portal make sure this dataset can be found there. Additional steps might be needed to get that to work. Please consult Jeanette and see the data portals section. 9.3 Processing templates We have developed some partially filled R scripts to get you started on working on your first dataset. They outline common functions used in processing a dataset. However, it will differ depending on the dataset. You can use this template where you can fill in the blanks to get familiar with the functions we use and workflow at first. We also have a more minimal example A filled example as a intermediate step. You can look at the filled example if you get stuck or message the #datateam. In addition, you may find this cheat sheet of data team R functions helpful. Once you have updated the dataset to your satisfaction and reviewed the Final Checklist, post the link to the dataset on #datateam for peer review. 9.4 Final Checklist You can click on the assessment report on the website to for a general check. Fix anything you see there. Send the link over slack for peer review by your fellow datateam members. Usually we look for the following (the list is not exhaustive): 9.4.1 Special Datasets Please refer to the dedicated pages for instructions to handle these cases: MOSAiC DBO 9.4.2 System Metadata the format ids are correct 9.4.3 General EML Ethical Research Practice section is completed Publisher and system information have been added: doc &lt;- eml_add_publisher(doc) doc &lt;- eml_add_entity_system(doc) 9.4.4 Title Include geographic and temporal coverage (when and where data was collected) Abbreviations are removed or defined 9.4.5 Abstract longer than 100 words Abbreviations are removed or defined. No garbled text Tags such as &lt;superscript&gt;2&lt;/superscript&gt; and &lt;subscript&gt;2&lt;/subscript&gt; can be used for nicer formatting 9.4.6 DataTable / OtherEntity / SpatialVectors In the correct one: DataTable / OtherEntity / SpatialVector / SpatialRaster for the file type entityDescription - longer than 5 words and unique physical present and up-to-date and format is correct 9.4.6.1 Attribute Table Complete attributeDefinitions longer than 3 words Variables match what is in the file Measurement domain - if appropirate (ie dateTime correct) Missing Value Code - accounted for if applicable Semantic Annotation - appropriate semantic annotations added, especially for spatial and temporal variables: lat, lon, date etc. Custom units were created if necessary 9.4.7 People Complete information for each person in each section includes ORCID and e-mail address for all contacts people repeated across sections have consistent information 9.4.8 Geographic region the map looks correct and matches the geographic description check if negatives (-) are missing 9.4.9 Project If it is an NSF award you can use the helper function: doc$dataset$project &lt;- eml_nsf_to_project(awards) for other awards that need to be set manually, see the set project page 9.4.10 Methods Present No garbled text Acronyms and units are defined the first time they are used 9.4.11 Check EML Version Currently using: eml-2.2.0 (as of July 30 2020) Review to see if the EML version is set correctly by reviewing the doc$`@context` that it is indeed 2.2.0 under eml Re-run your code again and have the lineemld::eml_version(\"eml-2.2.0\") at the top Make sure the system metadata is also 2.2.0 9.4.12 Access If necessary, granted access to PI using set_rights_and_access() make sure it is http:// (no s) note if it is a part of portals there might be specific access requirements for it to be visible using set_access() 9.4.13 SFTP Files If there are files transferred to us via SFTP, delete those files once they have been added to the dataset and when the ticket is resolved 9.4.14 Updated datasets All the above applies. These are some areas to do a closer check when users update with a new file: New data was added Temporal Coverage and Title Confirm if new data used same methods or if methods need updating Files were replaced update physical and entityName double-check attributes are the same check for any new missing value codes that should be accounted for Was the dataset published before 2021? update project info, annotations Glance over entire page for any small mistakes (ie. repeated additionalMetadata, any missed &amp;amps, typos) After all the revisions send the link to the PI in an email through RT. Send the draft of the email to Daphne or Jeanette on Slack. 9.5 Email templates This section covers new data packages submitted. For other inquiries see the PI FAQ templates Please think critically when using these canned replies rather than just blindly sending them. Typically, content should be adjusted/ customized for each response to be as relevant, complete, and precise as possible. In your first few months, please run email drafts by the #datateam Slack and get approval before sending. Remember to consult the submission guidelines for details of what is expected. Quick reference: Initial email template Final email templates Additional email template 9.5.1 Initial email template Hello [NAME OF REQUESTOR], Thank you for your recent submission to the NSF Arctic Data Center! From my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below: [COMMENTS HERE] After we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface. Best, [YOUR NAME] 9.5.2 Comment templates based on what is missing 9.5.2.1 Portals Multiple datasets under the same project - suggest data portal feature I would like to highlight the Data Portals feature that would enhance the ability of your datasets to be discovered together. It also enables some custom branding for the group or project. Here is an example page that we created for the Distributed Biological Observatory: https://arcticdata.io/catalog/portals/DBO. We highly suggest considering creating a portal for your datasets, you can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/. Data portals can be set up at any point of your project. If they ask to nest the dataset We are no longer supporting new nested datasets. We recommend to create a data portal instead. Portals will allow more control and has the same functionality as a nested dataset. You can get one started here: https://arcticdata.io/catalog/edit/portals/new/Settings. More information on how to set up can be found here: https://arcticdata.io/data-portals/, but as always we are here to help over email. 9.5.2.2 Dataset citations If there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be done at any point. 9.5.2.3 Title Provides the what, where, and when of the data We would like to add some more context to your data package title. I would like to suggest: ‘OUR SUGGESTION HERE, WHERE, WHEN’. Does not use acronyms We wanted to clarify a couple of abbreviations. Could you help us in defining some of these: [LIST OF ACRONYMS TO DEFINE HERE] 9.5.2.4 Abstract Describes DATA in package (ideally &gt; 100 words) We would like to add some additional context to your abstract. We hope to add the following: [ADJUST THE DEPENDING ON WHAT IS MISSING] The motivation of the study Where and when the research took place At least one sentence summarizing general methodologies All acronyms are defined At least 100 words long Offer this if submitter is reluctant to change: If you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [NSF AWARD URL]. 9.5.2.5 Keywords We noticed that there were no keywords for this dataset. Adding keywords will help your dataset be discovered by others. 9.5.2.6 Data Sensitive Data We will need to ask these questions manually until the fields are added to the webform. Data sensitivity categories Once we have the ontology this question can be asked: Based on our Data sensitivity categories, which of the 3 does your dataset align with most: Non-sensitive data - None of the data includes sensitive or protected information. Some or all data is sensitive with minimal risk - Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized. Some or all data is sensitive with significant risk - The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body. Ethical research proceedures We were wondering if you could also address this question specifically on Ethical Research Procedures: Describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Insitutional Review Board approvals, consent waivers, procedures for co-production, data sovreignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution. We can help add your answers to the question to the metadata. Adding provenance Is the [mention the file names here] files related? If so we can add provenance to show the relationship between the files. Here is an example of how that is displayed: https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2WS8HM6C#urn%3Auuid%3Af00c4d71-0242-4e9d-9745-8999776fa2f2 At least one data file We noticed that no data files were submitted. With the exception of sensitive social science data, we seek to include all data products prior to publication. We wanted to check if additional files will be submitted before we move forward with the submission process. Open formats Example using xlsx. Tailor this reponse to the format in question. We noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term. The data files can be replaced by going to the green Edit button &gt; Click the black triangle by the Describe button for the data file &gt; Select Replace (attached is also a screenshot on how to get there). Zip files Except for very specific file types, we do not recommend that data are archived in zip format. Data that are zipped together can present a barrier to reuse since it is difficult to accurately document each file within the zip file in a machine readable way. File contents and relationships among files are clear Could you provide a short description of the files submitted? Information about how each file was generated (what software, source files, etc.) will help us create more robust metadata for long term use. Data layout Would you be able to clarify how the data in your files is laid out? Specifically, what do the rows and columns represent? We try not to prescribe a way the researchers must format their data as long as reasonable. However, in extreme cases (for example Excel spreadsheets with data and charts all in one sheet) we will want to kindly ask them to reformat. We would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. [DESCRIBE WHAT MAY NEED TO BE CHANGED IN THE DATA SET]. Our data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help. 9.5.2.7 Attributes Identify which attributes need additional information. If they are common attributes like date and time we do not need further clarification. Checklist for the datateam in reviewing attributes (NetCDF, CSV, shapefiles, or any other tabular datasets): A name (often the column or row header in the file). A complete definition. Any missing value codes along with explanations for those codes. For all numeric data, unit information is needed. For all date-time data, a date-time format is needed (e.g. “DD-MM-YYYY”). For text data, full descriptions for all patterns or code/definition pairs are needed if the text is constrained to a list of patterns or codes. Helpful templates: &gt; We would like your help in defining some of the attributes. Could you write a short description or units for the attributes listed? [Provide a the attribute names in list form] &gt; Could you describe ____? &gt; Please define “XYZ”, including the unit of measure. &gt; What are the units of measurement for the columns labeled “ABC” and “XYZ”? Missing value codes What do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice. This section is not yet available on our webform so we will add that information on your behalf. We noticed that the data files contain [blank cells - replace with missing values found]. What do these represent? 9.5.2.8 Funding All NSF funded datasets need a funding number. Non-NSF funded datasets might not have funding numbers, depending on the funding organization. We noticed that your dataset does not appear to contain a funding number. The field accepts NSF funding numbers as well as other numbers by different organizations. 9.5.2.9 Methods We noticed that methods were missing from the submission. Submissions should include the following: provide instrument names (if applicable) specify how sampling locations were chosen if citations for sampling methods are used, please provide a brief summary of the methods referenced any software used to process the data Note - this includes software submissions as well (see https://arcticdata.io/submit/#metadata-guidelines-for-software) Your methods section appears to be missing some information. [ADJUST THIS DEPENDING ON WHAT IS MISSING - Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files.] Comprehensive methods information should be included directly in the metadata record. Pointers or URLs to other sites are unstable. A full example - New Submission: methods, excel to csv, and attributes Thank you for your recent submission to the NSF Arctic Data Center! From my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below: If there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. This can be added at any point. We would like to add some more context to your data package title. I would like to suggest: Holocene lake-based Arctic glacier and ice cap records. We noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format); this helps ensure your data are usable in the long-term. We also would like to suggest a couple of modifications to the structure of your data. This will others to re-use it most effectively. If the data was in a long rather than wide format, it would be easier to us to document. Our data submission guidelines page (https://arcticdata.io/submit/) outlines what are best practices for data submissions to the Arctic Data Center. Let us know if you have any questions or if we can be of any help. What do the missing values in your measurements represent? A short description of the reason why the values are missing (instrument failure, site not found, etc.) will suffice. We noticed that methods were missing from the submission. Submissions should: - provide instrument names (if applicable) - specify how sampling locations were chosen - provide citations for sampling methods that are not explained in detail - any software used to process the data After we receive your responses, we can make the edits on your behalf, or you are welcome to make them yourself using our user interface. Best, Name 9.5.3 Final email templates 9.5.3.1 Asking for approval Hi [submitter], I have updated your data package and you can view it here after logging in: [URL] Please review and approve it for publishing or let us know if you would like anything else changed. For your convenience, if we do not hear from you within a week we will proceed with publishing with a DOI. After publishing with a DOI, any further changes to the dataset will result in a new DOI. However, any previous DOIs will still resolve and point the user to the newest version. Please let us know if you have any questions. 9.5.3.2 DOI and data package finalization comments Replying to questions about DOIs We attribute DOIs to data packages as one might give a DOI to a citable publication. Thus, a DOI is permanently associated with a unique and immutable version of a data package. If the data package changes, a new DOI will be created and the old DOI will be preserved with the original version. DOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to that latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists. Clarification of updating with a DOI and version control We definitely support updating a data package that has already been assigned a DOI, but when we do so we mark it as a new revision that replaces the original and give it its own DOI. We do that so that any citations of the original version of the data package remain valid (i.e.: after the update, people still know exactly which data were used in the work citing it). 9.5.3.3 Resolve the ticket Sending finalized URL and dataset citation before resolving ticket [NOTE: the URL format is very specific here, please try to follow it exactly (but substitute in the actual DOI of interest)] Here is the link and citation to your finalized data package: https://doi.org/10.18739/A20X0X First Last, et al. 2021. Title. Arctic Data Center. doi:10.18739/A20X0X. If in the future there is a publication associated with this dataset, we would appreciate it if you could register the DOI of your published paper with us by using the Citations button right below the title at the dataset landing page. We are working to build our catalog of dataset citations in the Arctic Data Center. Please let us know if you need any further assistance. 9.5.4 Additional email templates 9.5.4.1 Deadlines If the PI is checking about dates/timing: &gt; [give rough estimate of time it might take] &gt; Are you facing any deadlines? If so, we may be able to expedite publication of your submission. 9.5.4.2 Pre-assigned DOI If the PI needs a DOI right away: We can provide you with a pre-assigned DOI that you can reference in your paper, as long as your submission is not facing a deadline from NSF for your final report. However, please note that it will not become active until after we have finished processing your submission and the package is published. Once you have your dataset published, we would appreciate it if you could register the DOI of your published paper with us by using the citations button beside the orange lock icon. We are working to build our catalog of dataset citations in the Arctic Data Center. 9.5.4.3 Sensitive Data Which of the following categories best describes the level of sensitivity of your data? A. Non-sensitive data None of the data includes sensitive or protected information. Proceed with uploading data. B. Some or all data is sensitive but has been made safe for open distribution Sensitive data has been de-identified, anonymized, aggregated, or summarized to remove sensitivities and enable safe data distribution. Examples include ensuring that human subjects data, protected species data, archaeological site locations and personally identifiable information have been properly anonymized, aggregated and summarized. Proceed with uploading data, but ensure that only data that are safe for public distribution are uploaded. Address questions about anonymization, aggregation, de-identification, and data embargoes with the data curation support team before uploading data. Describe these approaches in the Methods section. C. Some or all data is sensitive and should not be distributed The data contains human subjects data or other sensitive data. Release of the data could cause harm or violate statutes, and must remain confidential following restrictions from an Institutional Review Board (IRB) or similar body. Do NOT upload sensitive data. You should still upload a metadata description of your dataset that omits all sensitive information to inform the community of the dataset’s existence. Contact the data curation support team about possible alternative approaches to safely preserve sensitive or protected data. Ethical Research Procedures. Please describe how and the extent to which data collection procedures followed community standards for ethical research practices (e.g., CARE Principles). Be explicit about Institutional Review Board approvals, consent waivers, procedures for co-production, data sovereignty, and other issues addressing responsible and ethical research. Include any steps to anonymize, aggregate or de-identify the dataset, or to otherwise create a version for public distribution. 9.5.4.4 Asking for dataset access As a security measure we ask that we get the approval from the original submitter of the dataset prior to granting edit permissions to all datasets. 9.5.4.5 No response from the researcher Please email them before resolving a ticket like this: We are resolving this ticket for bookkeeping purposes, if you would like to follow up please feel free to respond to this email. 9.5.4.6 Recovering Dataset submissions To recover dataset submissions that were not successful please do the following: Go to https://arcticdata.io/catalog/drafts Find your dataset and download the corresponding file Send us the file in an email 9.5.4.7 Custom Search Link You could also use a permalink like this to direct users to the datasets: https://arcticdata.io/catalog/data/query=\"your search query here” for example: https://arcticdata.io/catalog/data/query=Beaufort%20Lagoon%20Ecosystems%20LTER 9.5.4.8 Adding metadata via R KNB does not support direct uploading of EML metadata files through the website (we have a webform that creates metadata), but you can upload your data and metadata through R. Here are some training materials we have that use both the EML and datapack packages. It explains how to set your authentication token, build a package from metadata and data files, and publish the package to one of our test sites. I definitely recommend practicing on a test site prior to publishing to the production site your first time through. You can point to the KNB test node (dev.nceas.ucsb.edu) using this command: d1c &lt;- D1Client(\"STAGING2\", \"urn:node:mnTestKNB\") If you prefer, there are Java, Python, MATLAB, and Bash/cURL clients as well. 9.5.4.9 Finding multiple data packages If linking to multiple data packages, you can send a link to the profile associated with the submitter’s ORCID iD and it will display all their data packages. e.g.: https://arcticdata.io/catalog/profile/http://orcid.org/0000-0002-2604-4533 9.5.4.10 NSF ARC data submission policy Please find an overview of our submission guidelines here: https://arcticdata.io/submit/, and NSF Office of Polar Programs policy information here: https://www.nsf.gov/pubs/2016/nsf16055/nsf16055.jsp. Investigators should upload their data to the Arctic Data Center (https://arcticdata.io), or, where appropriate, to another community endorsed data archive that ensures the longevity, interpretation, public accessibility, and preservation of the data (e.g., GenBank, NCEI). Local and university web pages generally are not sufficient as an archive. Data preservation should be part of the institutional mission and data must remain accessible even if funding for the archive wanes (i.e., succession plans are in place). We would be happy to discuss the suitability of various archival locations with you further. In order to provide a central location for discovery of ARC-funded data, a metadata record must always be uploaded to the Arctic Data Center even when another community archive is used. 9.5.4.11 Linking ORCiD and LDAP accounts First create an account at orcid.org/register if you have not already. After that account registration is complete, login to the KNB with your ORCID iD here: https://knb.ecoinformatics.org/#share. Next, hover over the icon on the top right and choose “My Profile”. Then, click the “Settings” tab and scroll down to “Add Another Account”. Enter your name or username from your Morpho account and select yourself (your name should populate as an option). Click the “+”. You will then need to log out of knb.ecoinformatics.org and then log back in with your old LDAP account (click “have an existing account”, and enter your Morpho credentials with the organization set to “unaffiliated”) to finalize the linkage between the two accounts. Navigate to “My Profile” and “Settings” to confirm the linkage. After completing this, all of your previously submitted data pacakges should show up on your KNB “My Profile” page, whether you are logged in using your ORCiD or Morpho account, and you will be able to submit data either using Morpho or our web interface. Or, try reversing my instructions - log in first using your Morpho account (by clicking the “existing account” button and selecting organization “unaffiliated”), look for your ORCiD account, then log out and back in with ORCiD to confirm the linkage. Once the dataset is approved by the PI and there are no further changes, publish the dataset with a doi. doi &lt;- dataone::generateIdentifier(d1c_test@mn, &quot;DOI&quot;) dp &lt;- replaceMember(dp, metadataId, replacement=eml_path, newId=doi) newPackageId &lt;- uploadDataPackage(d1c_test, dp, public=TRUE, quiet=FALSE) 9.6 Categorize datasets As a final step we will categorize the dataset you processed. We are trying to categorize datasets so we can have a rough idea of what kinds of datasets we have at the Arctic Data Center. We will grant you access to the google sheet that has all of the categorized datasets We will categorize each dataset into one of the predefined themes (ie. biology, ecology etc.). Definition of the themes can be found in the google sheet Run the following line with your doi and themes as a list. datamgmt::categorize_dataset(&quot;your_doi&quot;, c(&quot;list&quot;, &quot;of&quot;, &quot;themes&quot;), &quot;your name&quot;) 9.7 Congrats! Congratulations on finishing your first ticket! You can head over to the the repository, data-processing to get your ticket processing code reviewed by the team so we can learn from each other! "],["beyond-your-first-ticket.html", "Chapter 10 Beyond your first ticket 10.1 Working with large data packages 10.2 Add physicals to submissions 10.3 Use references 10.4 Annotations 10.5 Streamlining your workflow 10.6 Code Snippets 10.7 Resources for R", " Chapter 10 Beyond your first ticket This section is meant to be read after you have processed a couple of tickets and you are comfortable with the workflow with a relatively simple dataset with 1-2 files and want to expand your skills and workflows further. 10.1 Working with large data packages 10.2 Add physicals to submissions New submissions made through the web editor will not have any physical sections within the otherEntitys. Add them to the EML with the following script: for (i in seq_along(doc$dataset$otherEntity)) { otherEntity &lt;- doc$dataset$otherEntity[[i]] id &lt;- otherEntity$id if (!grepl(&quot;urn-uuid-&quot;, id)) { warning(&quot;otherEntity &quot;, i, &quot; is not a pid&quot;) } else { id &lt;- gsub(&quot;urn-uuid-&quot;, &quot;urn:uuid:&quot;, id) physical &lt;- arcticdatautils::pid_to_eml_physical(mn, id) doc$dataset$otherEntity[[i]]$physical &lt;- physical } } As you can see from code above, we use a for loop here to add physical sections. The for loop is a very useful tool to iterate over a list of elements. With for loop, you can repeat a specific block of code without copying and pasting the code over and over again. When processing datasets in Arctic Data Center, there are many places where for loop can be used, such as publishing a bunch of objects with pids, updating formatID for pkg$data, adding physical section like above code, etc. A loop is composed of two parts: the sequence and the body. The sequence usually generates indices to locate elements and the body contains the code that you want to iterate for each element. Here is an example of adding the same attributeList for all the dataTables in the metadata using for loop. attributes &lt;- read.csv(&#39;attributes.csv&#39;) # attribute table in csv format attributeList &lt;- EML::set_attributes(attributes = attributes) for (i in 1:length(doc$dataset$dataTable)) { # sequence part doc$dataset$dataTable[[i]]$attributeList &lt;- attributeList # body part } 10.3 Use references References are a way to avoid repeating the same information multiple times in the same EML record. There are a few benefits to doing this, including: Making it clear that two things are the same (e.g., the creator is the same person as the contact, two entities have the exact same attributes) Reducing the size on disk of EML records with highly redundant information Faster read/write/validate with the R EML package You may want to use EML references if you have the following scenarios (not exhaustive): One person has multiple roles in the dataset (creator, contact, etc) One or more entities shares all or some attributes 10.3.1 Example with parties Do not use references for creators as it is used for the citation information. The creators will not show up on the top of the dataset if it is a reference. Until this issue is resolved in NCEAS/metacat#926 we will need to keep this in account. It’s very common to see the contact and creator referring to the same person with XML like this: &lt;eml packageId=&quot;my_test_doc&quot; system=&quot;my_system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1 eml.xsd&quot;&gt; &lt;dataset&gt; &lt;creator&gt; &lt;individualName&gt; &lt;givenName&gt;Bryce&lt;/givenName&gt; &lt;surName&gt;Mecum&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; &lt;contact&gt; &lt;individualName&gt; &lt;givenName&gt;Bryce&lt;/givenName&gt; &lt;surName&gt;Mecum&lt;/surName&gt; &lt;/individualName&gt; &lt;/contact&gt; &lt;/dataset&gt; &lt;/eml&gt; So you see those two times Bryce Mecum is referenced there? If you mean to state that Bryce Mecum is the creator and contact for the dataset, this is a good start. But with just a name, there’s some ambiguity as to whether the creator and contact are truly the same person. Using references, we can remove all doubt. doc$dataset$creator[[1]]$id &lt;- &quot;reference_id&quot; doc$dataset$contact &lt;- list(references = &quot;reference_id&quot;) print(doc) &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:stmml=&quot;http://www.xml-cml.org/schema/stmml-1.1&quot; packageId=&quot;id&quot; system=&quot;system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1/ eml.xsd&quot;&gt; &lt;dataset&gt; &lt;title&gt;A Minimal Valid EML Dataset&lt;/title&gt; &lt;creator id=&quot;reference_id&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Bryce&lt;/givenName&gt; &lt;surName&gt;Mecum&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; &lt;contact&gt; &lt;references&gt;reference_id&lt;/references&gt; &lt;/contact&gt; &lt;/dataset&gt; &lt;/eml:eml&gt; The reference id needs to be unique within the EML record but doesn’t need to have meaning outside of that. 10.3.2 Example with attributes To use references with attributes: Add an attribute list to a data table Add a reference id for that attribute list Use references to add that information into the attributeLists of the other data tables For example, if all the data tables in our data package have the same attributes, we can set the attribute list for the first one, and use references for the rest: doc$dataset$dataTable[[1]]$attributeList &lt;- attribute_list doc$dataset$dataTable[[1]]$attributeList$id &lt;- &quot;shared_attributes&quot; # use any unique name for your id for (i in 2:length(doc$dataset$dataTable)) { doc$dataset$dataTable[[i]]$attributeList &lt;- list(references = &quot;shared_attributes&quot;) # use the id you set above } 10.4 Annotations If there are multiple tables with similar annotations you can try something like this: #go through each dataTable for(i in 1:length(doc$dataset$dataTable)){ #get all the attibute names an &lt;- eml_get_simple(doc$dataset$dataTable[[i]], &quot;attributeName&quot;) #go through the attributes figure out what will match for(a in 1:length(an)){ annotation &lt;- dplyr::case_when( # the attributeName to match ~ valueLabel an[[a]] == &quot;Sample ID&quot; ~ &quot;Identity&quot;, an[[a]] == &quot;Location&quot; ~ &quot;study location name&quot;, an[[a]] == &quot;Latitude&quot; ~ &quot;latitude coordinate&quot;, an[[a]] == &quot;Longitude&quot; ~ &quot;longitude coordinate&quot;, an[[a]] == &quot;Elevation (m)&quot; ~ &quot;elevation&quot;, str_detect(an[[a]], &quot;Depth|depth&quot;) ~ &quot;Depth&quot;) #only run this code when the annotations match if(!is.na(annotation)){ #based on the entity Name create a unique id entity &lt;- str_split(doc$dataset$dataTable[[i]]$entityName, &quot;_&quot;) doc$dataset$dataTable[[i]]$attributeList$attribute[[a]]$id &lt;- paste0(entity[[1]][[1]], &quot;_&quot;, an[[a]]) #add the annotation doc$dataset$dataTable[[i]]$attributeList$attribute[[a]]$annotation &lt;- eml_ecso_annotation(annotation) } } } 10.5 Streamlining your workflow 10.6 Code Snippets Code snippets help with templating portions of code that you will be using regularly. To add your own, go to the toolbar ribbon at the top of your Rstudio screen and select: Tools &gt; Global Options... &gt; Code &gt; Edit Snippets &gt; Add these chunks to the end of the file More info can be found in this blog post by Mara Averick on how to add them: https://maraaverick.rbind.io/2017/09/custom-snippets-in-rstudio-faster-tweet-chunks-for-all/ Usual arcticdatautils ticket workflow snippet ticket library(dataone) library(arcticdatautils) library(EML) cn &lt;- CNode(&#39;PROD&#39;) adc &lt;- getMNode(cn, &#39;urn:node:ARCTIC&#39;) rm &lt;- &quot;add your rm&quot; pkg &lt;- get_package(adc, rm) doc &lt;- EML::read_eml(getObject(adc, pkg\\$metadata)) doc &lt;- eml_add_publisher(doc) doc &lt;- eml_add_entity_system(doc) eml_validate(doc) eml_path &lt;- &quot;eml.xml&quot; write_eml(doc, eml_path) #update &lt;- publish_update(adc, # metadata_pid = pkg\\$metadata, # resource_map_pid = pkg\\$resource_map, # metadata_path = eml_path, # data_pids = pkg\\$data, # public = F) #datamgmt::categorize_dataset(update\\$metadata, c(&quot;theme1&quot;), &quot;Your Name&quot;) The datapack ticket workflow snippet datapack library(dataone) library(datapack) library(digest) library(uuid) library(dataone) library(arcticdatautils) library(EML) d1c &lt;- D1Client(&quot;PROD&quot;, &quot;urn:node:ARCTIC&quot;) packageId &lt;- &quot;id here&quot; dp &lt;- getDataPackage(d1c, identifier=packageId, lazyLoad=TRUE, quiet=FALSE) #get metadata id metadataId &lt;- selectMember(dp, name=&quot;sysmeta@formatId&quot;, value=&quot;https://eml.ecoinformatics.org/eml-2.2.0&quot;) #edit the metadata doc &lt;- read_eml(getObject(d1c@mn, metadataId)) #add the publisher info doc &lt;- eml_add_publisher(doc) doc &lt;- eml_add_entity_system(doc) doc\\$dataset\\$project &lt;- eml_nsf_to_project(&quot;nsf id here&quot;) #check and save the metadata eml_validate(doc) eml_path &lt;- arcticdatautils::title_to_file_name(doc\\$dataset\\$title) write_eml(doc, eml_path) dp &lt;- replaceMember(dp, metadataId, replacement=eml_path) #upload the dataset myAccessRules &lt;- data.frame(subject=&quot;CN=arctic-data-admins,DC=dataone,DC=org&quot;, permission=&quot;changePermission&quot;) packageId &lt;- uploadDataPackage(d1c, dp, public=F, accessRules=myAccessRules, quiet=FALSE) #datamgmt::categorize_dataset(&quot;doi&quot;, c(&quot;theme1&quot;), &quot;Jasmine&quot;) Quick way to give access to submitters to their datasets: snippet access library(dataone) library(arcticdatautils) library(EML) cn &lt;- CNode(&#39;PROD&#39;) adc &lt;- getMNode(cn, &#39;urn:node:ARCTIC&#39;) rm &lt;- &quot;rm here&quot; pkg &lt;- get_package(adc, rm) set_access(adc, unlist(pkg), &quot;orcid here&quot;) Quick access to the usual code for common Solr queries: snippet solr library(dataone) library(arcticdatautils) library(EML) cn &lt;- CNode(&#39;PROD&#39;) adc &lt;- getMNode(cn, &#39;urn:node:ARCTIC&#39;) result &lt;- query(adc, list(q = &quot;rightsHolder:*orcid.org/0000-000X-XXXX-XXXX* AND (*:* NOT obsoletedBy:*)&quot;, fl = &quot;identifier,rightsHolder,formatId, fileName, dateUploaded&quot;, sort = &#39;dateUploaded+desc&#39;, start =&quot;0&quot;, rows = &quot;1500&quot;), as=&quot;data.frame&quot;) 10.7 Resources for R 10.7.1 Learning The following online books are useful for expanding your R knowledge and skills: the most recent ADC training materials The cleaning and data manipulation section is useful for working with attribute tables Efficient R Programming In particular Chapter 3 Efficient Programming R for Data Science Section on Strings R Packages contributing to arcticdatatutils, datamgmt and EML Advanced R Object-oriented programming in R for S4 to understand how datapack and dataone packages are written bookdown: Authoring Books and Technical Documents with R Markdown formatting, troubleshooting and updating the training document Others Hands-On Programming with R R Programming for Data Science Exploratory Data Analysis with R Mastering Software Development in R Geocomputation with R R Markdown: The Definitive Guide The Tidyverse Style Guide The RStudio cheatsheets are also useful references for functions in tidyverse and other packages. 10.7.2 Packages The data team uses and develops a number of R packages. Here is a listing and description of the main packages: dataone reading and writing data at DataONE member nodes http://doi.org/10.5063/F1M61H5X datapack creating and managing data packages https://github.com/ropensci/datapack EML creating and editing EML metadata documents https://ropensci.github.io/EML arcticdatautils utility functions for processing data for the Arctic Data Center https://nceas.github.io/arcticdatautils/ datamgmt data management utilities for curating, documenting, and publishing data (sandbox package) https://nceas.github.io/datamgmt/ metadig authoring MetaDIG quality checks https://github.com/NCEAS/metadig-r metajam downloading and reading data and metadata from DataONE member nodes https://nceas.github.io/metajam/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
