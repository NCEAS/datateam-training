# Creating a data package

This chapter will teach you how to create and submit a data package to a DataONE MN via R. But first, please read [this paper](data/eml-principles.pdf) on the value of structured metadata, namely the Ecological Metadata Language (EML).

## What is in a package?

A data package generally consists of at least 3 components. 

1. Metadata: One object is the metadata file itself. In case you are unfamiliar with metadata, metadata are information that describe data (e.g. who made the data, how were the data made, etc.). The metadata file will be in an XML format, and have the extension `.xml` (extensible markup language). We often refer to this file as the EML, which is the metadata standard that it uses. This is also what you see when you click on a page in the Arctic Data Center.

2. Data: Other objects in a package are the data files themselves. Most commonly these are data tables (`.csv`), but they can also be audio files, NetCDF files, plain text files, PDF documents, image files, etc. 

3. Resource Map: The final object is the resource map. This object is a plain text file with the extension `.rdf` (<a href = 'https://www.w3.org/RDF/' target='_blank'>Resource Description Framework</a>) that defines the relationships between all of the other objects in the data package. It says things like "this metadata file describes this data file," and is critical to making a data package render correctly on the website with the metadata file and all of the data files together in the correct place. Fortunately, we rarely, if ever, have to actually look at the contents of resource maps; they are generated for us using tools in R.

![From the DataOne Community Meeting (Session 7)](images/data-submission-workflow2.png)

## Packages on the Website

All of the package information is represented when you go to the landing page for a dataset. When you make changes through R those published changes will be reflected here. Although you can edit the metadata directly from the webpage but we recommend to use R in most cases.

![](images/arctic_data_center_web.png)

## About identifiers

Each object (metadata files, data files, resource maps) on the ADC or the KNB (another repo) has a unique identifier, also sometimes called a "PID" (persistent identifier). When you look at the landing page for a dataset, for example <a href = 'https://arcticdata.io/catalog/#view/doi:10.18739/A2836Z' target='_blank'>here</a>, you can find the resource map identifier listed under the title in the gray bar after the words "Files in this dataset Package:" (`resource_map_doi:10.18739/A2836Z`), the metadata identifier in the "General > Identifier" section of the metadata record or after the title with blue font (`doi:10.18739/A2836Z`), and the data identifier by clicking the "more info" link next to the data object, and looking at the "Online Distribution Info" section (`arctic-data.9546.1`).

Note, all datasets submitted are given a temporary identifier (usually starting with `urn:uuid:`). When the dataset is finalized, a doi will be issued.


![](images/PIDs.png)


Different versions of a package are linked together by what we call the "version chain" or "obsolescence chain". Making an update to a data package, such as replacing a data file, changing a metadata record, etc, will result in a new identifier for the new version of the updated object. When making changes to a package, always use `datapack::uploadDataPackage()` for updating the entire package on the *latest versions* of all objects to ensure that the version chain is maintained.

## Upload a package

We will be using R to connect to the <a href = 'https://arcticdata.io/catalog/#data' target='_blank'>NSF Arctic Data Center (ADC)</a> data repository to push and pull edits in actual datasets. To identify yourself as an admin you will need to pass a 'token' into R. Do this by signing in to the ADC with your ORCid and password, then hovering over your name in the top right corner and clicking on "My profile", then navigating to "Settings" and "Authentication Token", copying the "Token for DataONE R", and finally pasting and running it in your *R console*.

```{block, type = "warning"}
**This token is your identity on these sites, please treat it as you would a password** (i.e. don't paste into scripts that will be shared). The easiest way to do this is to always run the token in the *console*. There's no need to keep it in your script since it's temporary anyway.
```

You will need to retrieve a new one after it either expires or you quit your R session.

Sometimes you'll see a placeholder in scripts to remind users to get their token, such as:

```{r token, message=FALSE, eval=FALSE}
options(dataone_test_token = "...")
```

```{block, type = "note"}
Since we will be working on the test site and not the production site, please remember to get your token from test.arcticdata.io
```

Next, please be sure these packages are loaded for the training (these should already exist if you are working on the server):

```{r load libraries, message=FALSE, eval=FALSE}
library(devtools)
library(dataone)
library(datapack)
library(EML)
library(remotes)
library(XML)
library(uuid)
```

If any package could not be loaded, use the following command (replacing package_name with the actual package name) to install the package, then load them.

```{r, eval = FALSE}
install.packages("package_name")
```

Now install a couple of packages:

```{r, eval = FALSE}
remotes::install_github("nceas/arcticdatautils")
library(arcticdatautils)
remotes::install_github("nceas/datamgmt")
library(datamgmt)
```

```{block, type = "note"}
When you are usually working with data packages you will only need the following:
library(dataone)
library(datapack)
library(EML)
library(arcticdatautils)
```

For this training, we will be working exclusively on the Arctic test site, or "node." In many of the functions you will use this will be the first argument. It is often referred to in documentation as `mn`, short for member node. More information on the other nodes can be found in the reference section under Set DataONE nodes [Set DataONE nodes](https://nceas.github.io/datateam-training/reference/set-dataone-nodes.html)

For example, if we are using the test site, set the node to the test Arctic node:

``` {r, eval = FALSE, class.source = 'exercise'}
d1c_test <- dataone::D1Client("STAGING", "urn:node:mnTestARCTIC")
```

Once all set up you can first publish an object (data)

If you are curious how everything magically works, here is a handy diagram:

![From the DataOne Community Meeting (Session 7)](images/data-submission-workflow1.png)

```{r, child = '../workflows/datapack_dataone/01_datapack_background.Rmd'}
```

```{r, child = '../workflows/datapack_dataone/02_create_package_data_pack.Rmd'}
```

## Exercise 2b {.exercise}
This exercise will take you through how to do the submission process through R instead of the webform (exercise 1).

### Part 1 - Gather your data files

For our convenience, we will be grabbing the metadata and data files from the file we published earlier:

* Locate the data package you published in [Exercise 1](#exercise-1) by navigating to the "My Profile > My Data" section on <a href = 'https://test.arcticdata.io' target='_blank'>test.arcticdata.io</a>.
* Download the metadata and data files and transfer them to the Datateam server.

### Part 2 - Working in R

Now we want to publish the metadata and data files we downloaded again to `test.arcticdata.io`

* Obtain a token and **please note** that for this exercise please make sure you grab the token from the <a href = 'https://test.arcticdata.io' target='_blank'> arcticdata test site</a>
* Publish your metadata and data file to the site.

``` {r, eval = FALSE, class.source = 'exercise'}
#set the node
d1c_test <- dataone::D1Client("STAGING", "urn:node:mnTestARCTIC")

dp <- new("DataPackage")

#add your metadata
metadataObj <- new(...)
dp <- addMember(...)

#add your data files
sourceObj <- new(...)
dp <- addMember(...)

#upload your package
myAccessRules <- data.frame(...) 
packageId <- uploadDataPackage(...)
```

* View your new data set by appending the metadata PID to the end of the URL test.arcticdata.io/#view/... 
* If you are successful it should look the same as the dataset you created in exercise 1
